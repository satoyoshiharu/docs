<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This site is to promote the thought that uses the daily behavior of people across the human-machine interactions.">
    <meta name="author" content="Yoshiharu Sato">
	<title>ヒトのからだと機械</title>
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Josefin+Slab:100,300,400,600,700,100italic,300italic,400italic,600italic,700italic" rel="stylesheet" type="text/css">

<style>
p, li { line-height: 1.5;  }
table { margin: auto; }
.pagebreak { break-before: page; }
.printfriendlyblock { page-break-inside: avoid; }
</style>
	
</head>

<body>
<h1>ヒトと機械</h1>
<hr>

<details open><summary>目次</summary>
	<!--https://shu-sait.com/mokuji-jidou-seisei/-->
	<div id="toc" class="l-toc"></div>
</details>

<div class="toc-contents"> <!--Start of TOC areas-->

<div class="pagebreak"><hr><h2>1. はじめに</h2>

	<details open><summary><strong>本文書の想定読者</strong></summary>
		<p>
			本文書は、これからITを志す若者、IT技術分野で現在活躍しているプロフェッショナル、ITに関する動向のインフルエンサーなどを、読者として想定している。
			また、IT技術に関心のある人すべてにとって、興味を引く論点を提供する。
			一般向けの読み物ではない。
		</p>
	</details>

	<details open><summary><strong>道具がヒトの能力と調和したとき、道具は有効なものとなり、ヒトの能力を拡大する</strong></summary>
		<p>
			ヒトは、その能力によって道具を発明し、自分ができることを拡大してきた。
			例えば、ヒトは、文字と紙という道具を発明した。
			それによって、ヒトは、記憶したり伝達したりする能力を、拡張した。
			また、ヒトは、紙という平面の物体に、3Dの物体の見取り図を表現する。
			ヒトは、紙とペンを操り、3Dを表現できる。
			これは生身の目では見通せない俯瞰的な視覚である。
			自分の能力の拡張である。
			紙に文字や図を書くとき、ヒトは、器用な手指を操作する。
			高性能な目で指の軌跡を確認する。
			手指の内部的な感覚とともに動きに修正をかける。
			といったことを、瞬時に連続的に繰り返し行っている。
			ヒトは、その高度な目の機能と手指の器用さによって、道具を使いこなしている。
			道具とヒト能力の好循環は、石器時代の昔からヒトを変えてきた。
			最近では、顕微鏡や望遠鏡が、見えるものを広げた。
			車が、ヒトの移動距離を変えた。
			インターネットが、コミュニケーションする集団の規模を変えた。
		</p>
		<p>
			道具が人の能力と調和したとき、道具は有効なものとなる。
			そして、有効な道具は、ヒトの能力を拡大する。
			さらに、拡大されたヒトの能力によって、道具は一層高度になる。
		</p>
	</details>

	<details open><summary><strong>コンピュータという道具は、ヒトの能力と調和していない</strong></summary>
		<p>
			しかし、現在のヒトとコンピュータの間の関係はどうか。
			例えば、ヒトは、はさみをヒトは難なく使える。
			それに対して、コンピュータは、マニュアルを読まないと使えない。
			マニュアルに書いてある、人工的な抽象物の建築中の歩き方を習熟してからでないと、何をどうするか見当もつかない。
			例えば、ヒトは、自転車に乗ったり、自動車のハンドルを操作するのに、少し訓練すれば、あとはほとんど無意識的に筋肉が動いてくれる。
			一方、コンピュータは、少し慣れたあとでも、操作する際、これをやれば何が起きるかと、常に知的な緊張を強いられる。
			例えば、子供から成長するにつれて、ほとんどのヒトは言葉をしゃべり、文字を書くことができるようになる。
			一方、コンピュータは、情報を流通するツールとしても、誰でも使えるものでない。
			現状のコンピュータの技術は、ヒトの能力と調和していない。
		</p>
	</details>

	<details open><summary><strong>コンピュータの違和感</strong></summary>
		<p>
			コンピュータは、ものであり、道具である。
			しかし、ハサミのような単機能の道具と比べると、はるかに抽象的な事柄を扱う。
			コンピュータの処理対象は情報であり、コンピュータの機能は計算や情報の加工であり、どちらもヒトの抽象物である点が、日常見る道具とは異なる。
		</p>
		<p>
			しかも、コンピュータは、人の抽象物でありながら、ほかの抽象物とは異な点がある。
			例えば、建築物の構造、都市の道路や建物の配列などは、人の抽象物でありながら、ヒトの生活や身体と密接に関連している。
			例えば、経済活動は、ヒトの抽象物でありながら、ヒトの生存や生活と密接にかかわっている。
			一方、コンピュータは、ヒトの身体との交渉がない点、具体性がない点で、数学や神話などに近い。
			それらは、頭の中から生まれたものである。
		</p>
		<p>
			コンピュータは、人の頭が生み出した道具である。
			しかしコンピュータと呼ぶには不思議なくらい、すでにいろいろなモノの中に組み込まれ浸透している。
			移動したり暇なときにはほとんど持っているスマホ、自動車、電子レンジ・洗濯機などの家電、などである。
			通信を介してコントロールされているものを含めたら、いろいろなインフラを支える装置もそうである。
		</p>
		<p>
			ところで、ヒトはモノ道具を操作し、ヒトとコミュニケーションする。
			今、操作とコミュニケーションという異なる関係性に注目してみる。
			それぞれ操作モデル、コミュニケーションモデルと呼ぶことにする。
			コンピュータは、モノ道具の一つとして生まれた。
			しかし、人が操作するモノ道具の一種とみなすには、あまりにも、具体性がない抽象物であり、偏在的でもある。
			むしろ、ヒト相手のコミュニケーションの相方とみなしたほうが、違和感が薄れる。
		</p>
		<p>
			相手がモノでなく、ヒトなら、コミュニケーションモデルの相棒としてなら、相手に期待されることはどう違うか？
			<ul>
				<li>
					相手は、こちらのまなざし、指さし、表情を理解するのは当たり前である。
					つまり、ヒトの身体と生得の能力に反応する。
				</li>
				<li>
					相手は、こちらの意図を思い計ってくれる。
					つまり、ヒトが日常で意識する意図レベルで反応する。
				</li>
			</ul>
		</p>

		<p>
			このように、コンピュータは、モノ道具という位置づけにはふさわしくないものであるのに、
			モノ道具のままの操作という関係性のままでいるから、違和感がある。
			この違和感をなくすには、コンピュータはヒトの身体に反応し、ヒトの言葉で意図を思い計るようでないといけない。
			ヒトがコンピュータに合わせるのでなく、コンピュータがヒトに合わせる。
			そのとき、コンピュータはヒトと調和する。
		</p>
		<p>
			それでは、コンピュータは、もっとハイテクを駆使できるようにならなければならないのか？
			そうではない。
			コンピュータを作る人、アプリを作る人が、すでにある技術をちょっと視点を変えてデザインをすればよい。
		</p>
	</details>

	<details open><summary><strong>本書の構成</strong></summary>
		<p>
			本文書の前半では、ヒトの身体機能を概観し、それと接する機械がどうあるべきかの参考にする。
			ナチュラルユーザーインターフェイスという言葉があるが、それはヒトの能力を尊重して実現できる。
			後半では、ヒトの身体に、意図レベルで、反応するようなコンピュータの具体的なイメージを説く。
			それは、ハイテクである必要はないことを説明する。
		</p>
	</details>

</div>

<div class="pagebreak"><hr><h2>2. ヒト生体の情報処理</h2>

	<div><h3>2.1 概観</h3>

		<details open><summary><strong>ヒトの生体機能を見ていく理由</strong></summary>
			<p>
				モノ道具であるコンピュータは、ヒトとコミュニケーションするモデルに沿って、ユーザインターフェイスや内部の概念を構成するべきだ。
				ヒトのカラダの能力を、そのまま受け止め、それに反応してほしい。
				ここでは、まず、ヒトが生物として世界に対処するときの、生体情報処理の特徴を見ていく。
			</p>
		</details>

		<details open><summary><strong>ヒトは8つの感覚器と2つの作用効果器で情報処理している</strong></summary>
			<p>
					ポール・ナースによれば、「あらゆる生命には、自分と子孫を永続させるという目的がある。
					あらゆる生命の中心には、情報がある。
					目的のための行動に、情報は利用される。
					<a href="#rPN">[rPN]</a>」。
				</p>
				<p>
					生体は、外の世界と自分の体の内側の世界との両方に関して、情報を絶えず集めて利用している。
					内外の環境の状況を把握するものを<strong>受容器官</strong>という。
					それらに応じて外部に働きかけるものを<strong>効果器官</strong>という。
					受容器官と効果器官とを連結し統括するものは、<strong>脳・神経系</strong>である。
			</p>
			<p>
				受容器官は、特殊感覚と体制感覚がある。
				<strong>特殊感覚</strong>は、特定の刺激に対して、特定の場所にある器官が反応する。
				<strong>体性感覚</strong>は、身体に分散して存在し環境への反応を助ける。
				一般的に五感と言われるものは、平衡感覚と固有感覚を除外した、視覚、味覚、嗅覚、聴覚、触覚である。
				一方、ヒトの効果器官には、機械系と音響系がある。
			</p>
			<p>
				<ul>
				<li>受容器官</li>
					<ul>
					<li>特殊感覚器官</li>
						<ul>
						<li>視覚器</li>
						<li>味覚器</li>
						<li>嗅覚器</li>
						<li>平衡・聴覚器（組織的に同居）</li>
						</ul>
					<li>体性感覚器官</li>
						<ul>
						<li>外部から受容する皮膚感覚</li>
						<li>筋、腱、関節内で感知する固有感覚</li>
						</ul>
					</ul>
				<li>効果器官</li>	
					<ul>
					<li>筋肉や骨格からなる手指や身体の機械的運動系</li>
					<li>発声器官</li>
					</ul>
				</ul>
			</p>
		</details>

		<details open><summary><strong>情報媒体は、光、振動、科学的・物理的刺激である</strong></summary>
			<p>
				視覚は、光という電磁波を感知する。
				聴覚は、ヒトを含む陸生動物の場合、空気振動を感知する。
				触覚（皮膚感覚）は物理的刺激を感知する。
				そして味覚と嗅覚は化学的物質を感知する。
				運動系は物理的な効果を持つ。
				音声発生は、空気振動を起こす。
			</p>
			<div class="printfriendlyblock">
				<p>
				<table border="1">
				<tr><td>受容器官</td><td>特殊感覚</td><td>視覚</td><td>光</td></tr>
				<tr><td></td><td></td><td>味覚</td><td>化学的物質</td></tr>
				<tr><td></td><td></td><td>嗅覚</td><td>化学的物質</td></tr>
				<tr><td></td><td></td><td>平衡・聴覚</td><td>重力、空気振動</td></tr>
				<tr><td></td><td>体性感覚</td><td>皮膚感覚</td><td>物理的</td></tr>
				<tr><td></td><td></td><td>筋、腱、関節内固有感覚</td><td>物理的</td></tr>
				<tr><td>効果器官</td><td></td><td>機械的運動系</td><td>物理的</td></tr>
				<tr><td></td><td></td><td>発声</td><td>空気振動</td></tr>
				</table>
				</p>
			</div>
		</details>

		<details open><summary><strong>近接感覚と遠隔感覚がある</strong></summary>
			<p>
				感覚には、遠くからのことを感知するものと、近いもことを感知するものと、どこでも偏在することを感知するものがある。
			</p>
			<p>
				どんな生物も、例外なく、遠近に関係なく偏在する、重力を感じる。
				動物は自分の体の傾きを、平衡器官で感知している
				<a href="#rIN">[rIN]</a>。
			</p>
			<p>
				味覚、接触感覚は、近くのもののの感覚である。
				動物の進化の中で、これらの近接感覚がまずあったと思われる。
				味覚は、化学物質を感知する。
				接触感覚は物理的な圧力を感知する。
				これらが、まずは、食べ物を識別し、仲間と生殖・交信するためにあったと思われる。
				また、運動するために、体の状態を感知する体性感覚も必須だったろう。
			</p>
			<p>
				一方、生物が行動範囲を広げる際に、より広い環境を感知し、よりうまく生存・生殖したい。
				嗅覚は、遠くからの化学物質を感知する。
				聴覚は、遠くからの水・空気振動を感知する。
				視覚は、遠くからの光電磁波刺激を感知する。
			</p>

			<div class="printfriendlyblock">
			<p>
				<table border="1">
				<tr><td>受容器官</td><td>特殊感覚</td><td>視覚</td><td>遠隔</td></tr>
				<tr><td></td><td></td><td>味覚</td><td>近接</td></tr>
				<tr><td></td><td></td><td>嗅覚</td><td>遠隔</td></tr>
				<tr><td></td><td></td><td>平衡、聴覚（器官的に同居している）</td><td>平衡感覚は偏在、聴覚は遠隔</td></tr>
				<tr><td></td><td>体性感覚</td><td>皮膚感覚</td><td>近接</td></tr>
				<tr><td></td><td></td><td>筋、腱、関節内固有感覚</td><td>近接</td></tr>
				<tr><td>効果器官</td><td></td><td>機械的運動系</td><td>近接</td></tr>
				<tr><td></td><td></td><td>発声</td><td>遠隔</td></tr>
				</table>
				</p>
			</div>
		</details>

		<details open id=通信容量><summary><strong>それぞれの器官の通信容量</strong></summary>
			<p>
				人間の全受容器から、感覚神経を経由して中枢神経系へ伝送される情報量は、10の9乗ビット/秒と言われる。
				うち、視覚は、10の6から8乗ビット/秒、聴覚は10の4から6乗/秒、触覚は10の6乗/秒と言われる。
				一方、中枢神経系から、運動神経を経由して効果器へ伝送される情報量は、10の7乗ビット/秒と言われる
				<a href="#rSO">[rSO]</a>、<a href="#rFT">[rFT]</a>、<a href="#n01">[脚注01]</a>。
				視覚は人間の情報獲得の80%ともいわれる<a href="#n02">[脚注02]</a>。
			</p>

			<div class="printfriendlyblock">
				<p>
				<table border="1">
				<tr><td>受容器官</td><td></td><td></td><td>1000000000 bit/sec</td></tr>
				<tr><td></td><td>特殊感覚</td><td>視覚</td><td>100000000 bit/sec</td></tr>
				<tr><td></td><td></td><td>味覚</td><td>10000 bit/sec</td></tr>
				<tr><td></td><td></td><td>嗅覚</td><td>100000 bit/sec</td></tr>
				<tr><td></td><td></td><td>聴覚</td><td>100000 bit/sec</td></tr>
				<tr><td></td><td>体性感覚</td><td>皮膚感覚</td><td>1000000 bit/sec</td></tr>
				<tr><td></td><td></td><td>筋、腱、関節内固有感覚</td><td></td></tr>
				<tr><td>効果器官</td><td></td><td></td><td>10000000 bit/sec</td></tr>
				<tr><td></td><td></td><td>機械的運動系</td><td></td></tr>
				<tr><td></td><td></td><td>発声</td><td></td></tr>
				</table>
				</p>
			</div>
			<p>
				ヒトは環境を認識するのに、主に視覚を使っている。
				また、環境に対する反応の情報量は、感覚よりも桁違いに小さい。
				ヒトという生物は、環境から多くの情報を仕入れて、生存・繁殖に有効なものを選んで反応し、環境に働きかけている。
			</p>
		</details>

		<details open><summary><strong>感覚、知覚、認知の違い</strong></summary>
			<p>
				受容器官に刺激が与えら脳に伝えられたもの感覚という。
				熱いとか、音が聞こえるとかである。
			</p>
			<p>
				感覚に、対象の構造や特徴が加えられて、意識されたものが知覚である。
				長いとか、強いとか。
				感覚はその一部が知覚となる。
				意識に上らないことは多い。
				無意識の過程により、感覚はフィルターされる。
			</p>
			<p>
				さらに、知覚が過去の経験や学習に基づいて解釈されたものが認知である。
				犬であるとか、母であるとか
				<a href="#rIN">[rIN]</a>。	
				認知になると、文化や社会の影響が濃厚に出る。
				社会の文化によって、虫の鳴き声を、雑音と感じるか、秋の風情と感じるか、が異なってくる。	
			</p>
			<p>
					感覚、知覚、認知は、ヒトが環境から情報を仕入れる、階層的な分類である。
					反応に関しての分類は、どうか？ 不随的・随意的、無意識的・意図的などの分類がある。
			</p>

		</details>

		<details open><summary><strong>脚注</strong></summary>
			<ul>
				<li id=n01>[01] 別のソースでは、視覚は10の8乗ビット/秒、触覚は10の6乗ビット/秒、聴覚と嗅覚はそれぞれ10の5乗ビット/秒、味覚は10の3乗ビット/秒、とも言われる。</li>
				<li id=n02>[02] これらに、厳密な根拠はないとも言われる<a href="#rKH">[rKH]。</a></li>
			</ul>
		</details>

	</div>

	<div><h3>2.2 受容器官</h3>

		<div><h4>2.2.1 視覚</h4>

			<details open><summary><strong>人は視覚的動物である</strong></summary>

				<p>
					人の得る情報の80%は視覚からと言われ、ヒトは視覚的動物である。
				</p>
				<p>
					光という情報媒体は、遠くまで届く、高速に伝わる、という特徴を持つ。
					生物が光を感知できるとしたら、瞬時に、遠方まで、敵ないし見方、餌を識別でき、生存に有利である。
					光は高速なので、どこに物があっても瞬時に把握できるという意味で、ほかの感覚と比べ、距離に左右されない。
				</p>
				<p>
					地球史の中に、カンブリア紀という、その時代からいきなり化石が出始めた時代がある。
					英人生物学者のアンドリュー・パーカー（Andrew Parker、1967～）は、2003年、「光スイッチ説」を唱えた。
					カンブリア紀に登場した三葉虫は、目を進化させた結果、食べ、食べられる関係で優位に立った。
					それが、淘汰圧として、ほかの生物の多様な進化を促した、と主張した<a href="#rMK">[rMK]</a>。
					それまでは、化石になるような骨や外殻がない生物しか存在していなかったが、
					視覚の登場が生存競争・選択淘汰を激烈にし、
					骨や外殻を備えた多様な生物がこのときに生まれたという。
				</p>

				<p>
					その後、生物の長い歴史の中、器官の発達とともに、視覚は機能的に進化してきた。
					まず、原始的な生物は、明暗識別ができた。
					ついで、明暗の方向視、形態視、動きの感知、色認識、そして、両眼視による遠近を含む探索・位置同定、ができるようになった。
				</p>

				<p>
					一方で、光は他のものによって遮られるという欠点がある。また、夜には光がなくなる。
					そのため、光あふれた昼間、地表での生存競争を避けた生物は、別の感覚を伸ばす必要があった。
					化学物質である臭いは、風に左右されるが、昼夜を問わず、どんな隙間にも入り込む。 
					中生代の恐竜の全盛期には、ヒトの先祖である哺乳類は、夜に活動し、光のないところで嗅覚を発達させて、生き延びた。
					陸上動物だけでなく、海生動物を含めても、視覚器が退化した動物は多いが、聴覚器を持たない動物は少ないという。
					それ以降、動物は、ヒトのような視覚型と、イヌのような嗅覚型に大別される<a href="#rIN">[rIN]</a>。
				</p>
				<p>
					やがて、恐竜がいなくなり、類人猿が森から草原に降りてきた。
					そのころ、ヒトの先祖である狭鼻猿類は、それまでの赤・青の2色視でなく、
					赤・青・緑の三色視ができるように進化し、弁別できる色情報を格段に増やし優位に立った
					<a href="#rMK">[rMK]</a>。
				</p>
			</details>

			<details open><summary><strong>ヒトの視覚の高性能の仕組み</strong></summary>
				<p>
					光刺激を瞬時に処理するため、ヒトの視覚は、情報圧縮と並列処理を活用している。
				</p>
				<p>まず情報圧縮である。</p>
				<ul>
					<li>
						脊椎動物の視細胞は、錐体(cone)と粁体(rod)の2種類ある。
						粁体(rod)は明暗に反応し、錐体は異なる波長の光(色)に反応する。
						ヒトでは、網膜に中心窩(fovea)と呼ばれる錐体だけが密集した部位があり、この部分での視覚が視野の中心部となる。
						人間には錐体が約600万個、 粁体が約1億2000万個存在する。
						一方、視細胞の情報を受け取る視神経は約100万個である。従って網膜は光刺激の情報を1/100ほどに圧縮して脳に送っていることになる。<a href="#rST">[rST]</a>
					</li>
					<li>
						視覚は、光刺激を電気信号へ変換して処理する。刺激が同じならば、神経内で電気信号が発火されない。
						つまり、同じ映像であるかぎり、刺激としての画像情報は消える。
						そこで、眼球を不随意に微動させて、注視したときの網膜像を絶えずリフレッシュしている。
						そして、網膜像の時間的差分だけを脳へ送るというデータ圧縮を行っている。
						<a href="#rSO">[rSO]</a>
					</li>
				</ul>
				<p>ヒトは瞬時に目から多くの情報を把握できるが、いくつかの並列処理を行っている。</p>
				<ul>
					<li>
						視細胞のレベルでは、錐体は色や空間的情報処理、杆体は明暗情報処理･時間的情報処理を分担している。
						また、注目しているところの注目視と、別に周辺視が、独立に機能している。
						歩きながらスマホを見ることができるのもそのせいである。
						明暗の変化や運動など時間的な変化を伴うものが、周辺視野で感知される。
						その後、注意すべきかどうかの判断のために、眼球運動で注目視し、知覚・認知が行われて、対処が判断される
						<a href="#rFT">[rFT]</a>。
						周辺視野には以下の特徴がある。
						<ul>
							<li>
								周辺視野は、素早く反応する。
								恐ろしい物体を中心視野で観察すると脳が反応するまでに140-190ミリ秒、周辺視野では80ミリ秒かかるという<a href="rSW">[rSW]</a>。
							</li>
							<li>
								周辺視野は、詳細認識ではなく、状況の概略を把握するのに使われる。
								台所の写真で、写真の周辺部を隠すとどこの写真かわからなくなるが、中央部を隠してもどこの写真が想像できる<a href="rSW">[rSW]</a>。
							</li>
						</ul>
					</li>
					<li>
						さらに、視覚情報は、3つルートで処理される。まずは大きく2つに分かれる。
						<ul>
							<li>
								一つは、脳へ情報が送られて眼球運動を制御するのに使われる。
							</li>
							<li>
								もう一つは、パターン認知に使うため脳（大脳皮質第1視覚野、視覚前野）へ送られる。
								後者は、網膜上の位置に依存した情報抽出を行ったあと、位置に依存しない空間・携帯情報を抽出する。
								まず、線分の方位、長さ、色、動き、両眼視差などの特徴に選択的に反応し、網膜上の位置に依存した要素情報を抽出する。
								そして、そこから更に二つのルートに分割され、信号が送られる。
								<ul>
									<li>空間知覚処理部（側頭連合野）</li>
									<li>形態知覚処理部（頭頂連合野）</li>
								</ul>
								そこでは、位置に依存しない、人の顔の認識などパターン処理が行われる。
							</ul>
						</ul>
					</li>
					<p>
						このように、視細胞から大脳にかけて、階層的な処理が並行して動く。
						<a href="#rSO">[rSO]</a>
					</p>
				</ul>

			</details>

			<details open><summary><strong>認知的なショートカット</strong></summary>
				<p>
					視覚認知レベルでも、生物的な効率化の仕組みを持っている。
					以下にカニッツアの三角形という図がある。
					ヒトは、知覚した情報を処理する際、すでに記憶に持っている認知パターン分類で解釈する<a href="rSW">[rSW]</a>。
					素早く対象を理解するための効率化である。
				</p>
				<p>
					<div align="center">
						<img width=30% src="Kanizsa_triangle.png"> 
						<p>[https://ja.wikipedia.org/wiki/カニッツァの三角形より]</p>
					</div>
				</p>
				<p>
					また、ヒトは、実世界の3D物体を、すでに記憶している基本的な立体（ジオン）パターンを組み合わせて識別しているという<a href="rSW">[rSW]</a>。
				</p>
			</details>

			<details open id=eyeball_movement><summary><strong>眼球の動き</strong></summary>
				<p>
					眼球は、直径24mmから25mmの球体である。
					眼窩の中で、脂肪に囲まれて、3対6種類の筋肉で支えられ、上下、左右、視軸回りの回転運動を行う<a href="#rFT">[rFT]</a>。
					それが、動体視、立体視を支えている。
				</p>
				<p>
					<div align="center">
						<img width=80% src="eyeball_movement.jpg"> 
						<p>[https://plaza.umin.ac.jp/jikei-np/symptoms/01_01_13.htmlより]</p>
					</div>
				</p>
				<p>
					中心視をするために，左右の眼は連動して動く。
					2種類の連動運動がある。
					一つは、移動する対象を追う運動で、両眼は同じ方向へ運動する（共同運動、conjugate）。
					これには滑らかな成分（最高25から30度/秒）と跳躍性の運動成分（ザッカード、300から600度/秒）がある。
					<a href="#rFT">[rFT]</a>、<a href="#rSO">[rSO]</a>
				</p>
				<p>
					もう一方の連動運動は、両眼が逆方向に運動する（幅そう運動、disjunctive, vergence）。
					これは、左右のわずかに異なった像を融合して一つの像として知覚し、
					立体視のデータを得るためにある。
					ヒトは、三次元の世界に住んでいるのである。
				</p>
				<p>
					一方、眼球は、固視微動といって、注目視野内の微小な不随意の動きを行い、静止した物体の網膜像が消えないようにしている。
				</p>
				<p>
					ヒトの睡眠には、レム睡眠という、夢を見て、目を激しく動かす時間帯があり、
					脳の中を整理しているらしい<a href="#rYT">[rYT]</a>。
					ヒトは、頭を整理するために、目を動かす。
					そのくらい、ヒトは視覚的動物だということだろうか。
				</p>
			</details>

			<details open><summary><strong>視覚は時間も見ている</strong></summary>
				<p>
					視覚は主に空間的情報を感知する。
					さらに、対象の動きという時間的な情報も感知する。
					雲は風に流される。遠くから見ればその動きは微々たるものである。
					しかし、ヒトの目は、雲が連続的に確実に動いていると感知する。
				</p>
			</details>

			<details open><summary><strong>文字の読み取り速度がすごい</strong></summary>

				<p>
					ヒトの視覚は、その神経の高速並列処理と機敏な眼球筋肉のおかげで、言語認知で高度なパフォーマンスを示す。
				</p>
				<p>
					注目視野は、20から30ビットを一度に把握できると言われる。
					これはアルファベットは5文字、ひらがなは4文字、漢字は2文字に相当する
					<a href="#rFT">[rFT]</a>。
					英語の場合、一度に、15文字を読み進むという。
					先頭の1から7文字で意味を取り、次の8-15文字は周辺視野でみている（予測している）という
					<a href="#rSW">[rSW]</a>。
					またヒトは、聞き取りであれば1分間に160語ほどを把握できるが、読み取りは1分間に300語ほど把握できる。
				</p>
				<p>
					一方、耳で聞き取るためには、音は時系列で並んでいるので、それらを逐次処理しないといけない。
					聞き取りが、読み取りよりも遅い主因である。
				</p>
			</details>

			<details open><summary><strong>空間的に記憶できる</strong></summary>
				<p>
					動物は、餌の場所や住処をめぐって行動する。
					そのために、周囲の空間と自身を関連付ける認知機能、つまり空間記憶がある<a href="#rUT">[rUT]]</a>。
					聞いたことを記憶するのと比べて、見えたものは繰り返し確認できる。それが記憶保持を助ける。
				</p>
				<p>
					色分けした表紙のファイルを見て、どのファイルがどの内容のファイルだったかわかる。
					ある事柄が、本の分厚い厚みの中でどのあたりのページのどのあたりに書いてあったか、覚えている。
					机の上に、いくつも書類が乱雑に積み重なって置かれているが、どこに何があるか思い出すことができる。
					ピアノである曲を暗譜で弾くことが出来る。
				<p>
					このような空間記憶は、連想記憶にも似ている。
					キーボード操作に慣れた人は、キーボードを見なくても操作できる。
					しかし、キーボードがないところで、キーボードのキー配列を思い出そうとするとできない。
					つまり、頭にそのまま記憶しているのではないのである。
					しかし、キーボードを見ると、指が覚えていたかのように、すぐに上手にタイプできる。
					ここには視覚的な連想記憶が働いている。
					記憶は神経細胞の結合パターンとして保持される。
					何度もキーボードを見て触っていると、神経細胞結合の発火の痕跡が残る。
					キーボードを見たという刺激だけで、あるキーがこの辺にあったよなという残りの記憶が活性化される。
					これが、ドナルド・ノイマンが「外部知識」と呼んだもの<a href="#rDN">[rDN]</a>の正体である。
				</p>
			</details>

			<details open><summary><strong>構造を把握できる</strong></summary>
				<p>
					視覚は、いくつかの情報を同時に把握できる。
					そのため、ヒトの抽象的な概念のうち、構造的な情報を認知できる。
				</p>
				<p>
					例えば、一つの画面に、やることのTODOリストがあるとする。
					1個目と2個目を比べて、どっちを先にやるか考えているとき、目は二つの項目を認知し、作業記憶の中において比べている。
					例えば、来週の出張の飛行機を予約しているとする。
					WEBの画面を見て、日時と概略出発時刻を入力し、どの会社のどのフライトにするか選択肢がでて、どれにするか検討しているとする。
					ヒトの頭の中では、日時と概略出発時刻とともに、飛行機会社のブランド名も一緒に意識している。
				</p>
				<p>
					もっとも、この視覚は構造を把握できるために、視覚向けにデザインされた下手な情報は、複雑になってしまうこともある。
				</p>
			</details>

			<details open id=visual_symbol><summary><strong>象徴を認識できる</strong></summary>
				<p>
					ヒトは言語を駆使する。音響の連鎖が、音響言語として、複雑な概念を伝える。
				</p>
				<p>
					一方で、ヒトは、視覚的な形状でほかの何かを象徴することも行ってきた。
					壁画、象形文字、表意文字、アイコン、ピクトグラムなどである。
					表意文字が言語の要素であるように、視覚的な象徴の表現力は、言語に比べて、限られている。
					言語は、感覚を指示したり、実在物を指示したり、環境を自在に利用できる。
					言語は、構文構成で概念を自在に組み合わせる表現力がある。
					しかし、視覚的な象徴は、写実画とかでもなければ、あるいは明示的な固有名詞の文字列でなければ、特定の実在物を指示する力はない。
					概念の組み合わせも、並置くらいしかできない。
					視覚的な象徴は、表現力が限定される。
					視覚的な象徴は、間接的である。
					コンピュータのアイコンも間接的であるが、その限界が忘れられている。
				</p>
			</details>

			<details open><summary><strong>2次元と3次元</strong></summary>
				<p>
					ヒトが観察する世界は、3次元の立体である。
					両眼視によって、片眼からの2次元画像をもとに、脳内で3次元イメージを構築している。
					その3次元の世界は、自分の位置を変えれば、同じものでも、刻々と異なって見えてくる。
					異なった3次元の外見のものを同一の物体だとみなせるのは、ヒトの抽象化能力のためである。
				</p>
				<p>
					一方、ヒトは、3次元に現象する物体を、紙という道具の2次元空間に表現したりもする。
					3次元のものを相手にするより、2次元のものを相手にするほうが、認知エネルギーは小さい。
					それで、紙を持ち歩いたり、別のところで3次元を再現したりする。
					見た心象を、2次元空間に表現して、絵、漫画、浮世絵などとして、仲間に感情を伝えたりもする。
					そこでは、ヒトは、認知的に楽な情報を通して、実は高度な精神活動をしている。
					現在のコンピュータが、モニターやタッチ画面という2次元空間を駆使するのも、自然です。
				</p>
			</details>
		</div>

		<div><h4>2.2.2 聴覚</h4>

			<details open><summary><strong>聴覚の能力</strong></summary>
				<p>
					ヒトの聴覚は、空気の振動を感知する遠隔感覚である。 空気の振動は、距離によって、また障害物があると減衰しやすい。
				</p>
				<p>
					ヒトの聴覚受容細胞の数は23,500個（視細胞は1億以上）、聴神経は3万本（視神経は100万本）である。
					<a href="#rHK">[rHK]</a>
				</p>
				<p>
					視覚がおおむね空間情報を処理するのに対し、聴覚はおおむね時間的情報を処理する。
					会話の相手の話し声、赤ちゃんの泣き声、音楽のリズムやメロディ鳥の鳴き声、など、時間的な流れの中に存在する。
					聴覚は視覚に引きずられもする<a href="#n03">[脚注03]</a>。
				</p>
				<p>
					精度は視覚より低いが、両耳により、距離と方向を感知する音源定位ができる。
					車を運転中に、サイレンの音が聞こえる。どちらの方向からを確かめるために、よく聞こうとする。
					野鳥の鳴き声がきこえてきたら、どこにいるか方角の見当をつけて探す。
					ただ、目でそれを確かめないと、満たされない気がする。
				</p>
			</details>

			<details open><summary><strong>聴覚は発話に連動する</strong></summary>
				<p>
					聴覚に障害のある人は発話障害を伴うことが多い。
					情報は、言葉を聞く聴覚器官から、発話するための調音器官へ流れ、自分の音声を聞きながら発話する
					<a href="#rSO">[rSO]</a>。
					目から得た情報に基づき、手指が動くように、聞いた音に基づき、発話するのである。
					ヒトの聴覚は、音から危険を察したり気配を感じたりもするが、発話と一体となって音響言語を駆使するコミュニケーションの場面が、
					主な活躍場所に見える。
				</p>
			</details>

			<details open><summary><strong>聴覚認知は構造保持が苦手</strong></summary>
				<p>
					音響的な言語情報は、時系列情報である。そして、音声は消え去る。
					短期記憶に入っても、再確認できないし、しばらくたつと消える。
					そのため、ヒトは、聴覚からは、少ない量しか情報を把握できない。
					複数の情報を安定的に保持できない。
					そのため、聴覚は、フォームや手順といった複数の情報からなる構造の把握が苦手である。
				</p>
			</details>
		</div>

		<div><h4>2.2.3 嗅覚</h4>

			<details open><summary><strong>ヒトは嗅覚を退化させた</strong></summary>
				<p>
					嗅覚は、揮発性ないし水溶性の化学物質を感知する。味覚は、同じ化学物質でも接触感覚である。
					触覚という物理的な刺激に比べて、分子の組み合わせレベルの感覚であるため、嗅覚が発達すれば識別できる種類は膨大になる。
					しかし、ヒトは視覚という、より遠方まで感知する感覚を優先した。
				</p>
				<p>
					ヒトの嗅覚受容体数（種類数）は396個あり、その組み合わせで数十万種類のにおいを感知する。
					遺伝子レベルで見ると、哺乳類の嗅覚に関係する遺伝子は大きなファミリーをなしていて、動物にとって環境探知の重要な手段であることが示されている。
					また、匂い情報は、大脳の感情や記憶をつかさどる部分に流れ、内分泌で即座に反応できるようになっている。
					素早い反応によって生存するために重要な機能を果たしていた時代の名残である。
					その由来によって、匂いは長期記憶を呼び覚ます。
					<a href="#rTK">[rTK]</a>
				</p>
			</details>
		</div>

		<div><h4>2.2.4 体性感覚</h4>

			<details open><summary><strong>内外を感知する</strong></summary>
				<p>
					体性感覚器は外部を感知するものと内部の固有の情報を感知するものに大別される。 
				</p>
				<ul>
					<li>
						外部を感知するものは皮膚感覚である。
						ヒトの場合、皮膚は1.8平方メートルもあり、皮膚感覚の受容器は散在している。
						皮膚表面には触覚があり、皮膚の深部には圧力を感じる圧覚がある。
						その他、温覚、冷覚、痛覚の受容器がある。 
						<a href="#rIN">[rIN]</a>
					</li>
					<li>	
						一方、筋、腱、関節などに、自分の状態を感知する固有受容器がある。
						筋がどれだけ伸びているか、どれだけの力で引っ張られているか、角度はどうか、を感知している。
						これは姿勢を制御したり体を動かすためにある。
						<a href="#rIN">[rIN]</a>
					</li>
				</ul>
			</details>
			
			<details open><summary><strong>指先は鋭い</strong></summary>
				<p>
					皮膚感覚受容器は10の7乗個あり、神経は10の6乗個ある。指先には1平方ミリメートル当たりに1個の神経線維が大脳に接続している。
					2点を弁別する能力は、手のひらでは1センチメールだが、指先は2ミリメートルである。
					<a href="#rMS">[rMS]</a>
				</p>
				<p>このような手指の神経の細かさが、ヒトが箸やハサミを上手に扱う基礎となっている。</p>
			</details>

			<details open><summary><strong>脚注</strong></summary>
				<ul>
					<li id=n03>[03] gaを発音する唇を見せて、同時時にbaと言う音声を聞かせると、被験者は中間の調音位置をもった「da」を知覚する。
						これをMcGurk効果という。
						<a href="#rSO">[rSO]</a>
						聴覚情報は、知覚レベルになると視覚情報と融合、ないし引っ張られて、知覚される。
				</ul>
			</details>
		</div>

	</div>

	<div><h3>2.3 効果器官</h3>

		<div><h4>2.3.1 手、骨、筋</h4>

			<details open><summary><strong>手と脳の進化の相乗効果</strong></summary>
				<p>
					手、骨、筋という効果器官は、内部状態を感知しつつ、機械動作する。
					手には優れた皮膚感覚がある。
					一方で、視覚は手の動き周辺を観察するのに強力な機能を備えている。
				</p>
				<p>
					手の重要な役割は、人類史から見ると理解できる。
				</p>
				<p>
					320万年ほど前、ルーシーと呼ばれる化石により、類人猿が二足歩行を始めたことがわかっている。
					骨盤の形から二足歩行していたことが分かったが、脳の大きさはチンパンジーと変わらなかった。
					二足歩行のメリットは、四足歩行に比べて25%のエネルギーで移動できることだった。
					その後、240万年ほど前に、ホモ・ハピルスが登場し、肉を骨からそぎ落とす鋭利な石器を作り利用していた。
					ホモ・ハピルスの脳はルーシーの倍（ホモ・サピエンスの半分）に大きくなっていた。
					道具を作るには目と手を正確に連動させる必要がある。それが脳に刺激となり、脳の発達を促した。
					一方、大きな脳はたくさんのエネルギーを必要とする。たくさん食べる必要があり、狩猟してとった肉を食べた。
					肉をたくさん得るには、より高度な道具が必要となる。
					こうして、手と脳の進化の相乗効果の連鎖が始まった。
				</p>
				<p>
					脳はますます大きくなり、手指はますます器用になり、道具はますます精緻になった。
					200万年前ごろ、ホモ・ハピルスは、ホモ・エレクトスへ進化した。脳の大きさは、ホモ・ハピルスの1.5倍となった。
					ホモ・エレクトスは、槍を使い、火で食物を消化しやすいように変えられた。
					100人くらいの集団で暮らしていたとされる。
					170万年前頃に、ホモ・エレクトスはアフリカを出て、別々の地域に進出し、5種類のヒト属に分化したらしい。
					その後35万年前ほど、ホモ・サピエンスと同じくらいの脳を持つネアンデルタール人が現れた。
					頭蓋骨の底に発声に必要な神経の束を通す穴があり、多様な発声ができたと想定されている。
					また、ネアンデルタール人は、音楽、宗教、言語を持っていた。
					その後、19万5000年前に、ホモ・サピエンスが登場した。
					ホモ・サピエンスは、7から5万年前に、アフリカから出て世界中へ広がった。
					<a href="#rCL">[rCL]</a>
				</p>
				<p>
					<div align="center">
						<img width=40% src="brain_capacity_of_primates.png"> 
						<p>[http://user.keio.ac.jp/~rhotta/hellog/lib/brain_capacity_of_primates.pngより]</p>
					</div>
				</p>
				<p>
					以上のように、手によってこそ、脳が大きくなった、と言われる。
					ほかの説もある<a href="#n04">[脚注04]</a>。
				</p>
			</details>

			<details open><summary><strong>手指は器用</strong></summary>
				<p>
					ヒトの手は、器用である。
					ヒトの手には5本の指があり、指は3つの関節を持ち、動く。
					そして、親指は、ほかの指と独立して動き、ほかの指と対面して動作し、物をつかんだりできることできる。
					この対面動作できることが、ヒトの手指の特徴である。	
				</p>
				<p>
					<div align="center">
						<img width=40% src="fingers.png"> 
						<p><a href="#rYH2">[rYH2]より</a></p>
					</div>
				</p>
				<p>
					手指ができる運動は、以下のように分類されている。そに多様さに驚く。
					<ul>
						<li>握る(grip)</li>
						<li>つまむ(pinch): 指先(tip)、指腹(pulp)、側面(lateral)、ひっかけ(hook)、指間はさみ(finger)</li>
						<li>ねじる(twist)</li>
						<li>押す(push)</li>
						<li>すくう(scoop)</li>
					</ul>
					これらの組み合わせで、日常生活を送っている。
				</p>
				<p>
					<div align="center">
						<img width=40% src="finger_behavior.png"> 
						<p><a href="#rYH2">[rYH2]より</a></p>
					</div>
				</p>
			</details>

			<details open><summary><strong>身体と視覚が手指の器用さを支えている</strong></summary>
				<p>
					手指は、体幹が支えて身体がある姿勢をとって、肩や腕が支え、腕・肘・手首が動き、手首が支えることで、はじめて器用に動く。
					体性感覚と機械的効果器が、手指の器用さを支えている。
					まさに、手指の器用さは、全身運動の結果である。
				</p>
				<p>
					手指は、視覚に障がいがある方を除き、多くの場合、目に助けられてこそ機能する。
					例えば、ドアノブの位置を目にしながら、そこをつかんでひねりドアを開く。
					そこに、ドアを開けるという意図で行動した点に、あいまいさはない。
					例えば、左手に持った茶碗の位置を、目で感知しながら、右手の箸を動かして、茶碗のごはんを摘まみ上げる。
					例えば、机の上に置いた紙を左手で押さえて、その四角い方向を目で確認したうえで、右手のはさみで紙の一片を切り取る。
					例えば、キーボードがあってキートップの文字マークを見て、手指は動く。
					音声だとbaなのかpaなのかわかりにくいこともあるが、
					手指の動作は、目でキートップを確認しているため、コンピュータに対して、曖昧さなくpaという言語表現がされる。
					このように、手指は視覚と一緒に働くことで、様々な機能を果たす。
					ヒトの構築した人工的な構築物の中を動き回る際も、その手指の動作に曖昧性はない。
				</p>
			</details>

			<details open id=fitts_law><summary><strong>手指は距離に束縛される</strong></summary>
				<p>
					手を動かし、マウスで、別の場所のターゲットに移すという運動負荷に関し、
					その移動時間は、移動距離が大きいほど大きく、対象の大きさが大きいほど小さくなる。
					これをフィッツの法則という。
					手指は、機械動作なので、この距離の束縛を受ける。
				</p>
				<p>
					例えば、受信箱のメールのTriageを取り上げてみる。
					メールをざっと眺めて丁寧に読みたいメール以外は即削除する作業である。
					スマホでなら、指先の操作ですむので、素早くできる。
					しかし、デスクトップであると、画面が大きく、手から離れているので、マウスごしにGUI対象をポチポチするする必要がある。
					読む場所を設定する画面上の位置と、削除ボタンは離れているので、いちいちマウスポインターを移動する手間がある。
					そのため、メールのTriageは、スマホに比べて、デスクトップPCでの作業には、時間がかかる。
					手指の操作は、このように、距離の束縛を受ける。
				</p>
				<p>
					一方、視線の移動は、距離の束縛を受けない。
					移動は一瞬である。
					また、音声動作は、マイクが音を拾う限り、
					スマホでもデスクトップでも同じスピードでテキスト指定動作ができる。
					音の聞こえる範囲ならば、距離の束縛を受けない。
					現在のコンピュータのユーザインターフェイスは、ヒトが手指で道具を操作するという伝統にあるので、
					こういったヒトの体の特性が無視されている。
				</p>
			</details>
		</div>

		<div><h4>2.3.2 発声</h4>

			<details open><summary><strong>発声器官の進化</strong></summary>

				<p>
					ヒトの音声産出は、横隔膜・肺という呼吸器官から空気を吐き出し、咽頭・声帯で音源を作り、
					舌・咽頭・口唇で音に変化を与える、と3つの器官要素によって生産される。
					<a href="#rKH2">[rKH2]</a>
				</p>
				<p>
					<div align="center">
						<img width=50% src="resonance_cavity.jpg"> 
						<p>[https://band-knowledge.com/vocal-14/より]</p>
					</div>
				</p>
				<p>
					脊椎動物が陸に上がって肺呼吸をするに伴い、空気の振動を起こす能力が、生殖や警告などに利用されて進化したようである。
					一方、舌は、食べ物を飲み込むときに、精緻に、素早く動くように進化していた。
					咽頭を含む声道は、食べ物の摂取・嚥下を、不随意的に担うとともに、ヒトでは随意的に話し言葉を発声する役割を持つ。
				</p>
				<p>
					ヒトは、320万年前に直立歩行を始めた。そして、火を使うことで、やわらかい食事をとるようになった。
					そして、頭の重量のバランスをとるため頭の前後径が短くなり、柔らかい食事による顎の縮小とあいまって、脳が前にせり出し、
					舌は、前後に圧縮されて上下に厚みを持ち、丸い形状になった
					<a href="#rFJ">[rFJ]</a>。
					そして、40万年前ほど、ヒトの咽頭の位置が下がった。そして、喉に大きな空間をつくり、多様な共鳴を生み出せるようになった。
					ヒトの声道は、口腔と咽頭腔という二つの共鳴腔がほぼ垂直に結合していて、それぞれ独立に変形させることができる。
					ヒトの舌は、丸みがかっていて、形状変化で多様な音調整を可能にした。
					また、ヒトは、発話の際、1秒間に5〜6回、口が開閉する。
					<a href="#rKH3">[rKH3]</a>。
				</p>
				<p>
					ヒトは話をする時に、一回息を吐くという瞬間で、多くの異なる音を発声するために
					声道形状を連続的に素早く変形させることができる点が、特異であるとされる
					<a href="#rNT">[rNT]</a>。
				</p>
				<p>
					また、ヒトは成長に伴って、言語を習得し、多様な音声生産ができるようになる。
					この音声生産の可塑性ないし学習という点も、特徴的である
					<a href="#rKH2">[rKH2]</a>。
					それを可能にする神経系の進化が、別途、背景にあったらしい。
				</p>

			</details>

			<details open><summary><strong>テキスト生産速度は指の5倍</strong></summary>
				<p>
					ヒトの言語表現のスピードは、経験的におおよそ、
					しゃべりなら160語/分、手書きは30語/分、タイプなら40語/分である。
					しゃべりは、社会的な環境で育った健常者ならば誰でもできる。
					一方、手書きは教育が必要であり、キータイプなら機器操作に慣れが必要である。
					音声での言語表現は、習熟の必要がないばかりか、指よりも5倍速い。
				</p>
			</details>

			<details open><summary><strong>脚注</strong></summary>
				<ul>
					<li id=n04>[04] 養老先生による<a href="#rYT">[rYT]</a>と、脳が大きくなったのには、手との相互作用ではなく、内在的な理由があるという。
						知覚系の末端となる脊髄神経細胞は、発生の段階では成人になったときに残る細胞数の数倍、余剰な細胞があるという。
						ヒト以外の動物では、支配している末梢領域を持たない神経は、間引かれるという。
						ヒトではそうでない。
						ヒトの脳が大きくなったのは、知覚に対応しない神経が、互いに接続して支配領域を自前で持ち、
						かつ、互いに、知覚以外から、入力しあう関係を持ったからだという。
						それが意識だと。
				</ul>
			</details>
		</div>

	</div>

	<div><h3>2.4 脳・神経系</h3>

		<details open><summary><strong>脳の構成要素</strong></summary>
			<p>
				脳は、大脳、小脳、脳幹からなる。
			</p>
			<p>
				<div align="center">
					<img width=50% src="brain.jpg"> 
					<p>[https://atamanavi.jp/169/より]</p>
				</div>
			</p>
			<p>
				<ul>
					<li>大脳：
						大脳は、高度な機能をこなす。大脳には、皮質、辺縁系(海馬、偏桃体など)、基底核がある。
						<ul>
							<li>皮質
								<ul>
									<li>
										皮質の前頭部は、作業記憶を含んで認知にかかわり、ヒトらしい社会的な感情コントロールに関与する。
										ほかの脳の部分からボトムアップに情報が流れ、統合し、逆に各部へトップダウンに情報を流す。
									</li>
									<li>
										一次運動野は、脊髄を通して、体の各部の運動を指示する。
										運動前野は、外発的で、視覚情報を使い、運動を準備し構成する。手で握ったり、手で届いたり。
										補足運動野は、内発的で、記憶に基づき、両手の協調を図る。
									</li>
									<li>
										頭頂葉は、体性感覚野を含む。
										体の部位に即した一次体性感覚皮質から、部位にまたがる高次体性感覚皮質へと、階層的になっている。
									</li>
								</ul>
							</li>
						</ul>
					</li>
					<li>小脳：
						小脳は身体運動、発声の協調およびバランスを調整する。例えば、足の動かし方を意識せずに歩いたりできる。
						首が回転したときに、反射的に眼球が逆回転し、視界のブレをなくす。小脳は、運動パターンを学習するといわれる。
					</li>
					<li>脳幹：
						脳幹は、脊髄からシグナルを伝達して、基本的な体内の機能と反射を指令する。
						爬虫類脳とも呼ばれ、生命維持・繁殖に必須な機能を果たす。
						例えば、呼吸や心臓の拍動など不随意・自律的な機能をこなす。
						脳幹は、中脳、間脳（視床、視床下部など）を含む。
						間脳は、感覚を仲介し、感情を管理し、全体の内部システムを統括する。 
					</li>
				</ul>
			</p>
		</details>

		<details open><summary><strong>小脳は所作の匠</strong></summary>
			<p>
				大脳の皮質には神経細胞が140億あるが、小脳には1000億個ある。
				<a href="#rMA">[rMA]</a>
				小脳は、ヒトでは脳全体の15%程度の容積しかないが、脳全体の神経細胞の約半分が存在する。
				末梢感覚器や大脳皮質からの入力を受けて、運動を学習する。
				<a href="#rHN">[rHN]</a>
				鋏を扱うときの動作、箸を扱うときの動作、話す動作、歩く動作、大工の技、庭師の技、などにこそ、目立たないが、実は大量の知能が、詰まっている。
				ヒトを知識、いわば見える氷山の一角、で比べたりする。
				が、それ以前の、目立たない、生活のための知能のほうが、はるかに大きく、ヒト共通である。
			</p>
		</details>

		<details open><summary><strong>手は第2の脳である</strong></summary>
			<p>
				大脳皮質には機能が特定の場所に関連付けられる箇所がある。
				運動野を見ると、発声関係と、手指の部分が大きいことに気づく。
				発声関係は言語機能と関係して大きな部分を占めるのは理解しやすい。
				一方、手は第2の脳といわれるほど、大きな運動知能を持っている。
				<p>
					<div align="center">
						<img width=60% src="cerebral_penfield_map.jpg"> 
						<p>[https://www.akira3132.info/cerebral_cortex.htmlより]</p>
					</div>
				</p>
			</p>
		</details>

		<details open id=association_area><summary><strong>ヒトは総合する動物である</strong></summary>
			<p>
				感覚野と運動野という機能局在な部分を除くと、大脳皮質の約2/3にも相当する広い領域が、連合野と呼ばれる。
				連合野は、高次な脳機能を具現化している皮質領域である。
				感覚情報の高度な統合による認知、複数の感覚の総合、感覚と運動の統合、過去の経験（記憶）と関連、随意運動、
				情動行動、言語機能、精神機能、作業記憶（ワーキングメモリー）などである<a href="#rKW">[rKW]</a>。
				個々の感覚・運動機能よりも、それらを統合した部分が、ヒトのヒトたる部分である。そこが、ヒトの特徴である。
			</p>
			<p>
				現在のコンピュータのユーザ・インターフェイスは、手で操作し、目で知覚するのが、主である。
				しかし、このような特定のチャネルだけに頼るやり取りは、ヒトの脳の進化結果に照らしてみると、ヒトの生体機能に反している異常なことである。
			</p>
		</details>

		<details open id=magical_number><summary><strong>短期記憶の容量は4個まで</strong></summary>
			<p>
				一度に記憶できるのは、4個までだそうである。
				長期記憶から、あるカテゴリーで想起できるのは、3個までだそうである。
				チンパンジーは、4個の数字までは95%の正確さで覚えられるが、5個で65%まで落ちるそうである。
				また、ヒトは、選択肢が3つか４つまでの場合に限り、選ぶことができる<a href="#rSW">[rSW]</a>。
			</p>
			<p>
				これ以上の選択肢が、電子機器のリモコンやコンピュータ・ユーザ・インターフェイスに、あふれている。
			</p>
		</details>

		<details open><summary><strong>脳の可塑性</strong></summary>
			<p>
				ヒトの遺伝情報であるヒトゲノムは約30億個の塩基で構成される。
				一つの塩基はATGCの4種類の分子のいずれかで2ビットの情報をもつ。
				したがって、ヒトゲノムは2x30憶=60億ビットの情報量を持つ。
				一方、新皮質の神経細胞は100億個で、神経細胞当たりシナプスの結合数を1000個とし、結合係数は抑制と興奮の2値（1ビット）とすると、
				脳の情報量は約10兆ビット（100億X1000=10兆ビット）となる。
				ヒトが生まれる際に受け継いだ情報量よりも、ヒトが成人になって活用する情報量のほうが、1600倍大きいのである。
				つまり、脳の配線の大部分は遺伝ではなく、生後決定される。
				<a href="#rSO">[rSO]</a>
				つまり、ヒトは学習する。
			</p>
			<p>
				遺伝して得た情報量よりも、後天的に獲得する情報量が、1600倍というのは信じがたい。
				新しい知見が今後出てくるだろう。
				しかし、後天的な事柄、社会や文化というヒトの抽象物が、ヒトの生物としての生得物と比べて、大きな比重を持つという事実は、驚きである。
				これは、ヒトが今後どのような未来を持つことになるのか、生物的な進化よりも、もしかして社会的な思考の成果が、断然影響力を持つかもしれないことを、示唆する。
				ヒト集団は、生物の遺伝子の論理を超えて、歩み始めるかもしれない。
			</p>
		</details>

		<details open id=cognitive_interference><summary><strong>マルチタスクは実は時分割</strong></summary>
			<p>
				ヒトは、歩きながら、ものを考えることができる。
				今、そういう無意識的な行動でなく、ある程度注意力を要する作業のことを想定する。
				ヒトは、考えるという音響的言語活動と、文をタイプするという指の機械的・視覚的・空間的な言語活動を、
				うまく同時にこなせるように見える。
				活動と資源の関係のモデルを<a href="#rCW">[rCW]</a>が提唱した。
			</p>
			<p>
				<ul>
					<li>
						知覚・認知のための資源と、反応の選択と実行の資源とは、独立である。
						例えば、パイロットは飛行機の込み具合を認識しながら、同時に適切な対応をとる。
					</li>
					<li>
						音響的作業と視覚的作業とは、それぞれの作業を複数やるよりは、2つの異なる種類の作業をより効率的に時分割できる。
						例えば、何か文章を考えているときに他人とおしゃべりはできない。
						しかし、車の運転手は、声で指示を受けて、ハンドル操作ができる。
					</li>
					<li>
						周辺視野と注目視野とは、異なる資源を使う。例えば歩きスマホができる。
					</li>
					<li>空間的過程と、音響的過程とは、効率的に時分割できる。
						例えば、運転中、ハンドル操作以外の手操作は運転を中断させるが、声で機器操作するのはより楽である。
						耳で捕まえた議論のキーワード（音響的言語活動）を、記録する（空間的言語活動）ことができる。
					</li>
				</ul>
			</p>
			<p>
				最近の研究では、ヒトが一度にできるのは一つだけである<a href="#rSW">[rSW]</a>とわかったという。
				ヒトは、素早く切り替え、時分割して、二つの作業を同時にこなしているように見えるらしい。
				時分割できるとはいえ、
				自動車の運転をしながら、着信した電話で会話をするのは、実は注意力を散漫にしている。
				音楽を聴きながら、勉強したり、仕事をしているのは、実は、作業効率を下げている。	
			</p>
			<p>
				<a href="#rYT">[rYT]</a>では、言語活動に、視覚的言語活動と、音響的言語活動とがあるという。
				音声言語と文字言語といってもよい。
				頭で、数を数えながら（音響的言語活動）、しゃべること（音響的言語活動）はできない。
				しかし、頭で数字カードで数字が増えていくのをイメージしながら（視覚的言語活動）、しゃべること（音響的言語活動）はできる。
				つまり、視覚的言語活動と、音響的言語活動とは、効率的に時分割できる。
			</p>
			<p>
				ヒトの言語活動は、視覚と音響とで効率的に時分割できる。
				ヒト対ヒトのコミュニケーションは、特に原初的には主に音響的である。
				一方、ヒト対道具の関係で、視覚的な言語である文字や紙やペンや印刷などは、道具だったとらえることができる。
				ここで、ヒトは、これら音響言語と視覚的言語の両方を、効率的に活用できるであろうことに注目しておく。
			</p>

		</details>
	</div>

</div>

<div class="pagebreak"><hr><h2>3. 言語</h2>

	<details open id=言語の発生><summary><strong>言語発生</strong></summary>
		<p>
			言語の発生については、諸説ある。
			マイケル・コーバリスは、身振り・手振りで表現することが、言語のもとになったと唱えた
			<a href="#rMC">[rMC]</a>。
			身振り・手振り以外に、赤ちゃんでも利用し理解する、表情、まなざし、指さしなども重要なコミュニケーション手段である。
			これらは、空間的な情報伝達手段である。
			ヒトの場合、40万年前ほどのころ、複雑な発生器官を持つにいたり、音声言語を使いだした。
			ヒトがコミュニケーションをとる際、身振り・目振り、手振りという、空間的な表現で弁別してきた意味を、
			生物の進化で体得した器官を活用することで、次第に音響的に表現することを獲得したのかもしれない。
		</p>
		<p>
			養老先生による<a href="#rYT">[rYT]</a>と、言語には時間的言語（音声）と空間的言語（文字)とがある。
			視覚は光という電磁波を感知し、聴覚は空気振動を感知し、独立に進化した。
			視覚は空間の中に時間と無関係にあり、音は時間の中に空間と無関係にある。
			ヒトは、それら異なるものを連合し統一した。
			それが、ヒトにおける言語の発生ということだと。
			養老先生は、視覚的言語と聴覚的言語の連合を、ヒトの大脳の接続でも解説する。
			ウェルニッケ中枢という聴覚的言語中枢がある。
			そこから、神経が伸びて、角回という視覚的言語中枢に入る。
			聴覚的言語中枢と、視覚的言語中枢との、双方から、神経が伸びて、ブローカ中枢という運動性言語中枢に入る。
			つまり、聴覚と視覚からの入力が、音響発生神経に接続している。
			ヒトの言語は、視聴覚が一体なのである。
		</p>
	</details>

	<details open><summary><strong>言語の効果</strong></summary>
		<p>
			言語の効果は明確である。
			ヒトは、動物的に知覚に反応する以上に、抽象的な概念、シンボルを思念する力を持っていた。
			ヒトは、言語によって、概念を組み合わせ、無限の種類の意味を表現し、理解することができるようになった。
			言語は、実在するあるいは抽象的な何かを指示し、構文構造を持って、組み立てる。
			構文構造は、再帰的なので、実質、文の種類は無限である。
			感覚から離れた複雑な概念をも表現できることが、ヒトの世界を広げた。
			虚構がヒト社会の本質となった<a href="#rYH">[rYH]。</a>
		</p>
	</details>

	<details open id=文字の発明><summary><strong>文字の発明</strong></summary>
		<p>
			およそ5000年前のシュメール文明に、ヒトが文字を使った初めての痕跡が残っている<a href="#rCL">[rCL]</a>。
			ヒトは、器用な手指と、それを導く高度な視覚があって、文字を操れるようになった。
			文字によって、ヒトは、時間と空間を超えて、言語を通して、様々な情報を伝達できるようになった。
		</p>
	</details>

	<details open><summary><strong>文字の効果</strong></summary>
		<p>
			それ以来、ヒトのコミュニケーションは、生活圏内の人との会話による日常的なものと、時空を超えた書き物によるものと、2種類のものに分かれた。
			ヒトは、集団を形成するのに、個体数150（ダンパー数）くらいが限界であるという<a href="#rSW">[rSW]</a>。
			この時点で、150人くらいの集団を超えて、コミュニケーションし、何かを共有する道が開けた。
			音声は、近場の人にしか伝わらない。
			しかし、文字によって、時空を超えて伝達されるということは、個体や生活空間の枠を超えることである。
			集団としての文化形成、集団としての教育、集団としての思考の深化、に役立った。
			構成個体数が多くばなれば、その相乗作用によって、いわばネットワーク効果が生まれる。
			インターネットで起きた変化に類似の爆発的な変化が、文字によってもたらされたのだろう。
		</p>
		<p>
			さらに文字は、時間的な次元でも効果をもたらした。
			書き物によるコミュニケーションでは、文字を認識するためにもっぱら目を使う。
			ヒトは、言語に文字という視覚的な要素を合わせた。
			それによって、ヒトは、文字として、外部に永続的に記憶を保持することができるようになった。
			時間とともに流れ去る音声言語と異なる点である。
			紙切れに書いたメモである。
			知識を記録した本である。
			そして、世界中の人に共有されるインターネットである。
			外部的な永続性は、個体にとっても、意義は大きい。
		</p>
		<p>
			外部に視覚的に文構造を保持できるため、言語表現も様変わりした。
			省略がしやすくなった。
			文字の発明の前、ゴータマ・ブッタの弟子が生きた時代、師の教えを受け継ぐには、口伝しか方法がなかった。
			音声表現は、韻を踏んだものだと、覚えやすいという。
			また、教えを伝えるには、言い間違えが起きないように、詳細を何度も繰り返したから、ともいう。
			しかし、文字が利用できるようになると、韻律は関係なくなった。
			また、先に書いたことは省略できるようになった。
			目で、即座に先の文章を確認できるから。
		</p>
		<p>
			視覚的言語なので、一度に多くの言葉を認知することができる。
			永続的な記録は、目で何度も確認できるため、記憶しやすい。
			省略もできるため、記憶効率もいい。
			永続的な記録言語は何度も見直して、思考を深めることができる。
			個人は、望めばいくらでも知識や思考を深めるようになったのである。
		</p>
	</details>

</div>

<div class="pagebreak"><hr><h2>4. コンピュータ</h2>

<h3>7.1 コンピュータという操作道具</h3>

	<details open><summary><strong>コンピュータの恩恵</strong></summary>
		<p>
			ヒトは、動物的に知覚に反応する以上に、抽象的な概念、シンボルを思念する力を持った。
			ヒトは、言語によって、概念を組み合わせ、無限の種類の意味を表現し、理解することができるようになった。
			ヒトは、文字によって、時と空間を超えて、意味を共有できるようになった。
			ヒトは、コンピュータによって、記憶や計算を組み合わせてできることならなんでも、疲れ知らずに、どれほどでも、こなせる相棒をえた。
		</p>
		<p>
			コンピュータでディジタルデータが実在を主張し始めた。
			情報のディジタル化によって、記録、コピー、更新が楽になった。
			情報に対する検索やソートやチェックといった計算機能が難なく利用できるようになった。
			ディジタルになることで、紙の物理的な劣化から解放された。
			ハイパーリンクで、メディア間の関連付けが自由にできるようになった。
			さらに、人が使うコンピュータでネットワークを構成することで、インターネットが生まれた。
			このインターネットによって、ほぼ誰でも情報を発信し誰の発信内容もアクセスできるようになった。
			文字によってはじめられた外部の知識が、ほぼ誰でも生産し共有する巨大な情報倉庫になった。
			誰とでも即座につながり、いつでもどこでも巨大な情報倉庫に自由にアクセスできる。
		</p>
		<p>
			スマホの地図アプリを見てみよう。
			紙の地図と異なり、本屋へ出かけて買ってくる必要はない。
			紙の地図と異なり、スマホをポケットに入れておけば、かさばるものをカバンに入れて持ち歩く必要はない。
			紙の地図と異なり、探したい場所は、ズームインとズームアウトの組み合わせで、一瞬で見つかる。
			メモを付加することができる。
			スマホは、自分がいる場所をGPSで感知しているので、いまここから、目的までの道案内をしてくれる。
			近くのレストランをすぐに探せる。
			そこで人々が感じたことを読み、書気残す。
			近くの道路の混雑具合をすぐに調べられる。
			ヒトは、こんな素晴らしい道具を、だれで手にするようになった。
		</p>
	
	</details>

	<details open><summary><strong>コンピュータの操作のための概念は抽象的である</strong></summary>
		<p>
			コンピュータという道具を使って行うことは、ハサミのような具体性がない。
			いくつかの抽象的なステップをたどる。
			行う操作は、ハサミのように、結果がすぐに出るのではない。
			さらに、結果は、電子的な変化に過ぎず、具体的に見えない場合もある。
		</p>
		<p>
			ドナルド・ノーマンは、行為は7段階の手順を踏むと分析した<a href="#rDN">[rDN]</a>。
			「(1) ゴールがまずある。(2) 実行しようという意図が起きる。(3) 行為系列へ展開する。(4)行為系列を実行する。」 => 外界を操作し、結果が返る
			=>「(5) 外界の状態を知覚する。(6) 知覚したものを解釈する。 (7) 解釈を評価する。」 => (1)へ戻る。
		</p>
		<p>
			<div align="center">
				<img width=25% src="seven_steps_of_action.png"> 
				<p><a href="#rDN">[rDN]</a>より</p>
			</div>
		</p>
		<p>
			コンピュータを使う場合に当てはめてみる。
			行為系列へ展開する(3)がコンピュータ寄りで、ヒトの日常の概念より抽象的である。
			また、結果も抽象的なので、(6)の結果の解釈の段階も、抽象的である。
		</p>
		<p>
			例えば、スマホの通知が鳴ったり鳴らなかったり、変えようとするところ、(1)、(2)は明確である。
			鳴ったか鳴らないか、(7)も明確である。
			一方、どうすれば鳴ったり鳴らなかったりを変更できるのかの(3)が、コンピュータの操作のための概念を理解していないと難しい。
			また、どういう設定になっていればどうなるかを確認する(6)も、コンピュータの操作概念を理解していないと難しい。
		</p>		
		<p>
			こういう抽象的な過程は、どんなユーザーインターフェイスをかぶせても、ヒトには認知負荷がかかる。
		</p>
	</details>

	<details open><summary><strong>理想的な道具とは、誰でもすぐに目的達成できること</strong></summary>
		<p>
			現在のコンピュータは、理想的な道具なのであろうか？
		</p>
		<p>
			ドナルド・ノーマンは、スニーカーのマジックテープを技術の進歩のお手本とした。
			ヒトは、靴の紐を結ぶという習慣に慣れていた。
			マジックテープがでてから、それは一瞬の簡単なことに変わった。
			障がい者、子供も、マジックテープの恩恵を受けた。
			また、自動運転は、別のお手本である。
			車の運転は、手段である。
			目的は、ある場所へ移動することである。
			自動運転は、苦痛なく、目的達成を果たしてくれる。
		</p>
		<p>
			目的とは何か。
			ドリルを買う人は、ドリルが欲しいのではなく、穴が欲しい。
			PC が欲しいのは、PC自体が欲しいのではなく、PCでできるきれいな文章作成できるから。
			またスプレッドシートにパスワードをメモして保存しておけるから。
			スマホが欲しいのはいつでもどこでも情報を検索できるから。
			スケジュールをいつでも変更したり参照したりできるから。
		</p>
		<p>
			道具は、抵抗なく受け入れられ、かつ、中間でない最終目標を達成できることが、理想である。
			コンピュータは、道具として、利用するのに認知負荷があり、理想ではない。
			また、目的達成の中間の手順を、ユーザを強いるので、理想ではない。
		</p>
	</details>

<h3>7.2 ユーザ・インターフェイス</h3>

	<details open><summary><strong>目と手指での操作</strong></summary>
		<p>
			日常的なコミュニケーションを含め、ヒトはそもそも、いろんな知覚を総合して環境とやり取りをしている。
			そして、ヒトが視覚的動物であるといわれるように、主に視覚を使っている。
			現在のコンピュータ・ユーザ・インターフェイスにおいても、情報の入手はもっぱらモニターを目で見ることで行われている。
		</p>
		<p>
			他方、情報の生産は、ほぼ手で行っている。
			テキストをディジタル的に生産するときも、コンピュータに指示（コマンド）するときも、指を使う。
			位置指定・対象選択も、指で行う。
			手を使う限り、手指は機械的動作で効果を生み出すので、<a href="#fitts_law">距離の制約</a>を受ける。
		</p>
		<p>
			しかし、人は操作手段として主に手指を使い、目で見る。
			手は器用である。目と親和性が高い。
			手で操作し、目でモニターする、ということに慣れてきた。
			そのためコンピュータの時代でも、その延長上にいる。
		</p>
		<p>
			しかし、<a href="#association_area">ヒトは総合する動物</a>で見たように、そもそも、ヒトは感覚や運動を特定の部分に頼って生活してはおらず、
			総合して環境に対処している。
			今のユーザ・インターフェイスは、グラフィカル・ユーザ・インターフェイスと言われる。
			それを指で操作し、目で結果を得て、ヒトはコンピュータを利用する。
			コンピュータのグラフィック・ユーザ・インターフェイスは、目と手指での操作に頼りすぎ、いびつである。
		</p>
	</details>

	<details open><summary><strong>コンピュータのユーザインターフェイス設計は、制約が少ないため、難しい</strong></summary>
		<p>
			コンピュータのユーザ・インターフェイスをデザインするのは、水道の蛇口やドアのノブのデザインとは、かなり異なる。
			コンピュータは計算という目に見えない抽象的なレベルで動くからである。
			抽象的な操作によって、抽象的な結果を返すので、ヒトはそれに慣れなければならない。
			どういう操作オプションを提供し、どういう結果提示をすればよいか、それらをデザインするのはアートである。
		</p>
		<p>
			ドナルド・ノーマンによると、日常生活の道具は、物理的な制約、論理的な制約、社会・文化的な制約、意味的な制約などによって、
			操作できることが絞り込まれていると。
			<ul>
				<li>
					物理的な制約とは、例えば、あるプラグはその形状にあったコンセントにしか差し込めない。
					大きな突起は小さな穴に差し込めない。鍵は鍵穴に上下逆さに入れると回らない、などである。
				</li>
				<li>
					社会的文化的な制約とは、ネジは時計回りに回すと締り、逆回しにすると緩む。
					車の右側のサイドランプを点灯すると右へ曲がると言う印になる。
					時計の1時と2時の間の時間は、2時と3時の間の時間と同じ長さである、などである。 
				</li>
				<li>
					論理的な制約とは、棒を右に倒せば対面した相手からは左に倒すことになる。
					電灯は点灯しているか消灯しているかのいずれかである。
					飛行機の到着時間は、出発時間のあとである、などである。
				</li>
				<li>
					意味的な制約とは、りんごは歩かない。 オートバイに乗る時に前方は決まっている、などである。
				</li>
			</ul>
			一方で、コンピュータのユーザ・インターフェイスは、意味的・論理的な制約くらいしか、利用できない。
			抽象的な世界（コンピューターを作っている人の頭の世界、コマンド用語）の中で、
			必ずしもコンピュータを理解していないユーザ向けにデザインしなければならないとき、利用できる制約が少ない。
			非常に難しい領域である。
		</p>
	</details>

	<details open><summary><strong>アイコンは非力</strong></summary>
		<p>
			現在、グラフィカル・ユーザ・インターフェイスが主流である。
			グラフィカル・ユーザーインターフェイスの、物理的に見える操作の側面は、わかりやすい。
			操作は、マウスやタッチで行う。
			マウスは、動かすとカーソルが動き、操作と結果の対応が明確である。
			マウスカーソルを対象の上でクリックすると、対象が選択され、それも操作と結果が明確である。
			タッチは、手が届く範囲内ならば、カーソルを動かすという手間がなく、対象選択が素早くできる。
		</p>
		<p>
			一方、グラフィカル・ユーザ・インターフェイスの表示は、メニューとともにアイコンを多用する。
			アイコンは、視覚的な象徴である。
			それは、<a href="visual_symbol">象徴を認識できる</a>でみたように、視覚的な象徴は間接的である。
			水道の蛇口やドアのノブにはあった、何ができるかを示す支持機能が、弱い。
			グラフィカル・ユーザ・インターフェイスでは、そういうアイコンを対象として操作する。
			当初は、デスクトップ・メタファーと言って、デスクにあるペン、文書、ファイルキャビネットなどを図示するという発想があった。
			しかし、機能が拡大するにつれ、物理的なものを象徴するだけではない、抽象的・間接的な表現となった。
			現在のスマホのホーム画面のアイコンのラベルがもしもなかったとしましょう。
			使えるだろうか？ アプリのアイコンは、ラベルで説明を追加していなければ、利用しにくい。
			アイコンという象徴は、何ができるかを示す表現手段としては非力である。
			グラフィカル・ユーザ・インターフェイスは、こういった基本的な問題を持つ。
		</p>
	</details>

	<details open><summary><strong>内部の抽象を露出させている</strong></summary>
		<p>
			うまくアイコン経由でアプリを起動できたとする。
			内部の機能を見てみよう。
			電話アプリ、スプレッドシート、地図アプリ、ワードプロセッサ、電卓アプリなど、対象が豊かで明確な意味を備えている。
			そういうアプリは、できることもわかりやすい。
			やったことが目に見えて返ってくる。
			ショートメッセージ、ニュースアプリなど、ユーザの意図が明確で単純なアプリも、わかりやすい。
			例えば、高齢者が初めてスマホを使い始めた時、他に何もできなくても、電話アプリは使える。
			昔の黒電話、ガラケーの電話アプリ、で、概念モデルがしっかりとあるからである。
			数字パッドを打つと、ツーツーと音が鳴り、相手が出る、ないし「つながりませんでした」とくる。
			操作も知っているし、結果もすぐに出て、明確である。
		</p>
		<p>
			しかし、そういう利点を持たないアプリ、提供機能自体がほぼ抽象的なものも多い。
			それらは、コンピュータ特有の抽象的な用語でしか説明できない。
			例えば、スプレッドシートで、ユーザがやりたいのは表計算である。
			そのファイルを上書き保存や、名前を付けて保存というのは、なにか？
			表は、後で取り出せさえすれば、ユーザにとっては、保存方法など詳細は、どうでもいいことである。
			ファイル・フォルダー体系というコンピュータの概念を、ユーザに押し付けている。
			目的以外の抽象概念を、そのままユーザに見せている。
			機能が抽象的な場合、その結果も抽象的である。
			それもそのまま見せている。
			そして、抽象的なことと抽象的なことの対応関係は、なおさら抽象的である。
		</p>
	</details>

	<details open><summary><strong>メニューは掃きだめ</strong></summary>
		<p>
			<a href="#magical_number">記憶できる個数</a>でみたように、3個か4個までであれば、認知負荷が小さい。
			メニューは、機能が多いときに詰め込む、いい掃きだめになっている。
			メニューの項目が、PCの一画面の底まで並んでいることは珍しくない。
			しかも、メニューの用語は、多くの場合、コンピュータエンジニアの用語である。
			また、コンピュータ内部の抽象的な用語である。
			あるいは、ユーザがめったに使わない。
			あるいは、ユーザがやりたいこと以上に、ユーザに見つけてほしい広告的なメニュー項目だったりする。
			メニュー、つまりあるアプリでできることに関して、デザイナーはユーザに必要なことだけに絞り込むことに多くが失敗している。
		</p>
		<p>
			しかも、メニューは、簡単に項目数が増える。
			メニューは、別メニューを項目として持つこともある。
			こうして、ヒトがやりたいことを実行するのに、広くて深い探索空間を相手にすることになる。
			理想は、狭くて浅い選択肢のみであってほしいのに。
		</p>
	</details>

	<details open><summary><strong>複雑になりすぎた</strong></summary>
		<p>
			グラフィカル・ユーザ・インターフェイスは、ヒトの概念的な構築物である。
			時間を経て発展するうちにどんどん複雑になる。
			スマホが登場したとき、当然、グラフィカル・ユーザ・インターフェイスが応用された。
			広い画面のユーザーインターフェイスが、小さい画面のものへと変化し、マウスはタッチという直接操作へ変わった。
			しかし、グラフィカルユーザーインターフェイスをもとに、その進化を重ねたため、複雑さはそのまま、ないし、より増した。
			対応関係が複雑怪奇となった。
			地図アプリを開き、スクロールしたり、つまんで広範囲を表示したり、など、ユーザのジェスチャー操作と操作対象の反応が
			直感的に対応している場合は、いい。
			が、そういう直感的な対応ができることばかりではない。
			一つの操作で、複数の意味がある。
			例えば、タップは、アプリの選択＋起動であり、項目のタップは選択＋メニューの起動でもあり、
			入力フィールドの選択でもあり、などなど。
			一つの意味なのに、その操作は複数ある。
			例えば、スマホでアプリを削除するのはアプリアイコンを長押ししてから行う。
			しかし、写真ギャラリーで写真を削除するには、タップして行う。
			ドナルド・ノーマンの言う、機能の数がスイッチの数を超えている状態である。
			慣れない人が慣れるのは至難である。
		</p>
	</details>

	<details open><summary><strong>脚注</strong></summary>
		<ul>
			<li id=n05>
				[05] コンピュータと人の情報のやり取りの観点では、
				コンピュータからの出力が知覚したり認知する対象となる。
				また、人からコンピュータへ情報を入力したりする。
				本文書では、コンピュータの入出力という言い方ではなく、ヒトを中心にした言い方をする。
				コンピュータに対して、ヒトは意図的動作をしたり言語表現をする。
				コンピュータからの情報を、ヒトが知覚し認知する。
			</li>
		</ul>
	</details>

</div>
	
<div class="pagebreak"><hr><h2>8. 操作からコミュニケーションへ</h2>

	<h3>8.1 操作からコミュニケーションへ</h3>

	<details open><summary><strong>操作からコミュニケーションへ</strong></summary>
		<p>
			コンピュータは、道具の一つである。 
			ヒトは、道具を操作する。
			対象を目で把握し、道具を手で操作し、結果を目でモニターする。
			しかし、コンピュータは、操作される道具として、あまりにも具体性がなく、抽象的である。
			また、様々なところに、すでに見えない形でも、浸透している。 
			コンピュータは、モノ道具という位置づけにはふさわしくない。
			コンピュータは、ヒトとの関係で、モノ道具を操作するという関係性ではなく、
			コミュニケーションの相棒という関係性を持つものにふさわしい。
		</p>
		<p>
			操作ではなく、ヒト相手のコミュニケーションをモデルとしてみた場合、コンピュータの道具としての要件が変わってくる。
			ヒトが手と目だけで操作する一方的な関係でない。
			ヒトは連合野で考え、行動する。
			環境に対し、五感を総合して、反応する。
			機械・道具のほうも、ヒトの生得的な身体能力を尊重するのが自然である。
			ヒトの能力をあるがままに引き出し、ヒトの発する身体からの言語を受け止め、反応するのが自然である。
			ヒトの身体に反応するとは、手指だけで操作す関係でなく、機械側がヒトの身振り、目振り、手振り、口振りを、受け止め、やり取りすることである。
		</p>
		<p>
			また、ヒトの身体のしぐさ、具体的には音声言語表現は、日常的である。
			それはヒトの意図を表現する。
			ヒトの身体に反応するということは、ヒトの意図レベルでやり取りをするということである。
			ヒトの脳だけで構築された抽象的なコンピュータ特有の中間概念は、そもそも不要である。
			ヒトが、電気ドリルで欲しいのは、穴であり、電気ドリルではない。
			コンピュータの内部の抽象的な概念をそのままさらすのでなく、ヒトの意図した目的の水準で、ヒトとやり取りする。
		</p>
		<p>
			それでは、コンピュータは、もっとハイテクでないといけないか？ 
			そうではない、すでに技術はある。
			音声ユーザーインターフェイスが鍵である。
			口振りに反応するとは、機械側がヒトの音声を認識することである。
			また、ヒトの音声言語は、日常的なレベルで意図を表現する。
			音声認識でアプリが組めるのか?
			組み方を変えればいいのである。
			すでにある技術を、ちょっと視点を変えて、デザインをすればよい。
			さらに、身振り、目ぶり、手振りに反応するとは？
			ヒトの身振り、目ぶり、手振りは、日常的な意図を表現し、コンピュータに構築された抽象的な概念階層とは無縁である。　
			どこにでもコンピュータはいるので、それらに目（カメラ）をつければよい。
			これは現状のハードに多少味付けをしなければならないが、技術はすでにある。
			掃除ロボットに目をつけて、ヒトがあっちを掃除してと指示することは、今でも実現できる。
		</p>
	</details>

	<h3>8.2 音声による意図表現の活用</h3>

	<details open><summary><strong>音声言語は意図を直観的に表現する</strong></summary>
		<p>
			ヒトとコンピュータが、道具を手と目で操作するという関係でなく、相棒とコミュニケーションするという関係モデルでは、
			音声が重要な役割を果たす。
		</p>
		<p>
			音声言語は日常生活空間で使われるため、意図を自然に表現できる。
			あいまいな部分は、話す相手や周囲の状況などのコンテキストで、実は明確である。
			ヒトの意図は、周りのコンテキストがあれば、言語音声に自然に表現されている。
			コンピュータは、それを利用すべきだ。
			グラフィカル・ユーザ・インターフェイスで、やりたいことを満たすアプリを選ぶのは、視覚的象徴経由である。
			間接的で、意図を伝達するにしては、最適ではない。
			絵を見て、何を意味するのか想起して、アプリの機能を認知する。
			そういう複雑な認知過程が必要となる。
			一方、音声だと、意図を、日常語彙で、直接、表現できる。
			音声で意図を伝え、それに加えて質問などによるやり取り、インタラクションを利用して、意図解釈をする。
			音声ならば、意図がすでに包含されているので、何ができるかを示すための設計というのが不要である。
			音声認識で意図を表現し、コンピュータがそれを解釈すれば、ドナルド・ノーマンの行為7段階説で行為系列への展開という部分がなくせる。
			また、結果の解釈という中間段階も消えるべきである。
			ヒトは意図を表現し、結果を評価する。
		</p>
	</details>

	<details open><summary><strong>音声は視覚と融合する</strong></summary>
		<p>
			コンピュータが、ヒトの身体の動作である口振り、に反応しようというのが、音声認識である。
		</p>
		<p>
			スマートスピーカーが出て、音声だけでやり取りするアプリが試されている。
			スマホの音声アシスタントもある。
			いずれも、当初、音声のみのユーザーインターフェイスを目指していた。
			その後、画像情報も援用するようになってきた。
			そもそも、ヒトが環境に対するとき、複数の感覚と効果器を総合して、機能する。
			その意味で、音声のみのユーザーインターフェイスは不自然である。
		</p>
		<p>
			無理に音声のみのやり取りでアプリを組もうとしても、以下の欠点が出る。
			<ul>
				<li>モードの制御がやりにくい。
					<p>
						音声のみだと、音声をコンピュータに聞かせている状態なのかそうでないのかの区別ができない。
						また、コマンドなのかテキストなのかの区別ができない。
						そういった区別状態のことをを、IT用語ではモードという。
						音声のみでは、モードの制御がやりにくい。
						「OK Google」とか、「Hey Siri」とか言って、音声をコンピュータに聞き取らせるのを始めるが、不格好である。
						聞き取ったことに対し、音声で返すしか手段がないとすると、アプリも作りにくい。
						音声をコンピュータに聞き取らせるのは、マイクの絵をたたいて始めるほうが自然である。
						またコンピュータが聞き取ったよというフィードバックも、視覚的な反応もあったほうが、アプリは組みやすい。
					</p>
				</li>
				<li>音声のみでは、そもそも空間内の位置指定ができない。
					<p>
						ヒトは、指差しや、マウスやで、空間の位置指定を容易に行うことができる。
						しかし、それを音声でやるとなると、あいまいな指示か、相当冗長な指示しか表現できない。
					</p>
				</li>
				<li>ボリューム調整など、アナログ量の制御が苦手である。
					<p>
						指つまみで簡単に指定できるボリューム量も、音声では難しい。
					</p>
				</li>
				<li>構造的な情報を扱うのが困難。
					<p>
						構造的な情報を扱うのも音声＋聴覚は苦手である。例えば、フライトを予約したい。
						日時を指定する、人数を指定する。出発場所と到着場所を指定する。
						その上で、値段込みの選択オプションを検索したい。
						それら関連した情報をコンピュータに指定する過程で、ある時点までに何の指定を済ませたか、何がまだなのか、
						を意識していないと、情報の支持がやりにくいであろう。
						すでに指定したものは、画面上で、目に見えておいてほしい。
						</p>
				</li>
				<li>多数からの選択が困難。
					<p>
						2、3個以上の選択肢がある場合、視覚的な補助なしに選択を行うことは困難である。
					</p>
				</li>
				<li>同音語で困難。
					<p>
						例えば、音声による言語表現では、コンピュータのほうに、キーボードのような選択・修正UIが完備されていない。
						そのため、人名入力や地名入力など同音語が多く、表記を選ぶ必要があるケースでは、コンピュータが音声だけで同音語を識別するのは難しい。
					</p>
				</li>
			</ul>
		</p>
		<p>			
			こうして、音声のみでコンピュータを操作しようとしても無理である。
			音声は意図を表現する。
			その水準でアプリを組もうとしたら、実は、指によるコントロール・空間指示と、視覚による構造把握と、併用すべきである。
			ヒトはそもそも特定のチャネルに限定せずにコミュニケーションする。
			同様に、ヒトのパートナーたるコンピュータも、複数のチャネルを融合して、人とコミュニケーションすべきである。
		</p>
	</details>

	<details open><summary><strong>音声アプリも支持と制約を利用すればよい</strong></summary>
		<p>
			音声でのやり取りは、意図レベルでのやり取りであり、ヒトの身体レベルのやり取りである。
			ヒト同士のコミュニケーションに近い。
			すると、かなり高度なテクノロジーを使わないとできないのでは？
			そうではない。
		</p>
		<p>
			ドナルド・ノーマンによれば<a href="rDN">[rDN]</a>、
			日常的な道具は、アフォーダンス、制約、概念モデルを利用しているという。
			例えば、はさみの穴は指を入れる場所を示し、指を入れることを支持（アフォード)
			穴の大きさは、1本指を入れるという制約を課している。
			ハサミの機能は、二つの刃が交わって紙などを切るという仕組みが目に見えているため、
			何をする道具なのかヒトにとっては容易に理解できる。
			アフォーダンスは何ができるかを示し、制約は選択肢を制限する。
			アフォーダンスと制約があるために、生活の中に無数に道具があって使い方を丁寧に教わらなくても、ヒトは使える。
		<p>
			コンピュータのグラフィカル・ユーザ・インターフェイスでは、見えているものが、手での操作の支持であり制約となる。
			キーを下に押し下げること自体は、何のための動作か曖昧である。
			しかし、Aと刻印されたキーを押し下げることは、Aの言語表現として、曖昧性がない。
			これは、ドナルド・ノーマンの言う外界の知識<a href="rDN">[rDN]</a>を利用している。
			また、グラフィッカル・ユーザ・インターフェイスでは、メニューや画面遷移というコンテキストがあるために、ある対象のクリックには、曖昧性がない。
			そのように、視覚情報やコンテキストによって制約されて、手指動作の曖昧性がなくなる。
			そのような曖昧でない動作を前提に、アプリが編み上げられる。
			アプリが組みやすいのである。
		</p>
		<p>
			音声による言語表現は、グラフィカル・ユーザーインターフェイスと同様に、視覚やコンテキストによる支持と制約を利用すればよい。
			同様な制約を利用すれば、曖昧性をなくすことができ、アプリの構成要素にできる。
		</p>
		<p>
			ヒトの音響系と空間視覚系とは、効果的に時分割できる。それも利用すればよい。	
		</p>
	</details>

	<details open><summary><strong>音声ワープロ？</strong></summary>
		<p>
			「<a href="#cognitive_interference">マルチタスクは事実上不可能</a>」のところで見たように、
			ヒトは、考えながらしゃべるのは難しい。しかし、一瞬考えて、それをしゃべる、それを繰り返す、という時分割なら、うまくできる。
		</p>
		<p>
			ヒトにとって言語は、視覚的でありかつ音響的である。その融合が、文明を発展させた。
		</p>
		<p>
			音声言語表現は、文書作成の時間を短くしない。
			しかし、アイデアを素早く完全な形で記録する。
			音声による言語生産スピードは、手指の5倍速い。
			そして、たとえ誤認識でノイズが多くても、文であるため、後で編集するときに、想起しやすい。
			それで、音声でスケッチした後での文書作成はやりやすい。
			音声言語表現は、頭の中にある整理されていないモヤモヤした概念を、忘れないうちに、目に見える形で取り出して、
			編集可能な形にし、考えを発展させられるようにするための手段である<a href="#rNY">[rNY]</a>。
			また、例えば、本を読んでいるときに、触発されて思いついたことを、音声でメモする。
			フルな文章で記録する。
			本に、指で、そのキーワードを書き込むよりは、後で、想起しやすい。
			音声を一度視覚的なテキストとして保存してしまえば、それは外部の記憶となる。
			外部の記憶として何回でも再認できるので、じっくり構成・構成する素材となる。
		</p>
		<p>
			このように、音声表現を介在させて、視覚的言語を生産するのは、効率的なのである。
		</p>
	</details>

	<h3>8.3 注目を利用する</h3>

	<details open><summary><strong>注目は重要な制約</strong></summary>
		<p>
			ヒトは、視線で注目する。
			ヒトは、指さしで、注目対象を示したりする。
			このような注目行動は、コミュニケーションの中では、重要な制約となる。
			コンピュータが、ヒトとコミュニケーションするとき、この制約を利用しない手はない。
		</p>

	<details open><summary><strong>視線は速い</strong></summary>
		<p>
			ヒトが、随意的に中心視野を動かすのは、一瞬である。
			ヒトがあるものに注目するとき、それに対する手足の動作を起こす前に、目はすでにそれを見ている。
			手足の関節を動かすよりも、小さな局所的な筋肉の動きで眼球はすでに反応している。
			ヒトの効果器のどれよりも、眼のほうで早く位置情報を認知している。
			ほかの手段のおよそ倍、速いといわれる<a href="#rKK">[rKK]</a>。
		</p>
		<p>
			このために、視線をマウスカーソルとして使おうという発想があった。
			視線を追跡することで、位置指定の操作手段として使おうと。
			手指が動かせないハンディを持ったユーザでも、目を動かせられるヒトはいる。
			そういうヒトのための表現手段となる。
			また、健常者でも、指とマウスは指の機械的速度と距離に制約されるが、視線は素早いので距離に束縛されない。
			そこで、ゲームの発射コマンド、画面の位置選択などに利用されたことがある。
		</p>
		<p>
			しかし、ヒトの目は受容器官としての役割で進化したものである。
			それを無理に操作手段として使うと、以下の不具合が出る。
			<ul>
				<li>
					受容器官としての動き以外の動きを期待するのは、ユーザに不自然な動作を強いることになる。
					不自然な動作は、疲労に導くし、慣れるまで学習を要する。
				</li>
				<li>
					また目だけを操作手段とすると、位置指定という視線を使った操作なのか、単に探索しているための目の移動なのか、
					状態の区別を機械に伝えるのが難しい。
				</li>	
			</ul>
		</p>	
	</details>
	
	<details open><summary><strong>視線は注目を示す</strong></summary>
		<p>
			目に効果器としての役割を求めるのでなく、進化してきた受容器のまま生かすべきである。
			視線は、空間的な注目箇所を示す。
			また、視線は、コミュニケーションにおいて「まなざし」として、情緒的な情報を伝える重要要素である。
			動物でさえ、まなざしで意思を伝える。
			機械から見れば、ユーザの意図を受け取る際に重要な情報となる。
			知能を持つ機械にとっての、制約をヒトが与えるのである。
			例えば、以下のようなユース・ケースがありうる。
		</p>
		<p>
			<ul>
				<li>
					スマホでニュース記事を読んでいる。
					画面上部のテキストを読みつつ、下にスクロールするため画面下部の画面に触れる。
					たまたま指が触れたところに、広告枠が表示されていた。
					画面が切り替わり広告が表示されて、びっくりする。
					この時は、意図はスクロールである。
					もしも、広告を見ながら、そこに触れたなら、広告に興味を持ったのである。		
				</li>
				<li>
					画面に連絡先の人物の写真がいくつか表示されている。
					視線を追跡することで、きょろきょろしているかどうかで、ユーザが現在探索中かどうかが推定できる。
					探索中ならば、探索を助けるように反応する。
					視線がある連絡先に注目し、停留したならば、この連絡先を選択するかと返す。
				</li>
				<li>
					部屋にいくつか電化製品がある。
					その時、エアコンを見ながら、「つけて」といったら、エアコンをつけるという意図である。
					部屋の明かりをつけるという意図ではない。
				</li>
			</ul>
	</details>

	<h3>8.4 ジェスチャー</h3>

		<details open><summary><strong>ジェスチャーは言語の一部である</strong></summary>
			<p>
				表情、身振り、手振りは重要なコミュニケーション手段である。
				ヒト画像から、それらを読み取るのは、すでに実現できている。
				音声による言語表現と総合して意図を読み取るのは、技術的にもう可能である。
				その時、コンピュータはロボットとなり、コミュニケーションの相棒となり、ヒトが操作するときの認知負荷が消える。
			</p>
		</details>

<div class="pagebreak"><hr><h2>9. まとめ</h2>
	<details open><summary><strong>コンピュータ・ユーザー・インターフェイスのデザイン思考を変える</strong></summary>
		<p>
			筆者は、コンピュータ・ユーザー・インターフェイスの分野でキャリアを積み、その技術の未熟さや課題を痛切に見てきた。
			高齢者が、スマホを使いこなせない点に、課題が先鋭化している。
			そういう課題を克服するには、どうあるべきかをずっと考えてきた。
			自分なりの解答を本書にまとめた。
			ヒトが、道具を操作するということの延長で、コンピュータを位置づけていることが、問題の大きな一因である。
			むしろ、ヒトがヒトとコミュニケーションするのをモデルにするべきである。
			そうしたとき、コンピュータは、ヒトの身体能力・五感とやり取りできなければならない。
			その時に、コミュニケーションの相棒になる。
		<p>
		<p>
			しかし、それを実現するのに、新しい技術開発やハイテクが必要かというとそうではない。
			すでにある技術で十分に実現できる。
			問題は、システムやアプリをデザインするときの考え方である。
			目と手でのインターフェイスの延長で考えるのでは、課題が先鋭化するのみである。
			そうではなく、音声でのやり取りは、視覚と融合したデザインを最初から目指すべきである。
			音声での意図解釈も、グラフィカル・インターフェイズと同様にコンテキストや制約を利用すればよい。
			コンピュータには目をつけて、ヒトの身振り、手振りなども総合して、意図解釈すべきである。
		</p>
	</details>

	<details open><summary><strong>BMIは夢のインターフェイスか？</strong></summary>
		<p>
			ヒトの脳・神経は、環境とのやり取りで、生物として、受容・効果器官との協同作業のために進化した。
			そのしくみは、未知なことが多い。
			一方、脳に直接機械・道具を接続することで、道具を自在に操ることが、昔から夢見られてきた。
		</p>
		<p>
			一方、大脳には可塑性という特質があり、生得の知能よりは、生まれた後で学習して得る知能のほうが、圧倒的に大きい。
			手に第6の指というのを装着し、腕の筋肉の動きでコントロールするように、少し訓練すると、
			自分の体の一部であるかのような感覚で、動かせるという。
			大脳の可塑性のおかげで、人の生体機能さえも機械を使って拡張(進化)できる。
			脳に直接接続するのが近道になるかわからない。
			脳は身体とともに進化してきた。身体から切り離して機会とつなげるよりは、
			むしろ、ヒトの身体と協調するという方向が現実的かもしれない。
		</p>
	</details>
</div>

</div> <!--End of TOC areas-->

<div class="pagebreak"></div><hr>
<h2>参考文献</h2>
<details open><summary>出典</summary>
<ul>
<li id=rBD>[rBD] Multimodal Interfaces: A Survey of Principles, Models and Frameworks、Bruno Dumas, Denis Lalanne, Sharon Ovian、2009. </li>
<li id=rBS>[rBS] The Limits of Speech Recognition. s.l. : Communication of ACM、Shneiderman, Ben、2000.</li>
<li id=rCL>[rCL] 「137億年の物語」（What On Earth Happendd?)、クリストファー・ロイド（Christopher Lloyd)
<li id=rCW>[rCW] Multiple resources and performance prediction、Wickens, Cristopher D、 2002</li>
<li id=rDN>[rDN] 「誰のためのデザイン？」（The Psychlogy of Everyday Things）、D.A.ノーマン、新曜社認知科学選書,1988</li>
<li id=rFJ>[rFJ] http://fujiidental.jp/clinic/archives/18 </li>
<li id=rFT>[rFT] 「生体情報システム論」、福田忠彦、1995、産業図書、ISBN978-4-7828-5303-0C335</li>
<li id=rHK>[rHK] 「視覚と聴覚はどうちがうか」、樋渡涓二（ひわたり　けんじ）、1997、NHK 第31巻第11号</li>
<li id=rHN>[rHN] 廣野守俊 永雄総一 小脳 脳科学辞典　https://bsd.neuroinf.jp/wiki/%E5%B0%8F%E8%84%B3 (2021)</li>
<li id=rIN>[rIN] 「図解・感覚器の進化 原始動物からヒトへ水中から陸上へ」、岩堀修明（いわほり・のぶはる）著、2011、講談社、ISBN978-4-06-257712-0</li>
<li id=rKH>[rKH] 『「視覚は人間の情報入力の８０％」説の来し方と行方』、加藤宏、筑波技術大学テクノレポート Vol.25 (1) Dec. 2017</li>
<li id=rKH2>[rKH2] 「霊長類の音声の運動基盤及び多様性とその進化的な背景」、香田啓貴（こだ　ひろき）、日本音響学会誌71巻7号、2015
<li id=KH3> [rKH3] サルの発声から見るヒトの言語の起源、香田啓貴 https://www.brh.co.jp/publication/journal/102/rp/research01/ </li>	
<li id=rKK>[rKK] Eye Tracking in Human-Computer Interaction and Usability Research; Ready to Deliver the Promises、Karn, J. K. Jacob and Keith S.、2003</li>
<li id=rKW>[rKW] 蔵田潔、渡辺雅彦 大脳皮質 脳科学辞典 https://bsd.neuroinf.jp/wiki/%E5%A4%A7%E8%84%B3%E7%9A%AE%E8%B3%AA (2021)</li>
<li id=rMA>[rMA] 「脳の神経細胞の数」、三上章允（みかみ あきちか）、http://web2.chubu-gu.ac.jp/web_labo/mikami/brain/10/index-10.html</li>
<li id=rMC>{rMC] https://www.ted.com/talks/michael_corballis_evolution_s_great_mystery_language/transcript?language=ja</li>
<li id=rMK>[rMK] 『ヒトとは視覚を発達させ、嗅覚を退化させた「か弱きサル」である』、三谷宏治（みたに こうじ）、https://www.careerinq.com/blog/mitani/2015/05/post-6.shtml</li>
<li id=rMS>[rMS] 「皮膚感覚の情報処理」、下条誠、計測と制御、第４１巻、第１０号、２００２年１０月。
<li id=rNT>[rNT] 「霊長類の音声器官の比較発達ー言葉の系統発生」、西村剛(（にしむら　たけし）, The Japanese Journal of Animal Psychology, 60, 1 49-58 (2010)</li>
<li id=rNY>[rNY] 野口悠紀雄『「超」書く技術』(プレジデント社)</li>
<li id=rPN>[rPN] 「生命とは何か？」、ポール・ナース、ダイヤモンド社、ISBN978-4-478-11107-9 C0045</li> 
<li id=rSO>[rSO] 「生体情報処理」、杉江昇、大西昇、昭晃堂、2001年、ISBN4-7856-9060-7</li>
<li id=rST>[rST] 橘木 修志 視細胞 脳科学辞典　https://bsd.neuroinf.jp/wiki/%E8%A6%96%E7%B4%B0%E8%83%9E (2019)</li>
<li id=rSW>[rSW] 「インターフェイスデザインの心理学」(100 Things Every Designer Needs to KNow ABout People)、スーザン・ワインチェンク(Suzan Weinschenk)、O’REILLY、ISBN978-4-87311-557-3</li>
<li id=rTK>[rTK] 「嗅覚の匂い受容メカニズム」、東原和成（とうはら かずしげ）、2015</li>
<li id=rYH>[rYH] 「サピエンス全史」、ユヴァル・ノア・ハラリ、2016</li>
<li id=rYH2>[rYH2] 「指の機能」、米満弘之、1973、精密機械、40巻1号、https://www.jstage.jst.go.jp/article/jjspe1933/40/468/40_468_18/_pdf</li>
<li id=rYT>[rYT]「唯脳論」養老孟司、1998、ISBN4-480-08439-8</li>
<li id=rUT>[rUT] 上村朋子 空間記憶 脳科学辞典 https://bsd.neuroinf.jp/wiki/%E7%A9%BA%E9%96%93%E8%A8%98%E6%86%B6 (2018)</li>
<li id=rVB>[rVB] 「ヒトの脳 : 解剖学的構造と機能」、https://www.visiblebody.com/ja/learn/nervous/brain</li>

</li>
</ul>
</details>

<!--https://shu-sait.com/mokuji-jidou-seisei/-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="toc.js"></script>
<script>
    $(function () {
        $(".toc-contents").toc({
			startLevel: 'h2',
		});
    });
</script>

</body>
</html>