<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="This site is to promote the thought that uses the daily behavior of people across the human-machine interactions.">
    <meta name="author" content="Yoshiharu Sato">
	<title>ヒトと機械の会話</title>
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Josefin+Slab:100,300,400,600,700,100italic,300italic,400italic,600italic,700italic" rel="stylesheet" type="text/css">

<style>
p { line-height: 1.5;  }
li { line-height: 1.7;  }
table { margin: auto; }
.pagebreak { break-before: page; }
.printfriendlyblock { page-break-inside: avoid; }
</style>
	
</head>

<body>
<h1>ヒトと機械の会話</h1>
<hr>

<details open open><summary>目次</summary>
	<!--https://shu-sait.com/mokuji-jidou-seisei/-->
	<div id="toc" class="l-toc"></div>
</details>

<div class="toc-contents"> <!--Start of TOC areas-->

<div class="pagebreak"><hr><h2>1. 始めに</h2>

	<details open><summary><strong>本文書の想定読者</strong></summary>
		<p>
			本文書は、以下の方々を読者として想定し、一般向けではない。
			<ul>
				<li>これから情報処理を志す若者</li>
				<li>情報処理技術分野で現在活躍している、プロフェッショナル</li>
				<li>I情報処理に関するインフルエンサー</li>
			</ul>
			ただ、本文書の扱うトピックおよび主張は、情報処理に関心のある人すべてが興味を持たれると思う。
		</p>
	</details>

	<details open><summary><strong>道具がヒトの能力と調和したとき、ヒトの能力を拡大する</strong></summary>
		<p>
			ヒトは、その能力によって道具を発明し、自分ができることを拡大してきた。
			例えば、ヒトは、文字と紙という道具を発明した。
			それによって、ヒトは、記憶したり伝達したりする能力を、拡張した。
			また、ヒトは、紙という平面の物体に、ペンを操り、三次元の物体の見取り図を表現する。
			これは、生身の目では見通せない俯瞰的な視覚である。
			最近では、顕微鏡や望遠鏡が、見えるものを広げた。
			車が、ヒトの移動距離を変えた。
			インターネットが、交信する集団の規模を変えた。
			これらすべてヒト能力の拡張である。
		</p>
		<p>
			紙に文字や図を書くとき、ヒトは、器用な手指を操作する。
			高性能な目で指の軌跡を確認し、手指の内部的な感覚とともに、動きに修正をかける。
			そういったことを、瞬時に連続的に繰り返し行っている。
			ヒトは、その高度な目の機能と手指の器用さによって、道具を使いこなしている。
			このように、道具が人の能力と調和したとき、道具は有効なものとなる。
			そして、有効な道具は、ヒトの能力を拡大する。
			さらに、拡大されたヒトの能力によって、道具は一層高度になる。
			道具とヒト能力の相乗効果は、石器時代の昔か、ヒト社会を変えてきた。
		</p>
	</details>

	<details open><summary><strong>コンピュータはヒトの能力と調和していない</strong></summary>
		<p>
			しかし、現在のヒトとコンピュータの間の関係はどうか。
		</p>
		<ul>
			<li>
				例えば、ヒトは、ハサミを難なく使える。
				それに対して、コンピュータは、マニュアルを読まないと使えない。
				マニュアルに書いてある、人工的な抽象物の建築物の歩き方を習熟してからでないと、何をどうするか見当もつかない。
				つまり、コンピュータがもたらす概念体系は、ヒトの頭に素直に自然と入ってくるもので、ない。
			</li>
			<li>
				例えば、ヒトは、意識せずに歩くことができる。
				また、自転車に乗ったり、自動車のハンドルを操作するのに、少し訓練すれば、あとは無意識的に筋肉が動いてくれる。
				一方、コンピュータは、少し慣れたあとでも、操作する際、これをやれば何が起きるかと、常に知的な注意と緊張を強いられる。
				つまり、ヒトが難なく自然と覚え、吸収できるもので、ない。
			</li>
			<li>
				例えば、子供から成長するにつれて、ほとんどのヒトは言葉をしゃべり、文字を書くことができるようになる。
				これは、社会的教育による。
				一方、コンピュータは、情報を流通するツールとしても、誰でも使えるものでない。
				つまり、人が社会の教育的環境の中で自然と身に着けられる、ヒト文化の一部では、ない。
			</li>
		</ul>
		<p>
			現状のコンピュータは、ヒトの日常的な能力で即、使えるものでない。
			その意味で、コンピュータはヒトの能力と調和していない。
		</p>
		<p>
			本文書では、高齢者を例として、ときどき、取り上げる。
			高齢者は、情報処理機器に限らず、新たに何かを習得することを嫌う傾向がある。
			そういったヒトたちに、この調和していないという問題が、先鋭化する。
		</p>
		<p>
			例えば、2011年、老テク研究会という組織が、
			高齢者が情報機器が使いにくい理由について定量調査した（<a href="#rRT">[rRT]</a>）。
			その理由の上位に、マニュアルのことが並んでいる。
			今になっても、状況は変わっていない。
		</p>
		<ul>
			<li>説明書のどこに自分の知りたいことが書いてあるかわからない…151名</li>
			<li>説明書を読んでも、英語やカタカナが多く言葉の意味がよくわからない…131名</li>
			<li>取り扱い説明書の文字が小さくて読めない…130名</li>
			<li>説明書が分厚くてとても読む気になれない…82名</li>
		</ul>
		<p>
			今の高齢者向けスマホは、壮年向けのデバイスと操作インターフェイスを、後で高齢者向けに改造して提供している。
			その結果、より複雑になり、マニュアルが厚くなった。
			せっかく、タッチで直接操作できて、より直観的な操作インターフェイスになったのに、である。
			なぜこうなったか、どう克服できるかを、本文書で見ていく。
		</p>
	</details>

	<details open><summary><strong>コンピュータの違和感</strong></summary>

		<p>
			コンピュータは、後述する（<a href="#elite_intelligence_tool">「エリートが知能の拡張を目指した」</a>）通り、
			もともとエリートの知能の拡張のための道具であった。
		</p>
		<p>
			ところで、コンピュータは、モノであり、道具である。
			しかし、ハサミのような単機能の道具と異なり、抽象的な事柄を扱う。
			コンピュータの処理対象は、情報である。
			コンピュータの機能は、計算や情報の加工である。
			どちらもヒトの抽象物である点が、日常で見る道具とは、異なる。
		</p>
		<p>
			しかも、コンピュータは、ほかの抽象物とも異なる点がある。
			例えば、建築物の構造、都市の道路、建物の配列は、人の抽象物である。
			また経済活動のための貨幣も、人の抽象物である。
			しかしこれらは、ヒトの生活や身体と、密接に関連している。
			一方、コンピュータの抽象物は、ヒトの身体と物理的な交渉がない。
			純粋に抽象的な、神話や数学などに近い。
			頭の中だけにあって、人の身体や生活と関わらない。
		</p>
		<p>
			コンピュータは、人の頭が生み出した道具である。
			しかし、すでにいろいろなモノの中に組み込まれ浸透している。
			スマホ、自動車、電子レンジ・洗濯機などの家電、などである。
			通信を介してコントロールされているものを含めたら、いろいろな社会インフラを支える装置もそうである。
			ヒトの能力と調和していない、純粋抽象物が、これほど、どこにでもある存在になってしまっている。
			歪んでいる。
		</p>
		<p>
			コンピュータは、エリートの知能の拡張のための道具、ということを超えて、応用され、普及した。
			いま、スマホの時代になって、コンピュータは、だれでも使う道具である。
			コンピュータの抱える抽象的な構築物や操作インターフェイスは、今の典型的な利用者像に合っていない。
			それが違和感の原因である。
		</p>
		<div class="printfriendlyblock">
			<p>
			<table border="2" cellpadding="5">
			<caption align="bottom">コンピュータの利用者と用途</caption>
			<tr>
				<th></th>
				<th>1940年頃～</th>
				<th>2010年頃～</th>
			</tr>
			<tr>
				<th>形態</th>
				<td>メインフレーム～パソコン</td>
				<td>スマホ</td>
			</tr>
			<tr>
				<th>主な利用者</th>
				<td>エリート</td>
				<td>万人</td>
			</tr>
			<tr>
				<th>用途</th>
				<td>知能の拡張、情報処理</td>
				<td>情報流通、ライフライン</td>
			</tr>
			</table>
		</p>
		</div>

	</details>
	
	<details open><summary><strong>操作と会話</strong></summary>
		<p>
			ところで、ヒトはモノ道具を操作し、ヒトと会話する。
			今、操作と会話という、異なる二つの関係性に注目してみる。
			それぞれ操作モデル、会話モデルと呼ぶことにする。
		</p>
		<p>
			コンピュータは、モノ道具の一つとして生まれた。
			ヒトが操作するものである限り、ヒトの指示に従うだけの存在である。
			ハサミなどは、ヒトが生得的な能力でコントロールできた。
			しかし、コンピュータは複雑な抽象から構成されているため、ヒトの生得的な能力とギャップがある。
			そのギャップのため、ヒトが誰でも操作できるものでなくなった。
			このギャップの部分は、従来、ユーザー・インターフェイス（UI）と言われてきた。
			違和感は、そのヒトがコンピュータを扱うインターフェイスにある。
		</p>
		<p>
			ヒトが道具を操作するのは一方向の関係である。
			こちらの操作から、向こうからの反応を見るまでが、ブラックボックスである。
			操作してから結果が返ってきて、その対応がどうなのかの解釈は、不幸なことに、利用者の認知負荷となる。
			そこで、逆の関係を隠しもってしまう危険がある。
			道具越しに、向こう側に、使っている主体以外のヒト組織が動いているケースがある。
			使っているヒトが、道具の向こう側の組織や主体に、実はコントロールされることもある。
			例えば、こちら側のヒトの意図と無関係に、向こう側から来た広告や不要な通知などに、突き動かされる。
			その場合、向こう側の組織や主体は隠れていて、道具がこちら側のヒトを操作しているかのようである。
		</p>
		<div class="printfriendlyblock">
			<p>
			<table border="2" cellpadding="5">
			<caption align="bottom">ヒトとコンピュータの関係性</caption>
			<tr>
				<th>関係性</th>
				<th>操作</th>
				<th>会話</th>
			</tr>
			<tr>
				<th>特徴</th>
				<td>
						一方向
						<br>&nbsp;&nbsp;ヒトから機械へ
						<br>&nbsp;&nbsp;機械からヒトへもあり		
				</td>
				<td>双方向</td>
			</tr>
			<tr>
				<th>表現水準</th>
				<td>コンピュータへの指示命令と結果表示</td>
				<td>ヒトの意図と反応</td>
			</tr>
			</table>
			</p>
		</div>
		<p>
			一方、ヒト相手の会話ではどうか。
		</p>
		<p>
			ヒト相手に話をするときは、詳細な指示を出す関係ではない。
			意図を伝え、依頼し、反応をみて、さらに会話する。
			ヒトの意図レベルで、やり取りをする。
			そこに、コンピュータ相手のコマンドをくみ上げるような、抽象的で難解なプロセスはない。
			ヒトが、文章を考え、しゃべる、という体得した能力ですむ。
			自然で誰でもできる行為ですむ。
		</p>
		<p>
			そして、意図を伝えるために、互いの視線を意識したり、指さしや表情を見合ったりする。
			見て、聞く。
			指さしも、表情も、体得的な行為である。
			ヒトは、体得的な能力で、身体を使って、会話する。
		</p>
		<p>
			会話モデルでは、意図を伝えて、その反応が返ってくる。
			その過程は、操作と同じくブラックボックスであるが、
			反応が意図に沿ったものかそうでないかを、ヒトは容易にわかる。
			会話は、ヒトが制御しやすい。
		</p>
		<p>
			ヒトとコンピュータは、モノ道具を操作するという関係性のままだった。
			また、エリートの知能の拡張のための道具だった。
			そのために、操作するためのインターフェイスに、ギャップがあることは、看過されてきた。
			さらに、道具がヒトを操作しているとも解釈できことも起きている。
			ヒト不在である。
			ギャップをなくし、操作される危険を小さくするには、
			会話モデルでインターフェイスを設計したほうがよい。
			意図レベルで、ヒトの身体ごとのやりとりにする。
			身体性を取り戻す。
			ヒトがコンピュータに合わせるのでなく、コンピュータがヒトに合わせる。
			ヒトが意図したことに、コンピュータが答える関係になる。
			ヒトの意図を軸にやり取りが回るので、ヒトが道具から操作されるということは起きにくい。
			そのとき、コンピュータはヒトと調和する。
		</p>
		<p>
			コンピュータがヒトと調和すると、何がいいか。
			ヒトが歩くように、誰でもストレスなく、コンピュータと付き合える。
			ヒトが自転車に乗るように、誰でもストレスなく、コンピュータの力を利用して、自分の力を拡張できる。
			コンピュータがもたらす無限の便益を、ヒトが自然に享受できるようになる。
		</p>
	</details>

	<details open><summary><strong>必要なこと</strong></summary>
		<p>
			ヒト同士の会話を、インターフェイスの理想だととする考えは、昔からあった。
			人工知能の会話ボットなどである。
			すると、もっとハイテクを駆使しないと、会話モデルにできないのか？
			そうではない。
			コンピュータを作る人、アプリを作る人が、すでにある技術をもとに、ちょっと視点を変えてデザインをすればよい。
			少なくとも、技術はあって、組み上げ方を変えればよい。
		</p>
		<p>
			ヒトが現在のコンピュータを操作するときのインターフェイスの歪みを意識し、克服しようとした考えは、以前からあった。
			例えば、石井裕のタンジブル・ビッツである(<a href="#rIY">[rIY]</a>)。
			豊かなコンピューター・ネットワーク空間の世界を、モニターとマウス越しでしかインターフェイスしないことに異を唱え、
			環境自体をインターフェイスにしようとした。
			形のない情報というものを、より実体のあるもので表現しようとした。
			環境という物理的な実体で、コンピューター・ネットワーク空間の情報を表現しようとした。
			すると、環境を進化させないと、人が現在のコンピュータを扱うインターフェイスの歪みは解決しないのか？
			いや。
			コンピューター・ネットワークの一部であるコンピュータが、豊かな人体能力を尊重するように変わればよい。
			環境ごと変わる必要はない。
			コンピュータが変われば済む。
			豊かなコンピューター・ネットワーク空間より、豊かなヒト能力をまず尊重する。
			環境をインターフェイスにするというより、ヒトをインターフェイスにする。
		</p>
		<p>
			また、坂村健やマーク・ワイザーによるユビキュタス（どこにでもある）・コンピューティングという考えもあった。
			Internet Of Things 、モノがインターネットでつながる、ということにつながる考えである。
			コンピュータがいずれ、いろんな所に埋め込まれて、それらが協調動作する。
			ハイテク家電には、すでに小さなコンピュータが組み込まれている。
			それらよりも格段に多く、様々な物に、コンピュータを埋め込む。
			そうすることで、ヒトはコンピュータを意識しなくても、いろんなことができる社会が実現すると。
			インターフェイスはなくなるべき、という考えもある。
			すると、コンピュータがいろんなものに埋め込まれるまで待たなければならないのか？
			そうではない。	
			コンピュータに作りこむアプリのデザインを変えればいい。
			また、画面付きスマートスピーカーとか、会話ロボットとか、今、出てきているコンピュータの形態を利用して、
			アプリ機能をくみ上げればよい。		
		</p>
	</details>

	<details open><summary><strong>本文書の構成</strong></summary>
		<p>
			本文書の前半では、ヒトの身体機能を概観し、それと接する機械がどうあるべきかの参考にする。
			後半では、ヒトの身体に向き合い、意図レベルで、やり取りするようなコンピュータの具体的なイメージを説く。
			それは、ハイテクである必要はないことを説明する。
		</p>
	</details>

</div>

<div class="pagebreak"><hr><h2>2. ヒト生体の情報処理</h2>

	<div><h3>2.1 概観</h3>

		<details open><summary><strong>ヒトの生体機能を見ていく理由</strong></summary>
			<p>
				コンピュータを、エリートの知能の拡張のための道具ととらえ、従来のヒトと道具操作のモデルでとらえるのは、
				誰でも使う時代に合わない。
				ヒトとの会話をモデルにして、インターフェイスを構成したほうが良い。
				人の意図レベルでやり取りをし、ヒトのカラダの能力を、そのまま受け止め、それに反応してほしい。
				そのために、コンピュータは、ヒトの生体の能力と特徴を、尊重しなければならない。
				ここでは、まず、ヒトが生物として世界に対処するときの、生体情報処理の特徴を見ていく。
			</p>
		</details>

		<details open><summary><strong>ヒトは8つの感覚器と2つの作用効果器で情報処理している</strong></summary>
			<p>
				ポール・ナースによれば（<a href="#rPN">[rPN]</a>）、
				「あらゆる生命には、自分と子孫を永続させるという目的がある。
				あらゆる生命の中心には、情報がある。
				目的のための行動に、情報は利用される。」と。
			</p>
			<p>
				生体は、外の世界と、自分の体の内側の世界と、両方から、情報を絶えず集めて利用している。
				内外の環境の状況を把握するものを、<a href="#receptor"><strong>受容器官</strong></a>という。
				それらに応じて外部に働きかけるものを、<a href="#effector"><strong>効果器官</strong></a>という。
				受容器官と効果器官とを連結し統括するものは、<a href="#brain"><strong>脳・神経系</strong></a>である。
			</p>
			<p>
				受容器官は、特殊感覚と体制感覚とがある。
				<strong>特殊感覚</strong>は、特定の刺激に対して、特定の場所にある器官が反応する。
				<strong>体性感覚</strong>は、身体に分散して存在し、環境への反応を助ける。
				一般的に五感と言われるものは、平衡感覚と固有感覚を除外した、視覚、味覚、嗅覚、聴覚、触覚である。
			</p>
			<p>
				一方、ヒトの効果器官には、機械系と音響系がある。
				機械系は、筋肉や骨格からなる手指や身体の機械的運動系である。モーター系とも。
				また、音響系とは、発声器官である。
			</p>
			<p>
				<ul>
				<li>受容器官</li>
					<ul>
					<li>特殊感覚器官</li>
						<ul>
						<li>視覚器</li>
						<li>味覚器</li>
						<li>嗅覚器</li>
						<li>平衡・聴覚器（組織的に同居）</li>
						</ul>
					<li>体性感覚器官</li>
						<ul>
						<li>外部から受容する皮膚感覚</li>
						<li>筋、腱、関節内で感知する固有感覚</li>
						</ul>
					</ul>
				<li>効果器官</li>	
					<ul>
					<li>筋肉や骨格からなる手指や身体の機械的運動系</li>
					<li>発声器官</li>
					</ul>
				</ul>
			</p>
		</details>

		<details open><summary><strong>情報媒体は、光、振動、科学的・物理的刺激である</strong></summary>
			<p>
				視覚は、光という電磁波を感知する。
				聴覚は、ヒトを含む陸生動物の場合、空気振動を感知する。
				触覚（皮膚感覚）は物理的刺激を感知する。
				そして味覚と嗅覚は化学的物質を感知する。
				運動系は物理的な効果を持つ。
				音声発生は、空気振動を起こす。
			</p>
			<div class="printfriendlyblock">
				<p>
				<table border="3" cellpadding="3">
				<caption align="bottom">ヒトの器官と情報媒体</caption>
				<tr><td rowspan="6">受容器官</td><td rowspan="4">特殊感覚</td><td>視覚</td><td>光</td></tr>
				<tr><td>味覚</td><td>化学的物質</td></tr>
				<tr><td>嗅覚</td><td>化学的物質</td></tr>
				<tr><td>平衡・聴覚</td><td>重力、空気振動</td></tr>
				<tr><td rowspan="2">体性感覚</td><td>皮膚感覚</td><td>物理的</td></tr>
				<tr><td>筋、腱、関節内固有感覚</td><td>物理的</td></tr>
				<tr><td rowspan="2">効果器官</td><td rowspan="2"></td><td>機械的運動系</td><td>物理的</td></tr>
				<tr><td>発声</td><td>空気振動</td></tr>
				</table>
				</p>
			</div>
		</details>

		<details open><summary><strong>近接感覚と遠隔感覚がある</strong></summary>
			<p>
				感覚には、遠くからのことを感知するものと、近いもことを感知するものと、どこでも偏在することを感知するものがある。
			</p>
			<p>
				どんな生物も、例外なく、重力を感じる。
				重力は、遠近に関係なく偏在する。
				動物は自分の体の傾きを、平衡器官で感知する。
			</p>
			<p>
				味覚、接触感覚は、近くのものの感覚である。
				動物の進化の中で、これらの近接感覚がまずあったと思われる。
				味覚は、化学物質を感知する。
				接触感覚は、物理的な圧力を感知する。
				これらが、食べ物を識別し、仲間と生殖・交信するためにあったと思われる。
				また、運動するために、体の状態を感知する体性感覚も必須だったろう。
			</p>
			<p>
				一方、生物が行動範囲を広げる際に、より広い環境を感知し、よりうまく生存・生殖したい。
				嗅覚は、遠くからの化学物質を感知する。
				聴覚は、遠くからの水・空気振動を感知する。
				視覚は、遠くからの光電磁波刺激を感知する。
			</p>

			<div class="printfriendlyblock">
			<p>
				<table border="3" cellpadding="3">
				<caption align="bottom">ヒトの器官と情報属性</caption>
				<tr><td rowspan="6">受容器官</td><td rowspan="4">特殊感覚</td><td>視覚</td><td>遠隔</td></tr>
				<tr><td>味覚</td><td>近接</td></tr>
				<tr><td>嗅覚</td><td>遠隔</td></tr>
				<tr><td>平衡、聴覚（器官的に同居している）</td><td>平衡感覚は偏在、聴覚は遠隔</td></tr>
				<tr><td rowspan="2">体性感覚</td><td>皮膚感覚</td><td>近接</td></tr>
				<tr><td>筋、腱、関節内固有感覚</td><td>近接</td></tr>
				<tr><td rowspan="2">効果器官</td><td rowspan="2"></td><td>機械的運動系</td><td>近接</td></tr>
				<tr><td>発声</td><td>遠隔</td></tr>
				</table>
				</p>
			</div>
		</details>

		<details open id=通信容量><summary><strong>それぞれの器官の通信容量</strong></summary>
			<p>
				人間の全受容器から、感覚神経を経由して、中枢神経系へ伝送される情報量は、10の9乗ビット/秒と言われる。
				うち、視覚は、10の6から8乗ビット/秒、聴覚は10の4から6乗/秒、触覚は10の6乗/秒と言われる。
				一方、中枢神経系から、運動神経を経由して、効果器へ伝送される情報量は、10の7乗ビット/秒と言われる
				（<a href="#rSO">[rSO]</a>、<a href="#rFT">[rFT]</a>、<a href="#n01">[脚注01]</a>）。
				また、人間の情報獲得の80%は視覚から、ともいわれる（<a href="#n02">[脚注02]</a>）。
			</p>

			<div class="printfriendlyblock">
				<p>
				<table border="3" cellpadding="3">
				<caption align="bottom">ヒトの器官と中枢系との間の通信容量</caption>
				<tr><td rowspan="7">受容器官</td><td></td><td></td><td>1000000000ビット/秒</td></tr>
				<tr><td rowspan="4">特殊感覚</td><td>視覚</td><td>100000000ビット/秒</td></tr>
				<tr><td>味覚</td><td>10000ビット/秒</td></tr>
				<tr><td>嗅覚</td><td>100000ビット/秒</td></tr>
				<tr><td>聴覚</td><td>100000ビット/秒</td></tr>
				<tr><td rowspan="2">体性感覚</td><td>皮膚感覚</td><td>1000000ビット/秒</td></tr>
				<tr><td>筋、腱、関節内固有感覚</td><td></td></tr>
				<tr><td rowspan="3">効果器官</td><td rowspan="3"></td><td></td><td>10000000ビット/秒</td></tr>
				<tr><td>機械的運動系</td><td></td></tr>
				<tr><td>発声</td><td></td></tr>
				</table>
				</p>
			</div>
			<p>
				ヒトは環境を認識するのに、主に視覚を使っている。
				また、環境に対する反応の情報量は、感覚よりも桁違いに小さい。
				ヒトという生物は、環境から多くの情報を仕入れて、生存・繁殖に有効なものを選んで反応し、環境に働きかけている。
			</p>
		</details>

		<details open><summary><strong>感覚、知覚、認知の違い</strong></summary>
			<p>
				受容器官に刺激が与えら脳に伝えられたもの感覚という。
				熱いとか、音が聞こえるとかである。
			</p>
			<p>
				感覚に、対象の構造や特徴が加えられて、意識されたものが知覚である。
				長いとか、強いとか。
				感覚はその一部が知覚となる。
				意識に上らないことは多い。
				あるものは無意識的な反応として行動に現れる。
				あるものはエネルギーを節約するために意識を向けられずにフィルターされる。
			</p>
			<p>
				さらに、知覚が過去の経験や学習に基づいて解釈されたものが、認知である。
				犬であるとか、母であるとか。
				認知になると、文化や社会の影響が濃厚に出る。
				社会の文化によって、虫の鳴き声を、雑音と感じるか、秋の風情と感じるか、が異なってくる。	
			</p>
			<div class="printfriendlyblock">
				<p>
				<table border="2" cellpadding="5">
				<caption align="bottom">情報受容の階層</caption>
				<tr>
					<td>階層</td>
					<td>特徴</td>
				</tr>
				<tr>
					<td>感覚</td>
					<td>刺激が中枢に伝達されたもの</td>
				</tr>
				<tr>
					<td>知覚</td>
					<td>対象の構造や特徴が加えられて意識されたもの</td>
				</tr>
				<tr>
					<td>認知</td>
					<td>過去の経験・学習に基づいて解釈されたもの</td>
				</tr>
			    </table>
				</p>
			</div>
			<p>
				感覚、知覚、認知は、ヒトが環境から情報を仕入れる、階層的な分類である。
				一方、反応に関しては、不随（無意識）的か、随意（意図）的か、の分類がある。
			</p>

		</details>

	</div>

	<div id=receptor class="pagebreak"><hr><h3>2.2 受容器官</h3>

		<div><h4>2.2.1 視覚</h4>

			<details open id=visual_animal><summary><strong>人は視覚的動物である</strong></summary>

				<p>
					人の得る情報の80%は視覚からと言われ、ヒトは視覚的動物である。
					ここでは、ヒトの視覚が、優れた能力を持つことを示す。
					それが、ヒトが今日のコンピュータを操作するときのインターフェイスで、
					目を酷使することにつながっている。
				</p>
				<p>
					光という情報媒体は、遠くまで届き、高速に伝わる、という特徴を持つ。
					生物が光を感知できると、遠方まで、敵か味方か、餌を識別できる。
					生存に有利である。
					光は高速なので、どこに物があっても瞬時に把握できる。
					その意味で、ほかの感覚と比べ、距離に左右されない。
				</p>
				<p>
					視覚が生物進化で決定的な役割を果たしたという物語が、生物史にある。
					地球史の中に、カンブリア紀という時代があった。
					その時代からいきなり化石が出始めた。
					英人生物学者のアンドリュー・パーカー（Andrew Parker、1967～）は、
					2003年、「光スイッチ説」を唱えた。
					カンブリア紀に登場した三葉虫は、目を進化させた。
					その結果、食べ、食べられる、食物連鎖関係で優位に立った。
					それが、淘汰圧として、ほかの生物の多様な進化を促した、と（<a href="#rMK">[rMK]</a>）。
					それまでは、化石になるような骨や外殻がない生物しか存在していなかった。
					視覚の登場が生存競争・選択淘汰を激烈にした。
					そのために骨や外殻を備えた多様な生物が生まれたと。
				</p>
				<p>
					その後、生物の長い歴史の中、視覚は機能的にも進化してきた。
					まず、原始的な生物は、明暗識別ができた。
					ついで、明暗の方向視、形態視、動きの感知、色認識ができるようになった。
					そして、両眼視による遠近を含む探索・位置同定、ができるようになった。
				</p>
				<p>
					一方で、光あふれた昼間、地表での生存競争を避けた生物があった。
					光は、他のものによって遮られる。
					また、夜には光がなくなる。
					地表を避けた生物は、別の感覚を伸ばす必要があった。
					化学物質である臭いは、風や水流に左右されるが、昼夜を問わず、どんな隙間にも入り込む。 
					中生代の恐竜の全盛期、ヒトの先祖である哺乳類は、恐竜から逃げた。
					哺乳類は、夜に活動し、光のないところで嗅覚を発達させて、生き延びた。
					陸上動物だけでなく、海生動物を含めても、視覚器が退化した動物は多いが、
					嗅覚器を持たない動物は少ないという（<a href="#rIN">[rIN]</a>）。
					それ以降、動物は、ヒトのような視覚型と、イヌのような嗅覚型に大別される。
				</p>
				<p>
					やがて、恐竜がいなくなり、類人猿が森から草原に降りてきた。
					そのころ、ヒトの先祖である狭鼻猿類は、それまでの赤・青の二色視でなく、
					赤・青・緑の三色視ができるように進化し、弁別できる色情報を格段に増やし、
					餌の識別などで優位に立った
					（<a href="#rMK">[rMK]</a>）。
				</p>
			</details>

			<details open><summary><strong>ヒトの視覚の高性能の仕組み</strong></summary>
				<p>
					ヒトの視覚の高性能の仕組みを見てみる。
					光刺激を瞬時に処理するため、ヒトの視覚は、情報圧縮と並列処理を活用している。
				</p>
				<p>まず情報圧縮である。</p>
				<ul>
					<li>
						脊椎動物の視細胞には、錐体(cone)と粁体(rod)という2種類がある。
						粁体(rod)は明暗に反応し、錐体は異なる波長の光(色)に反応する。
						ヒトでは、網膜に中心窩(fovea)と呼ばれる錐体だけが密集した部位がある。
						中心窩での視覚が視野の中心部となる。
						人間には錐体が約600万個、 粁体が約1億2000万個存在する。
						一方、視細胞の情報を受け取る視神経は、約100万個である。
						従って網膜は、光刺激の情報を、約100万割る約1億、
						1/100ほどに圧縮して脳に送っていることになる（<a href="#rST">[rST]</a>）。
					</li>
					<li>
						視覚は、光刺激を電気信号へ変換して処理する。
						刺激が同じならば、神経内で電気信号が発火されない。
						つまり、同じ映像であるかぎり、刺激としての画像情報は消える。
						そこで、眼球を不随意に微動させて、注視したときの網膜像を絶えずリフレッシュしている。
						そして、網膜像の時間的差分だけを脳へ送るという、
						データ圧縮を行っている（<a href="#rSO">[rSO]</a>）。
					</li>
				</ul>
				<p>ヒトは瞬時に目から多くの情報を把握できるが、いくつかの並列処理を行っている。</p>
				<ul>
					<li>
						視細胞のレベルでは、錐体は色や空間的情報処理を分担している。
						杆体は明暗情報処理･時間的情報処理を分担している。
						また、注目しているところの注目視と、別に周辺視が、独立に機能している。
						歩きながらスマホを見ることができるのもそのためである。
						明暗の変化や運動など時間的な変化を伴うものが、周辺視野で感知される。
						その後、注意すべきかどうかの判断のために、眼球運動で注目視し、
						知覚・認知が行われて、対処が判断される（<a href="#rFT">[rFT]</a>）。
						周辺視野には以下の特徴がある。
						<ul>
							<li>
								周辺視野は、素早く反応する。
								物体を、中心視野で観察すると脳が反応するまでに140-190ミリ秒、
								周辺視野では80ミリ秒かかる（<a href="rSW">[rSW]</a>）。
							</li>
							<li>
								周辺視野は、詳細認識ではなく、状況の概略を把握するのに使われる。
								台所の写真で、写真の周辺部を隠すとどこの写真かわからなくなるが、
								中央部を隠してもどこの写真が想像できる（<a href="rSW">[rSW]</a>）。
							</li>
						</ul>
					</li>
					<li>
						さらに、視覚情報は、三つのルートで処理される。まずは大きく二つに分かれる。
						<ul>
							<li>
								一つは、脳へ情報が送られて眼球運動を制御するのに使われる。
							</li>
							<li>
								もう一つは、パターン認知に使うため脳（大脳皮質第1視覚野、視覚前野）へ送られる。
								こちらは、網膜上の位置に依存した情報抽出を行ったあと、
								位置に依存しない空間・形態情報を抽出する。
								まず、線分の方位、長さ、色、動き、両眼視差などの特徴に選択的に反応し、
								網膜上の位置に依存した要素情報を抽出する。
								そして、そこから更に二つのルートに分割され、信号が送られる。
								<ul>
									<li>空間知覚処理部（側頭連合野）</li>
									<li>形態知覚処理部（頭頂連合野）</li>
								</ul>
								そこでは、位置に依存しない、人の顔の認識などパターン処理が行われる。
							</ul>
						</ul>
					</li>
					<p>
						このように、視細胞から大脳にかけて、階層的な処理が並行して動く
						（<a href="#rSO">[rSO]</a>）。
					</p>
				</ul>

			</details>

			<details open><summary><strong>認知的な効率化</strong></summary>
				<p>
					ヒトの視覚は、認知レベルでも効率化の仕組みを持っている。
				</p>
				<p>
					以下にルビンの壺という絵がある。これは、図と地の分化という現象を示す。
					1つのまとまりのある形として認識される部分を「図」、図の周囲にある背景を「地」と呼ぶ。
					この絵は、両側をまとまりととらえるか、中央部をまとまりととらえるかで、全く異なる物体に見える。
					ヒトは、複数の解釈が可能な画像でも、必ずある特定の解釈をとり、認知資源の省エネを図る。
					そして、いったん解釈を決めた場合、その記憶の影響を受ける。
					<p>
						<div align="center">
							<img width=20% src="rubins_vase.png"> 
							<p>[https://ja.wikipedia.org/wiki/ルビンの壺 より]</p>
						</div>
					</p>
				</p>
				<p>
					以下にカニッツアの三角形という図がある。
					ヒトは、知覚した情報を処理する際、すでに記憶に持っている認知パターン分類で解釈する（<a href="rSW">[rSW]</a>）。
					素早く対象を理解するための効率化である。
				</p>
				<p>
					<div align="center">
						<img width=20% src="Kanizsa_triangle.png"> 
						<p>[https://ja.wikipedia.org/wiki/カニッツァの三角形より]</p>
					</div>
				</p>
				<p>
					また、ヒトは、実世界の三次元物体を、すでに記憶している基本的な立体（ジオン）パターンを組み合わせて識別している（<a href="rSW">[rSW]</a>）。
				</p>
				<p>
					また、視覚は、高度な知的処理と相互作用の結果得られる知覚である。
					下の図は、同じ画像でも、コンテキストによって、Hだったり、Aだったりに見える。
					文脈効果と呼ばれる。
					<p>
						<div align="center">
							<img width=20% src="context_effect.png"> 
							<p>[https://maruhi.heteml.net/chikakuninchi/?page_id=648 より]</p>
						</div>
					</p>
				</p>
			</details>

			<details open id=eyeball_movement><summary><strong>眼球の動き</strong></summary>
				<p>
					眼球は、直径24mmから25mmの球体である。
					眼窩の中で、脂肪に囲まれて、三対六種類の筋肉で支えられている。
					そして、上下、左右、視軸回りの回転運動を行う（<a href="#rFT">[rFT]</a>）。
				</p>
				<p>
					<div align="center">
						<img width=80% src="eyeball_movement.jpg"> 
						<p>[https://plaza.umin.ac.jp/jikei-np/symptoms/01_01_13.htmlより]</p>
					</div>
				</p>
				<p>
					目の筋肉は、何種類かの運動によって、形態視、動体視、立体視、位置同定などの機能をはたしている
					（<a href="#rFT">[rFT]</a>、<a href="#rSO">[rSO]</a>）。
					<ul>
						<li>
							中心視をするために，左右の眼は連動して動く。
							二種類の連動運動がある。
							<ul>
								<li>
									一つは、移動する対象を追う運動で、両眼は同じ方向へ運動する（共同運動、conjugate）。
									これには滑らかな成分（最高25から30度/秒）と跳躍性の運動成分（ザッカード、300から600度/秒）がある。
									視覚は主に空間的情報を感知する。
									さらに、対象の動きという時間的な情報も感知する。
									雲は風に流される。遠くから見ればその動きは微々たるものである。
									しかし、ヒトの目は、雲が連続的に確実に動いていると感知する。
								</li>
								<li>
									もう一方の連動運動は、両眼が逆方向に運動する（幅そう運動、disjunctive, vergence）。
									これは、左右のわずかに異なった像を融合して一つの像として知覚し、
									立体視のデータを得るためにある。
									ヒトは、三次元の世界に住んでいるのである。				
								</li>
							</ul>
						</li>
						<li>
							一方、眼球は、固視微動といって、注目視野内の微小な不随意の動きを行い、静止した物体の網膜像が消えないようにしている。
						</li>	
					</ul>
				</p>
				<p>
					ヒトの睡眠には、レム睡眠という、夢を見て、目を激しく動かす時間帯があり、
					脳の中を整理しているらしい（<a href="#rYT">[rYT]</a>）。
					ヒトは、頭を整理するために、目を動かす。
					そのくらい、ヒトは視覚的動物だということだろう。
				</p>
			</details>

			<details open id=fast_eye_sight><summary><strong>視線は速い</strong></summary>
				<p>
					ヒトが、随意的に中心視野を動かすのは、一瞬である。
					ヒトがあるものに注目するとき、それに対する手足の動作を起こす前に、目はすでにそれを見ている。
					手足の関節を動かすよりも、小さな局所的な筋肉の動きで、眼球はすでに反応している。
					ヒトの効果器のどれよりも、眼のほうがより早く位置情報を認知している。
					ほかの手段のおよそ倍、速いといわれる（<a href="#rKK">[rKK]</a>）。
				</p>
			</details>

			<details open id=three_d_space><summary><strong>二次元と三次元</strong></summary>
				<p>
					ヒトは、三次元の住民である。
					ヒトが観察する世界は、三次元の立体である。
					立体視の仕組みには、単眼によるものと両眼視によるものとがある。
				</p>
				<p>
					単眼視による手がかりとしては以下のものなどがある（<a href="#rCP">[rCP]</a>）。
					<ul>
						<li>
							物体の重なりから前後を見て取る。
						</li>
						<li>
							大きさから距離を感じる。
						</li>
						<li>
							平行線は遠ざかるほど幅が狭くなる。
						</li>
						<li>
							一定の模様は、遠いほどきめが細かくなる。
						</li>
						<li>
							遠くの景色ほど、ぼやけたりかすんで見える。
						</li>
					</ul>
				</p>
				<p>
					一方、両眼視によって、両眼網膜視差をもとに、脳内で三次元イメージを構築する。
					その三次元の世界は、自分の位置を変えれば、同じものでも、刻々と異なって見えてくる。
					異なった三次元の外見のものを同一の物体だとみなせるのは、ヒトの抽象化能力のためである。
				</p>
				<p>
					一方、ヒトは、三次元に現象する物体を、紙という道具の二次元空間に表現したりもする。
					三次元のものを相手にするより、二次元のものを相手にするほうが、認知エネルギーは小さい。
					そして、紙を持ち歩いたり、別のところで三次元を再現したりする。
					見た心象を、二次元空間に表現して、絵、漫画、浮世絵などとして、仲間に感情を伝えたりもする。
					ヒトは、認知的に楽な情報を通して、実は高度な精神活動をしている。
				</p>
				<p>
					現在のコンピュータは、モニターやタッチ画面という二次元空間を利用する。
					ヒトは二次元を通して高度な精神活動を行うことができる。
					一方で、日常の生活空間は、三次元の豊かさを備えている。
				</p>
			</details>

			<details open><summary><strong>素早く文字を読み取る</strong></summary>
				<p>
					ヒトの視覚は、神経の高速並列処理と機敏な眼球筋肉のおかげで、言語認知で高性能を示す。
				</p>
				<p>
					注目視野は、20から30ビットを一度に把握できると言われる。
					これはアルファベットは5文字、ひらがなは4文字、漢字は2文字に相当する
					（<a href="#rFT">[rFT]</a>）。
					英語の場合、一度に、15文字を読み進むという。
					先頭の1から7文字で意味を取り、次の8から15文字は周辺視野でみている、あるいは予測しているという
					（<a href="#rSW">[rSW]</a>）。
					またヒトは、聞き取りであれば1分間に160語ほどを把握できるが、読み取りは1分間に300語ほど把握できる。
					耳で聞き取るためには、音は時系列で並んでいるので、それらを逐次処理しないといけない。
					聞き取りが、読み取りよりも遅い主因である。
				</p>
			</details>

			<details open id=space_memory><summary><strong>空間的に記憶できる</strong></summary>
				<p>
					動物は、餌の場所や住処をめぐって行動する。
					そのために、周囲の空間と自身を関連付ける認知機能、つまり空間記憶がある（<a href="#rUT">[rUT]]</a>）。
					聞いたことを記憶するのと比べて、見えたものは繰り返し確認できる。
					それが記憶保持を助ける。
				</p>
				<p>
					例えば、あなたに次のような経験ないだろうか。
					色分けした表紙のファイルを見て、どのファイルがどの内容のファイルだったかわかる。
					ある事柄が、本の分厚い厚みの中でどのあたりのページのどのあたりに書いてあったか、覚えている。
					机の上に、いくつも書類が乱雑に積み重なって置かれているが、どこに何があるか思い出すことができる。
					ピアノである曲を暗譜で弾くことが出来る。
				<p>
					このような空間記憶は、連想記憶にも似ている。
					キーボード操作に慣れた人は、キーボードを見なくても操作できる。
					しかし、キーボードがないところで、キーボードのキー配列を思い出そうとするとできない。
					つまり、頭にそのまま記憶しているのではないのである。
					しかし、キーボードを見ると、指が覚えていたかのように、すぐに上手にタイプできる。
					ここには視覚的な連想記憶が働いている。
					記憶は神経細胞の結合パターンとして保持される。
					何度もキーボードを見て触っていると、神経細胞結合の発火の痕跡が残る。
					キーボードを見たという刺激だけで、あるキーがこの辺にあったよなという記憶が活性化される。
					先行刺激が後続刺激への処理影響を与えることを、プライミング効果という（<a href="#rCP">[rCP]</a>）。
					ある視覚刺激が、長期記憶から別のことを想起させた。
					これを、ドナルド・ノイマンは「外部知識」と呼んだ（<a href="#rDN">[rDN]</a>）。
				</p>
			</details>

			<details open id=structure><summary><strong>構造を把握できる</strong></summary>
				<p>
					空間記憶と関連して、視覚は、いくつかの情報を同時に把握できるという能力がある。
					そのため、ヒトの抽象的な概念のうち、構造的な情報を認知できる。
				</p>
				<p>
					例えば、一つの画面に、やることのTODOリストがあるとする。
					1個目と2個目を比べて、どっちを先にやるか考えているとき、目は二つの項目を認知し、作業記憶の中において比べている。
					例えば、来週の出張の飛行機を予約しているとする。
					WEBの画面を見て、日時と概略出発時刻を入力し、どの会社のどのフライトにするか選択肢がでて、どれにするか検討しているとする。
					ヒトの頭の中では、日時と概略出発時刻とともに、飛行機会社のブランド名も一緒に意識している。
					もっとも、この視覚は構造を把握できるために、視覚向けにデザインされた情報が複雑になってしまう原因ともなる。
				</p>
			</details>

			<details open id=visual_symbol><summary><strong>象徴を認識できる</strong></summary>
				<p>
					ヒトは言語を駆使する。
					音響の連鎖が、音響言語として、複雑な概念を伝える。
				</p>
				<p>
					一方で、ヒトは、視覚的な形状で、ほかの何かを象徴することも行ってきた。
					指ジェスチャー、地面に書いた絵、壁画、象形文字、表意文字、アイコン、ピクトグラムなどである。
					視覚的な象徴の表現力は、言語に比べて、限られている。
					言語は、感覚を指示したり、実在物を指示したり、環境を自在に利用できる。
					言語は、構文構成で概念を自在に組み合わせる表現力がある。
					しかし、視覚的な象徴は、写実画とかでもなければ、あるいは固有名詞の文字列でなければ、
					特定の実在物を指示する力はない。
					概念の組み合わせも、空間的配置くらいしかできない。
					視覚的な象徴は、表現力が限定され、間接的である。
				</p>
				<p>
					コンピュータのアイコンも間接的・限定的であるが、その限界が忘れられている。
				</p>
			</details>

		</div>

		<div><h4>2.2.2 聴覚</h4>

			<details open><summary><strong>聴覚の能力</strong></summary>
				<p>
					ヒト同士の会話では、聴覚と発話という音響的言語が、大きな役割を果たしている。
					一方、コンピュータのインターフェイスでは、聴覚および音響的言語は、ほとんど活用されてこなかった。
				</p>
				<p>
					ヒトの聴覚は、空気の振動を感知する遠隔感覚である。 
					空気の振動は、距離によって、また障害物があると、減衰しやすい。
				</p>
				<p>
					ヒトの聴覚受容細胞の数は23,500個（視細胞は1億以上）、聴神経は3万本（視神経は100万本）である
					（<a href="#rHK">[rHK]</a>）。
				</p>
				<p>
					視覚がおおむね空間情報を処理するのに対し、聴覚はおおむね時間的情報を処理する。
					会話の話し声、赤ちゃんの泣き声、音楽のリズムやメロディ、風の音、雨の音、雷鳴、鳥の鳴き声、虫の鳴き声、カエルの鳴き声など、
					すべて時間的な流れの中に存在する。
				</p>
				<p>
					聴覚は、空間的情報も知覚する。
					精度は視覚より低いが、両耳により、距離と方向を感知する音源定位ができる。
					車を運転中に、サイレンの音が聞こえる。どちらの方向からを確かめるために、よく聞こうとする。
					野鳥の鳴き声がきこえてきたら、どこにいるか方角の見当をつけて探す。
					ただ、目でそれを確かめないと、満たされない気がする。
					聴覚は視覚に引きずられもする（<a href="#n03">[脚注03]</a>）。
				</p>
			</details>

			<details open><summary><strong>聴覚は発話に連動する</strong></summary>
				<p>
					聴覚に障害のある人は、発話障害を伴うことが多い。
					情報は、言葉を聞く聴覚器官から、発話するための調音器官へ流れる。
					人は、自分の音声を聞きながら発話する（<a href="#rSO">[rSO]</a>）。
					目から得た情報に基づき、手指が動くように、聞いた音に基づき、発話する。
					ヒトの聴覚は、音から危険を察したり気配を感じたりもする。
					が、発話と一体となって音響言語を駆使する会話の場面が、
					主な活躍場所に見える。
				</p>

			</details>

			<details open id=anti_structure ><summary><strong>聴覚認知は構造保持が苦手</strong></summary>
				<p>
					視覚は、空間記憶ができ（<a href="#space_memory">[空間的に記憶できる]</a>）る。
					また、構造の把握ができる（<a href="#structure">[構造を把握できる]</a>）。
					一方、聴覚は、それらが苦手で、対照的である。
				</p>
				<p>
					音響的な言語情報は、時系列情報である。
					そして、音声は消え去る。
					短期記憶に入っても、再確認できないし、しばらくたつと消える。
					そのため、ヒトは、聴覚からは、少ない量しか情報を把握できない。
					複数の情報を安定的に保持できない。
					そのため、聴覚は、フォームや手順といった複数の情報からなる構造の把握が苦手である。
				</p>
				<p>
					ヒトがコンピュータを操作するインターフェイスとして、音響的情報のやり取りだけでは無理であることを示す。
				</p>
			</details>
		</div>

		<div><h4>2.2.3 嗅覚</h4>

			<details open><summary><strong>ヒトは嗅覚を退化させた</strong></summary>
				<p>
					嗅覚は、揮発性ないし水溶性の化学物質を感知する。
					味覚は、同じ化学物質でも接触感覚である。
					嗅覚は、触覚という物理的な刺激に比べて、分子の組み合わせレベルの感覚である。
					そのため、嗅覚が発達すれば識別できる種類は膨大になる。
					しかし、ヒトは、嗅覚よりも、視覚というより遠方まで感知する感覚を優先した。
				</p>
				<p>
					ヒトの嗅覚受容体数（種類数）は396個あり、その組み合わせで数十万種類のにおいを感知できる。
					遺伝子レベルで見ると、哺乳類の嗅覚に関係する遺伝子は大きなファミリーをなしているそうである。
					動物にとって環境探知の重要な手段であることが示されている。
					また、匂い情報は、大脳の感情や記憶をつかさどる部分に流れ、内分泌で即座に反応できるようになっている。
					嗅覚が素早い反応によって生存するために重要な機能を果たしていた時代の名残である。
					その由来によって、匂いは長期記憶を呼び覚ます（<a href="#rTK">[rTK]</a>）。
				</p>
				<p>
					化学物質のやり取りを、機械とヒトができるようになったら、どういうことができるか、想像できない。
				</p>
			</details>
		</div>

		<div><h4>2.2.4 体性感覚</h4>

			<details open><summary><strong>内外を感知する</strong></summary>
				<p>
					体性感覚器は、外部を感知する触覚などと、内部の固有の情報を感知するものに、大別される。 
					ヒトに限らず、生物の生存にとって必須な基本的感覚である。
					しかし、ヒトがコンピューターを操作するときには、表立っては活躍していない。
				</p>
				<ul>
					<li>
						外部を感知するものは皮膚感覚である。
						ヒトの場合、皮膚は1.8平方メートルもあり、皮膚感覚の受容器は散在している。
						皮膚表面には触覚があり、皮膚の深部には圧力を感じる圧覚がある。
						その他、温覚、冷覚、痛覚の受容器がある
						（<a href="#rIN">[rIN]</a>）。
					</li>
					<li>	
						一方、筋、腱、関節などに、自分の状態を感知する固有受容器がある。
						筋がどれだけ伸びているか、どれだけの力で引っ張られているか、角度はどうか、を感知している。
						これは姿勢を制御したり体を動かすためにある
						（<a href="#rIN">[rIN]</a>）。
					</li>
				</ul>
			</details>
			
			<details open><summary><strong>指先は鋭い</strong></summary>
				<p>
					皮膚感覚受容器は10の7乗個あり、神経は10の6乗個ある。
					指先には1平方ミリメートル当たりに1個の神経線維が大脳に接続しているという。
					2点を弁別する能力は、手のひらでは1センチメールだが、指先は2ミリメートルである
					（<a href="#rMS">[rMS]</a>）。
				</p>
				<p>
					このような手指の神経の細かさが、ヒトが箸やハサミを上手に扱う基礎となっている。
					大脳皮質の中で、手指を感じ、また動かす部分は、大きな部分を占めている（<a href="#penfield_map">「手は第2の脳」</a>）。
				</p>
			</details>

		</div>

	</div>

	<div id=effector  class="pagebreak"><hr><h3>2.3 効果器官</h3>

		<div><h4>2.3.1 手、骨、筋</h4>

			<details open id=hand_and_brain><summary><strong>手と脳の進化の相乗効果</strong></summary>
				<p>
					手、骨、筋という効果器官は、内部状態を感知しつつ、機械動作する。
					手には優れた皮膚感覚がある。
					一方で、視覚は、手の動き周辺を観察するのに、強力な機能を備えている。
				</p>
				<p>
					手の重要な役割は、人類史から見ると理解できる（<a href="#rCL">[rCL]</a>）。
				</p>
				<p>
					320万年ほど前、ルーシーと呼ばれる化石により、類人猿が二足歩行を始めた。
					骨盤の形から二足歩行していたことが分かったが、脳の大きさはチンパンジーと変わらなかった。
					二足歩行のメリットは、四足歩行に比べて25%のエネルギーで移動できることだった。
					その後、240万年ほど前に、ホモ・ハピルスが登場した。
					ホモ・ハピルスは、肉を骨からそぎ落とす鋭利な石器を作り利用していた。
					ホモ・ハピルスの脳は、ルーシーの倍（だが、ホモ・サピエンスの半分）に大きくなっていた。
					道具を作るには、目と手を正確に連動させる必要がある。
					それが脳に刺激となり、脳の発達を促した。
					一方、大きな脳はたくさんのエネルギーを必要とする。
					たくさん食べる必要があり、狩猟してとった肉を食べた。
					肉をたくさん得るには、より高度な道具が必要となる。
					こうして、手と脳の進化の相乗効果の連鎖が始まった。
				</p>
				<p>
					脳はますます大きくなり、手指はますます器用になり、道具はますます精緻になった。
					200万年前ごろ、ホモ・ハピルスは、ホモ・エレクトスへ進化した。
					脳の大きさは、ホモ・ハピルスの1.5倍となった。
					ホモ・エレクトスは、槍を使い、火で食物を消化しやすいように変えられた。
					ホモ・エレクトスは、100人くらいの集団で暮らしていたとされる。
					170万年前頃に、ホモ・エレクトスはアフリカを出て、別々の地域に進出し、五種類のヒト属に分化したらしい。
					その後35万年前ほど、ホモ・サピエンスと同じくらいの脳を持つ、ネアンデルタール人が現れた。
					頭蓋骨の底には、発声に必要な神経の束を通す穴があり、多様な発声ができたと想定されている。
					また、ネアンデルタール人は、音楽、宗教、言語を持っていた。
					その後、19万5000年前に、ホモ・サピエンスが登場した。
					ホモ・サピエンスは、7から5万年前に、アフリカから出て世界中へ広がった。
				</p>
				<p>
					<div align="center">
						<img width=40% src="brain_capacity_of_primates.png"> 
						<p>[http://user.keio.ac.jp/~rhotta/hellog/lib/brain_capacity_of_primates.pngより]</p>
					</div>
				</p>
				<p>
					以上のように、手によってこそ、脳が大きくなった、とよく言われる。
					あとで<a href="#penfield_map">「手は第2の脳」</a>に示すように、
					体性感覚および運動制御の中で、手指に関する脳機能は、確かに、大きな部分を占めている。
					ほかの説もある（<a href="#n04">[脚注04]</a>）。
				</p>
			</details>

			<details open id=hand_dexterity><summary><strong>手指は器用</strong></summary>
				<p>
					ヒトの手は、器用である。
					ヒトの手には5本の指があり、指は3つの関節を持ち、動く。
					そして、親指は、ほかの指と独立して動き、ほかの指と対面して動作し、物をつかんだりできる。
					この対面動作できることが、ヒトの手指の特徴である。	
				</p>
				<p>
					<div align="center">
						<img width=40% src="fingers.png"> 
						<p><a href="#rYH2">[rYH2]より</a></p>
					</div>
				</p>
				<p>
					手指ができる運動は、以下のように分類されている。
					<ul>
						<li>握る(grip)</li>
						<li>つまむ(pinch): 指先(tip)、指腹(pulp)、側面(lateral)、ひっかけ(hook)、指間はさみ(finger)</li>
						<li>ねじる(twist)</li>
						<li>押す(push)</li>
						<li>すくう(scoop)</li>
					</ul>
					これらの組み合わせで、日常生活を送っている。
				</p>
				<p>
					<div align="center">
						<img width=40% src="finger_behavior.png"> 
						<p><a href="#rYH2">[rYH2]より</a></p>
					</div>
				</p>
				<p>
					ここで、ヒトが現在のコンピュータを操作する際には、「押す」（クリック、キータイプ、タップ、スワイプ）しか使っていないことに注意しよう。
					コンピュータ操作では、ほかの多様な手指の機能を使っていない。
				</p>
			</details>

			<details open id=body_supports_hand><summary><strong>身体と視覚が手指の器用さを支えている</strong></summary>
				<p>
					手指は、体幹が支えて身体がある姿勢をとって、肩や腕が支え、腕・肘・手首が動き、手首が支えることで、はじめて器用に動く。
					体性感覚と機械的効果器が、手指の器用さを支えている。
					まさに、手指の器用さは、全身運動の結果である。
				</p>
				<p>
					手指は、視覚に障がいがある方を除き、多くの場合、目に助けられてこそ機能する。
					例えば、ドアノブの位置を目にしながら、そこをつかんでひねりドアを開く。
					そこに、ドアを開けるという意図で行動した点に、あいまいさはない。
					例えば、左手に持った茶碗の位置を、目で感知しながら、右手の箸を動かして、茶碗のごはんを摘まみ上げる。
					例えば、机の上に置いた紙を左手で押さえて、その四角い方向を目で確認したうえで、右手のはさみで紙の一片を切り取る。
					例えば、キーボードがあってキートップの文字マークを見て、手指は動く。
					音声だとbaなのかpaなのかわかりにくいこともあるが、
					手指の動作は、目でキートップを確認しているため、コンピュータに対して、曖昧さなくpaという言語表現がされる。
					このように、手指は視覚と一緒に働くことで、様々な機能を果たす。
					ヒトの構築した人工的な構築物の中を動き回る際も、その手指の動作に曖昧性はない。
				</p>
			</details>

			<details open id=fitts_law><summary><strong>手指は移動距離に束縛される</strong></summary>
				<p>
					手指は、機械動作なので、この距離の束縛を受ける。
					手を動かし、マウスで、別の場所のターゲットに移すという運動負荷に関し、
					その移動時間は、移動距離が大きいほど大きく、対象の大きさが大きいほど小さくなる。
					これをフィッツの法則という。
				</p>
				<p>
					例えば、受信箱のメールのTriageを取り上げてみる。
					メールをざっと眺めて丁寧に読みたいメール以外は即削除する作業である。
					スマホでなら、指先の操作ですむので、素早くできる。
					しかし、デスクトップであると、画面が大きく、手から離れているので、マウスごしにGUI対象をポチポチするする必要がある。
					読む場所を設定する画面上の位置と、削除ボタンは離れているので、いちいちマウスポインターを移動する手間がある。
					そのため、メールのTriageは、スマホに比べて、デスクトップPCでの作業には、時間がかかる。
					手指の操作は、このように、距離の束縛を受ける。
				</p>
			</details>
		</div>

		<div><h4>2.3.2 発声</h4>

			<details open id=vocal_organ><summary><strong>発声器官の進化</strong></summary>

				<p>
					ヒトの音声産出は、横隔膜・肺という呼吸器官から空気を吐き出し、咽頭・声帯で音源を作り、
					舌・咽頭・口唇で音に変化を与える、と3つの器官要素によって生産される
					（<a href="#rKH2">[rKH2]</a>）。
				</p>
				<p>
					<div align="center">
						<img width=50% src="resonance_cavity.jpg"> 
						<p>[https://band-knowledge.com/vocal-14/より]</p>
					</div>
				</p>
				<p>
					脊椎動物が陸に上がって肺呼吸をするに伴い、空気の振動を起こす能力が、生殖や警告などに利用されて進化したようである。
					一方、舌は、食べ物を飲み込むときに、精緻に、素早く動くように進化していた。
					咽頭を含む声道は、食べ物の摂取・嚥下を、不随意的に担う。
					それとともに、ヒトでは随意的に、話し言葉を発声する役割を持つ。
				</p>
				<p>
					ヒトは、320万年前に直立歩行を始めた。
					そして、火を使うことで、やわらかい食事をとるようになった。
					そして、頭の重量のバランスをとるため頭の前後径が短くなった。
					柔らかい食事による顎の縮小とあいまって、脳が前にせり出した。
					その結果、舌は、前後に圧縮されて上下に厚みを持ち、丸い形状になった
					（<a href="#rFJ">[rFJ]</a>）。
					一方、類人猿では、咽頭の位置は下がり始めた。
					そして、ヒトでは、喉に大きな空間をつくり、多様な共鳴を生み出せるようになった。
					ヒトの声道は、口腔と咽頭腔という二つの共鳴腔がほぼ垂直に結合していて、それぞれ独立に変形させることができる。
					二共鳴管構造である。
					ヒトの舌は、丸みがかっていて、形状変化で多様な音調整を可能にした。
					また、ヒトは、発話の際、1秒間に5〜6回、口が開閉する
					（<a href="#rKH3">[rKH3]</a>）。
					そして、ヒトは話をする時に、一回息を吐くという瞬間で、多くの異なる音を発声するために
					声道形状を連続的に素早く変形させることができる点が、特異であるとされる
					（<a href="#rNT">[rNT]</a>）。
					ネアンデルタール人の舌骨は、ヒトと同様の形態をしていたことから、
					ネアンデルタール人もヒトと同様の発話が可能だったとされる。
				</p>
			</details>

			<details open id=talkative_animal><summary><strong>ヒトはおしゃべり</strong></summary>
				<p>
					あとで<a href="#penfield_map">「手は第2の脳」</a>に示すように、
					体性感覚および運動制御の中で、発声器官周辺に対応する脳機能は、大きな部分を占めている。
					また、音響言語を専門に処理する部分も、大脳の大きな部分を占めている。
				</p>
				<p>
					また、ヒトは成長に伴って、言語を習得し、多様な音声生産ができるようになる。
					この音声生産の可塑性ないし学習という点も、特徴的である
					（<a href="#rKH2">[rKH2]</a>）。
					それを可能にする神経系の進化が、ヒトの発声能力の背景にあった。
					ヒトの脳は、後で<a href="#rain_plasticity">「脳の可塑性」</a>で見るように、
					後天的に形成される部分が大きい。
					言語はそもそも社会活動である。
					ヒトは、手で道具を作ることで脳と相乗効果を持ったのと同じくらい、
					ヒトの社会活動とその言語活動が、脳の成長と相乗効果を持ってきたことは、容易に想像できる。
				</p>

			</details>

			<details open id=text_productivity><summary><strong>発声のテキスト生産速度は指の5倍</strong></summary>
				<p>
					ヒトの言語表現のスピードは、経験的におおよそ、
					しゃべりなら160語/分、手書きは30語/分、タイプなら40語/分である。
					しゃべりは、社会的な環境で育った健常者ならば誰でもできる。
					一方、手書きは教育が必要であり、キータイプなら機器操作に慣れが必要である。
					音声での言語表現は、習熟の必要がないばかりか、指よりも5倍速い。
				</p>
				<p>
					音響言語は、ヒトがコンピュータを操作するインターフェイスとして、ようやく使い始められたばかりである。
					まだ、広範囲なアプリで活用されていない。
				</p>
			</details>

		</div>

	</div>

	<div id=brain  class="pagebreak"><hr><h3>2.4 脳・神経系</h3>

		<details open><summary><strong>脳の構成要素</strong></summary>
			<p>
				脳は、大脳、小脳、脳幹からなる。
			</p>
			<p>
				<div align="center">
					<img width=50% src="brain.jpg"> 
					<p>[https://atamanavi.jp/169/より]</p>
				</div>
			</p>
			<p>
				<ul>
					<li>大脳：
						大脳は、高度な機能をこなす。
						大脳には、皮質、辺縁系、基底核がある。
						<ul>
							<li>
								大脳皮質は司令塔である。
								前頭葉、運動関連領域、頭頂葉、後頭葉、側頭葉がある。
								<ul>
									<li>
										前頭葉は、前頭前野と運動関連領域からなる。
									</p>
									<p>
										前頭前野は、ほかの皮質部位と接続してし、他からの情報を得ては指示する関係を持つ。
										前頭前野は、一時作業記憶を持ちその上に認知機能を持つ。
										視覚から得た、空間・形態・色情報を処理する。
										また、前頭前野は、他人の心を理解する。
										辺縁系という爬虫類以来の本能的情動の仕組みを、コントロールしている。
									</li>
									<li>
										運動関連領域は、脊髄に指示を送る。
										外部刺激に応じて、手を伸ばし把握したり、両手を協力させるなど、運動を準備し企画構成する。
										また、記憶を踏まえて運動する機能を持つ。
									</li>
									<li>
										頭頂葉は、運動に関連する高度機能を受け持つ。
										身体部位ごとの局在性がある部分から、複数の部位にまたがる高次な連合を担う部分へ、階層的につながっている。
										姿勢や自己身体を認識したり、手を能動的に動かして探索したり、手で操作する道具を手の延長としてイメージする。
									</li>
									<li>
										後頭葉は、視覚情報の中枢である。
									</li>
									<li>
										側頭葉は、形態情報を認識したり、聴覚情報を処理する。
										ここでは、表情など特定の視覚刺激に選択的に反応したり、海馬・偏桃体の記憶と照合したりする。
										聴覚系は、前頭葉とともに何がを認知し、頭頂葉とともにどこがを認知する。
									</li>
								</ul>
							</li>
							<li>
								辺縁系は古い脳であり、哺乳類では皮質のほうが徐々に大きくなり、これが覆われるようになった。
								辺縁系は、意欲や情動・本能行動の中心的司令塔である。
								辺縁系には、海馬、偏桃体、帯状回などがある。
								偏桃体は、外部刺激に対し、皮質での処理を待たずに、瞬間的な反応・行動を起こす。
								海馬は、記憶と空間認識に関与する。
								帯状回は、他者の心を想像する能力にかかわるらしい。これを「心の理論」という。
							</li>
							<li>
								大脳基底核は、(特に内発的な）随意運動の制御、認知・情動の制御、学習の強化を担っている。
								皮質の各部（運動領域、連合領域、辺縁系）と接続し、また脳幹と接続し、それらの機能を調整している。
							</li>
						</ul>
					</li>
					<li>小脳：
						小脳は、脊髄に伝わってきた体性感覚と視覚から、身体の姿勢を制御したり、外発的な運動を指令・学習する。
						例えば、足の動かし方を意識せずに歩いたりできる。
						例えば、首が回転したときに、反射的に眼球が逆回転し、視界のブレをなくす。
						これらは小脳のおかげである。
						運動のパターンを学習する。
						自転車を乗れるようになるのは、小脳のおかげである。
					</li>
					<li>脳幹：
						脳幹は、脊髄からシグナルを伝達して、基本的な体内機能と反射を指令する。
						爬虫類脳とも呼ばれ、生命維持・繁殖に必須な機能を果たす。
						例えば、呼吸や心臓の拍動など不随意・自律的な機能をこなす。
						脳幹は、中脳、間脳（視床、視床下部など）を含む。
						間脳は、感情を管理する。 
					</li>
				</ul>
			</p>
		</details>

		<details open id=小脳は所作の匠><summary><strong>小脳は所作の匠</strong></summary>
			<p>
				大脳の皮質には神経細胞が140億あるが、小脳には1000億個ある
				（<a href="#rMA">[rMA]</a>）。
				小脳は、ヒトでは脳全体の15%程度の容積しかないが、脳全体の神経細胞の約半分が存在する。
				末梢感覚器や大脳皮質からの入力を受けて、運動を学習する
				（<a href="#rHN">[rHN]</a>）。
				鋏を扱うときの動作、箸を扱うときの動作、話す動作、歩く動作、大工の技、庭師の技、などに、実は大量の知能が、詰まっている。
				よく、ヒトを記憶している知識、いわば見える氷山の一角、で比べたりする。
				受験戦争とか。
				が、小脳の持つ知恵は、目立たないが、生活のための知能である。
				その部分が、大脳皮質よりもはるかに大きく、ヒト共通である。
			</p>
			<p>
				ロボットに、ヒトや動物の所作をまねさせて、歩かせたりするアプローチがある。
				それらは、小脳を機械で再現しようとしている。
			</p>
		</details>

		<details open id=penfield_map><summary><strong>手は第2の脳である</strong></summary>
			<p>
				大脳皮質には、体性感覚野と運動野において、機能が特定の場所に関連付けられる箇所がある。
				運動野を見ると、発声関係と、手指の部分が大きいことに気づく。
				発声関係は、言語機能と関係して大きな部分を占めるのは理解しやすい。
				一方、手は第2の脳といわれるほど、大きな運動知能を持っている。
				大脳皮質は、ほかの脳・神経部位を制御・指令する部位として、哺乳類になって発達した部分である。
				それ以前は、脊髄や小脳レベルで反応することで、足りていた。
				また、哺乳類の中でも、人は特に大脳皮質が発達した。
				ヒトの大脳皮質の、感覚と運動に局所的な部分のかなりが、手指と発生器官周りに対応する。
				そのことは、手指と発声が、ヒト知能の本質的な部分であることを示唆する。
				<p>
					<div align="center">
						<img width=60% src="cerebral_penfield_map.jpg"> 
						<p>[https://www.akira3132.info/cerebral_cortex.htmlより]</p>
					</div>
				</p>
				従来、人がコンピュータを操作するときに、手指に頼って来たのは、その性能から見て、納得できる。
				しかし、発声のほうは、利用されていない。
			</p>
		</details>

		<details open id=association_area><summary><strong>人は総合する動物</strong></summary>
			<p>
				感覚野と運動野という機能局在な部分を除くと、大脳皮質の約2/3にも相当する広い領域が、連合野と呼ばれる。
				連合野は、高次な脳機能を具現化している皮質領域である。
				感覚情報の高度な統合による認知、複数の感覚の総合、感覚と運動の統合、過去の経験（記憶）と関連、随意運動、
				情動行動、言語機能、精神機能、作業記憶（ワーキングメモリー）などを担う（<a href="#rKW">[rKW]</a>）。
				個々の感覚・運動機能よりも、それらを統合した部分が、ヒトのヒトたる部分である。
				そこが、ヒトの特徴である。
			</p>
			<p>
				現在、ヒトがコンピュータを操作するインターフェイスは、手で操作し、目で知覚するのが、主である。
				しかし、このような特定のチャネルだけに頼るやり取りは、ヒトの脳の進化結果に照らしてみると、
				ヒトの生体機能に反している異常なことである。
			</p>
		</details>

		<details open id=magical_number><summary><strong>短期記憶は4個まで</strong></summary>
			<p>
				一度に記憶できるのは、4個までだそうである。
				長期記憶から、あるカテゴリーで想起できるのは、3個までだそうである。
				チンパンジーは、4個の数字までは95%の正確さで覚えられるが、5個で65%まで落ちるそうである。
				また、ヒトは、選択肢が3つか4つまでの場合に限り、選ぶことができる（<a href="#rSW">[rSW]</a>）。
				これ以上の選択肢が、テレビのリモコン叱り、ヒトが機械を操作するインターフェイスに、あふれている。
			</p>
		</details>

		<details open id=brain_plasticity><summary><strong>脳の可塑性</strong></summary>
			<p>
				ヒトの遺伝情報であるヒトゲノムは約30億個の塩基で構成される。
				一つの塩基はATGCの4種類の分子のいずれかで2ビットの情報をもつ。
				したがって、ヒトゲノムは2x30憶=60億ビットの情報量を持つ。
				一方、新皮質の神経細胞は100億個で、神経細胞当たりシナプスの結合数を1000個とし、結合係数は抑制と興奮の2値（1ビット）とすると、
				脳の情報量は約10兆ビット（100億X1000=10兆ビット）となる。
				ヒトが、生まれる際に受け継いだ情報量よりも、ヒトが成人になって活用する情報量のほうが、1600倍大きい。
				つまり、脳の配線の大部分は遺伝ではなく、生後決定される
				（<a href="#rSO">[rSO]</a>）と。
			</p>
			<p>
				つまり、ヒトは学習する。
				後天的な事柄、社会や文化というヒトの抽象物が、ヒトの生物としての生得物と比べて、1600倍。
				生物的な進化よりも、社会的な思考の成果が、断然影響力を持つかもしれない。
				これは、ヒトが今後どのような未来を持つことになるのか、その範囲が見こせないくらいだなと想像させる。
				ヒト集団は、生物の遺伝子の論理を超えて、歩み始めるかもしれない。
			</p>
		</details>

		<details open id=cognitive_interference><summary><strong>人は時分割でマルチタスクできる</strong></summary>
			<p>
				ヒトは、歩きながら、ものを考えることができる。
				今、そういう無意識的な行動でなく、ある程度注意力を要する作業のことを想定する。
				ヒトは、一見、考えるという音響的言語活動と、文をタイプするという指の機械的・視覚的・空間的な言語活動を、
				うまく同時にこなせる。
				認知活動と脳の資源の関係の以下のようなモデルを<a href="#rCW">[rCW]</a>が提唱した。
			</p>
			<p>
				<ul>
					<li>
						知覚・認知のための資源と、反応の選択と実行の資源とは、独立である。
						例えば、パイロットは飛行機の込み具合を認識しながら、同時に適切な対応をとる。
					</li>
					<li>
						音響的作業と視覚的作業とは、それぞれの作業を複数やるよりは、2つの異なる種類の作業をより効率的に時分割できる。
						例えば、何か文章を考えているときに他人とおしゃべりはできない。
						しかし、車の運転手は、声で指示を受けて、ハンドル操作ができる。
					</li>
					<li>
						周辺視野と注目視野とは、異なる資源を使う。例えば歩きスマホができる。
					</li>
					<li>空間的過程と、音響的過程とは、効率的に時分割できる。
						例えば、運転中、ハンドル操作以外の手操作は運転を中断させるが、声で機器操作するのはより楽である。
						耳で捕まえた議論のキーワード（音響的言語活動）を、記録する（空間的言語活動）ことができる。
					</li>
				</ul>
			</p>
			<p>
				ところが、最近の研究では、ヒトが一度にできるのは一つだけである（<a href="#rSW">[rSW]</a>）とわかったという。
				ヒトは、素早く切り替え、時分割して、二つの作業を同時にこなしているように見えるらしい。
				時分割できるとはいえ、自動車の運転をしながら、着信した電話で会話をするのは、実は注意力を散漫にしている。
				音楽を聴きながら、勉強したり、仕事をしているのは、実は、作業効率を下げている。	
				そういうことである。
			</p>
			<p>
				<a href="#rYT">[rYT]</a>では、言語活動に、視覚的言語活動と、音響的言語活動とがあるという。
				音声言語と文字言語といってもよい。
				頭で、数を数えながら（音響的言語活動）、しゃべること（音響的言語活動）はできない。
				しかし、頭で数字カードで数字が増えていくのをイメージしながら（視覚的言語活動）、しゃべること（音響的言語活動）はできる。
				つまり、ヒトは、視覚的言語活動と、音響的言語活動とは、効率的に時分割できる。
			</p>
			<p>
				ヒト同士の会話は、最初は身振り手振りの空間的な象徴だったかもしれない。
				その後、長く音響的言語が支配した。
				その後、文字、紙、ペン、印刷など、空間的な手段による言語を利用した。
				ヒトは、これら音響言語と視覚的言語とを、効率的に活用できる。
			</p>

		</details>
	</div>

</div>

<div class="pagebreak"><hr><h2>3. 言語と文字</h2>

	<details open><summary><strong>ヒト言語の前提</strong></summary>
		<p>
			ヒトの特徴について、振り返る。
		</p>
		<p>
			ヒトの脳は大きい。
			純粋な大きさでいえば、クジラのほうがヒトよりも脳は重い。
			クジラは、神経細胞の数も1億を超え、人と大差ない。
			が、脳の重さに体重を加味したある指標だと、ヒトの脳の大きさはほかの動物を引き離す。
			どんな要因が、ヒトの脳を大きくしたのか？
		</p>
		<p>
			400万年前の初期人類のアウストラロピテクスは、直立二足歩行を始めた。
			そして、ホモ・エレクトスで、直立姿勢が完成した。
			手が自由になったことが、手先の器用さを促した。
			すでに霊長類は、ほかの動物に比べて、手先が器用であった。
			ヒトでは、親指がほかの指と対面するようになって、上手にものをつかめた。
			次第に、ヒトはを手指で複雑な作業ができるようになった。
			手先の器用さと、道具の利用と、脳の発達に、相乗作用があったということは、
			<a href="#hand_and_brain">「手と脳の進化の相乗効果」</a>で見た。
		</p>
		<p>
			一方、霊長類では、社会的関係が強く、複雑である（<a href="#rCP">[rCP]</a>）。
			それが、脳の発達を促したという説がある。
			社会脳仮説といわれる。
			社会関係があったために、毛づくろいなどの会話が発達した。
			またほかの個体の道具使用を模倣できた。
			そして、他個体の心を推察することができた。
			他個体の心を推察することを、心理学や脳科学で、「心の理論」を持っていると言う。
		</p>
		<p>
			類人猿でも、他個体の視線や指さしによって、他個体の注意している対象へ注意を向けるという、共同注意行動がみられるという。
			また、類人猿でも、他個体の表情に対し、脳のヒトと同様な脳の場所が局所的に興奮するという。
			他人を意識する心の理論は、自己意識の源でもある（<a href="#n07">[n07]</a>）。
			サルやヒトには、ミラーニューロンというものがあることが知られている。
			ある道具を使うとき、身体の一部とみなして興奮する神経細胞がある。
			他個体が、目の前でその道具を使うとき、自分が操作しているのでないにもかかわらず、同じ神経が興奮するという。
		</p>
		<p>
			手と道具ということと、社会脳という二つの話は、直立二足歩行を共通要因として、つながる。
			直立二足歩行が、ヒトの社会性の一層の強化を招いた（<a href="#rYH">[rYH]</a>）。
			直立すると、腰周りを細める必要がある。
			産道が狭まった。
			そのため、子供は小さい早い時期に出産したほうが有利だった。
			子供は、ほかの動物よりも、未熟な状態でうまれることになった。
			未熟な子供には、周りで守る仲間が必要だった。
			子育てのために、大人の社会が必要であった。
		</p>
		<p>
			他方で、<a href="#vocal_organ">「発声器官の進化」</a>でみたように、ヒトは、発声器官を進化させていた。
			そして、上記のような発達した社会脳のもとに、音響言語での会話が発達した。
			逆に、会話が、さらに社会脳を大きくしたことも想像できる。
			社会と会話と脳の成長の相乗作用があった。
		</p>
		<p>
			さらに、30万年前ほどには火を使い始めたことも、脳の進化を促進した（<a href="#rYH">[rYH]</a>）。
			火を使った調理によって、消化が楽になった。
			そのおかげで、小さな歯と短い腸で足りるようになった。
			そして、得たエネルギーを脳に使うことができた。
		</p>
	</details>

	<details open id=言語の発生><summary><strong>言語発生</strong></summary>
		<p>
			言語の発生については、諸説ある。
		</p>
		<p>
			マイケル・コーバリスは、身振り・手振りで表現することが、言語のもとになったと唱えた
			（<a href="#rMC">[rMC]</a>）。
			身振り・手振り以外に、赤ちゃんでも利用し理解する、表情、まなざし、指さしなども重要な会話手段である。
			これらは、空間的な情報伝達手段である。
			ヒトの場合、40万年前ほどのころ、複雑な発生器官を持つにいたり、音声言語を使いだした。
			ヒトが会話をする際、身振り・目振り、手振りという、空間的な表現で弁別してきた意味を、
			生物の進化で体得した器官を活用することで、次第に音響的に表現することを獲得したのかもしれない。
		</p>
		<p>
			養老孟司によると、言語には時間的言語（音声）と空間的言語（文字)とがある（<a href="#rYT">[rYT]</a>）。
			視覚は光という電磁波を感知し、聴覚は空気振動を感知し、独立に進化した。
			視覚は空間の中に時間と無関係にあり、音は時間の中に空間と無関係にある。
			ヒトは、それら異なるものを連合し統一した。
			それが、ヒトにおける言語の発生ということだと。
			ヒトの言語は、視聴覚が一体なのである（<a href="#n06">脚注06</a>）。
		</p>
		<p>
			歴史家は、ヒトの言語特徴を指摘する。
			ハラリ（<a href="#rYH">[rYH]</a>）によると、ネアンデルタール人やデニソワ人でなくホモサピエンスだけが生き残ったのは、
			架空のことを語る柔軟な言語のせいだという。
			7万年前から3万年前にかけて、ヒトは、大人数で、アフリカ大陸を出て、
			ネアンデルタール人を絶滅させ、中東、ヨーロッパ、アジア、オーストラリアに広まった。
			ヒトは、そのころ、船、ランプ、弓矢、針などを発明した。
			宗教、交易、社会的階層も生まれた。
			この時期のヒトの変化を「認知革命」と呼んだ。
		</p>
		<p>
			どんな動物も、なにがしかの言語を持って、仲間と会話する。
			例えば、ハチはダンスというジェスチャーで、餌の場所を会話する。
			多くの動物は、フェロモンという匂いの化学物質で、生殖のための会話をする。
			ある魚は、電気で会話するという。
			ある動物は、体表の色で異性にアピールする。
			カラスやシジュウカラは、鳴き声の種類で、仲間と会話する。
			ある動物たちは、体臭を残すことで、縄張りを他個体に会話する。
			これらは、生存・生殖に密着した会話である。
			一方、サルは、声を、他個体を欺くためにさえ使うこともあり、
			類人猿は、社会的な関係を操作するのにも、言語を使い始めていた。
		</p>
			サルからさらに進化して、
			ハラリによれば、ヒトは、認知革命によって、実在しない架空の事物について語る柔軟な言語を持ったという。
		</p>
		<p>
			動物の世界では、社会的集団は、安定した関係を保つには、
			個体数150（ダンパー数）が限界だという（<a href="#rSW">[rSW]</a>）。
			ところが、ヒトが集団移動したときは、ダンパー数を越えて移動し、協力したに違いない。
			また、その後、都市などを建設したりもする。
			これを、ハラリは、ヒトは認知革命によって、架空の物語、つまり伝説、神話、宗教など、
			彼の言葉でいうと虚構を共有できる能力があったからこそだという。
		</p>
	</details>

	<details open><summary><strong>言語の効果</strong></summary>
		<p>
			言語の効果は明確である。
			ヒトは、動物的に知覚に反応する以上に、抽象的な概念、シンボルを思念する力を持っていた。
			ヒトは、言語によって、概念を組み合わせ、無限の種類の意味を表現し、理解することができるようになった。
			言語は、実在するあるいは抽象的な何かを指示し、構文構造を持って、組み立てる。
			構文構造は、再帰的なので、実質、文の種類は無限である。
			感覚から離れた複雑な概念をも表現できることが、ヒトの世界を広げた。
		</p>
	</details>

	<details open id=文字の発明><summary><strong>文字の発明</strong></summary>
		<p>
			およそ5000年前のシュメール文明に、ヒトが文字を使った初めての痕跡が残っている（<a href="#rCL">[rCL]</a>）。
			ヒトは、器用な手指と、それを導く高度な視覚があって、文字を操れるようになった。
			文字によって、ヒトは、時間と空間を超えて、言語を通して、様々な情報を伝達できるようになった。
		</p>
	</details>

	<details open><summary><strong>文字の効果</strong></summary>
		<p>
			それ以来、ヒトの会話は、生活圏内の人との会話による日常的なものと、時空を超えた書き物によるものと、2種類のものに分かれた。
			文字によって、ダンパー数を超えて、多数のヒトに会話し共有する道が開けた。
			音声は、近場の人にしか伝わらない。
			しかし、文字によって、時空を超えて伝達されるということは、個体や生活空間の枠を超えることである。
			集団としての文化形成、集団としての教育、集団としての思考の深化、に役立った。
			構成個体数が多くばなれば、その相乗作用によって、いわばネットワーク効果が生まれる。
			インターネットで起きた変化に類似の爆発的な変化が、文字によってもたらされたのだろう。
		</p>
		<p>
			さらに文字は、時間的な次元でも効果をもたらした。
			書き物による会話では、文字を認識するためにもっぱら目を使う。
			ヒトは、言語に文字という視覚的な要素を合わせた。
			それによって、ヒトは、文字として、外部に永続的に記憶として保持することができるようになった。
			時間とともに流れ去る音声言語と異なる点である。
			紙切れに書いたメモである。
			知識を記録した本である。
			そして、世界中の人に共有されるインターネットである。
			外部的な永続性は、個体にとっても、意義は大きい。
		</p>
		<p>
			外部に視覚的に文構造を保持できるため、言語表現も様変わりした。
			省略がしやすくなった。
			文字の発明の前、ゴータマ・ブッタの弟子が生きた時代、師の教えを受け継ぐには、口伝しか方法がなかった。
			口伝の記録は、同じことの繰り返しが多い。
			音声表現は、韻を踏んだものだと、覚えやすいという。
			また、教えを伝えるには、言い間違えが起きないように、詳細を何度も繰り返した、ともいう。
			しかし、文字が利用できるようになると、韻律は関係なくなった。
			また、前に書いたことは省略できるようになった。
			目で、即座に先行する文章を確認できるから。
		</p>
		<p>
			視覚的言語なので、一度に多くの言葉を認知することができる。
			永続的な記録は、目で何度も確認できるため、記憶しやすい。
			省略もできるため、記憶効率もいい。
			永続的な記録言語は何度も見直して、思考を深めることができる。
			視覚的な文字によって、個人は、望めばいくらでも知識や思考を深められるようになった。
		</p>
	</details>

</div>

<div class="pagebreak"><hr><h2>4. コンピュータ</h2>

	<div><h3>4.1 コンピュータという道具</h3>

		<details open><summary><strong>コンピュータの恩恵</strong></summary>
			<p>
				ヒトは、動物的に知覚に反応する以上に、抽象的な概念・シンボルを思念する力を持った。
				ヒトは、言語によって、概念を組み合わせ、無限の種類の意味を表現し、理解することができるようになった。
				ヒトは、文字によって、時と空間を超えて、意味を共有できるようになった。
				言語や文字によって伝達されるものを情報という。
				ヒトは、コンピュータによって、情報を記憶し、組み合わせて計算する道具を得た。
				コンピュータは、疲れ知らずに、どれほどでも、仕事をこなせる。
			</p>
			<p>
				コンピュータの中に、ディジタル・データがある。
				情報のディジタル化によって、ヒトは、紙の物理的な劣化から解放された。
				情報のディジタル化によって、記録、コピー、更新が楽になった。
				情報に対する検索やソートやチェックといった計算機能が、難なく利用できるようになった。
			</p>
			<p>	
				コンピュータをネットワークでつなぐインターネットが生まれた。
				このインターネットによって、ほぼ誰でも情報を発信し誰の発信内容もアクセスできるようになった。
				文字によってはじめられた外部の知識が、ほぼ誰でも生産し、共有し、消費する、巨大な情報倉庫になった。
				ヒトは、誰とでも即座につながり、いつでもどこでも巨大な情報倉庫に自由にアクセスできる。
			</p>
			<p>
				コンピュータとディジタルデータは、ヒトが生み出したほかの抽象物を融合して、さらに発展した。
				スマホの地図アプリを見てみよう。
				地図は、ヒトの空間認知の抽象である。
				地図アプリの中の情報は、紙の地図と異なり、本屋へ出かけて買ってくる必要はない。
				紙の地図と異なり、スマホをポケットに入れておけば、かさばるものをカバンに入れて持ち歩く必要はない。
				紙の地図と異なり、探したい場所は、ズームインとズームアウトの組み合わせで、一瞬で見つかる。
				メモを付加することができる。
				スマホは、自分がいる場所をGPSで感知しているので、いまここから、目的までの道案内をしてくれる。
				GPSも、ヒトの作った抽象物である。
				近くのレストランをすぐに探せる。
				そこで人々が感じたことを読み、書気残す。
				口コミは、ヒト社会の生産物である。
				近くの道路の混雑具合をすぐに調べられる。
				道路、自動車も、ヒトの抽象物である。
				ヒトは、こんな素晴らしい道具を、だれで手にするようになった。
			</p>
		</details>

		<details open id=elite_intelligence_tool><summary><strong>昔、エリートが知能の拡張を目指した</strong></summary>
			<p>
				これから、ヒトと現在のコンピュータのインターフェイスを振り返る前に、歴史を振り返っておく。
			</p>
			<p>
				コンピュータは、第2次世界大戦で、ミサイルの弾道計算などに利用され、成功をおさめた。
				そして、ヴァネバー・ブッシュが、1945年に、
				「私たちが考えるように」（As We May Think）という論文（<a href="#rVB2">[rVB2]</a> ）を公開した。
				それは、機械や安くて大量に生産できるハードの進歩を踏まえて、
				それらが人の物理的な力を拡張したのだが、
				ヒトの精神は拡張していない、と指摘する。
				そして、これからの科学者は、蓄積された知識を活用する作業に向かうべきだと唱えた。
				それは、以降のコンピュータ研究者の先人たちにインスピレーションを与え続けたという。
				その論文には、こうできる、ああできる、という様々な未来ストーリーが描かれている。
				その中に、情報の関連性をたどれる、現在のハイパーリンクのアイデアさえ、書かれている。
			</p>
			<p>
				つまり、コンピュータは、その創成期には、<strong>『知能の拡張』</strong>が目標だった。
				単純だが膨大な計算を代わりにやってくれた道具なので、その延長である。
				頭脳労働を助けてくれる、知能を拡張する道具という発想である。
				現にその論文の終わりの部分には、現在、BMI（脳機械インターフェイス）と呼ばれる技術を理想とした記述がある。
				ここで、コンピュータの主な利用者は、科学者や研究者という、
				インテリないし<strong>『エリート』</strong>だったことにも留意しておく。
			</p>
			<p>
				ブッシュの影響を受け、その後、いくつかの重要な概念が生まれた。
				それまでは、コンピュータとのやりとりは、一括してデータを与え、一括して結果を受け取るやり方だった。
				リックライダーという人は、1960年に、ディスプレィごしに対話式にやり取りすることを提唱した。
				エンゲルハートという人は、1962年、「Augmenting the Human Intellect: A conceptual framework」という論文を発表した。
				そして、1968年に学会で、マウス、ワードプロセサ、COPY&PASTE、ハーパーリンク、リアルタイムな共同作業、画面分割、ビデオ会議などを
				デモし（<a href="#rDE">[rDE]</a>）、参加者をびっくりさせた。
			</p>
			<p>
				その後、回路の集積度が高まり、ハードが小型化していった。
				そこで、アラン・ケイが、1970年代にパーソナルコンピュータの概念を発明した。
				ここに、コンピュータのグラフィカル・ユーザー・インターフェイス（GUI）と呼ばれるものが、ほぼ完成する。
			</p>
			<p>
				この時、アラン・ケイは、子供でも誰でも使えるコンピュータを狙っていた。
				つまり、コンピュータの歴史の中で、初めて、想定利用者層の転換という画期的なことをやった。
				しかし、それまでの経緯から必然的に、そのインターフェイスに盛り込まれたものは、エリートの知能を拡張する枠組みに基づいていた。
				パソコンは、あくまでも情報を処理する道具であった。
			</p>
			<p>
				コンピュータがさらに小型化される流れを踏まえ、ユビキュタス・コンピューティングというビジョンや、
				タンジブルビッツという新しい考えも生まれた。
				しかし、エリートの知能拡張のための道具としての概念立てということに対し、エリートたちに批判的な思考は起きなかった。
				頭脳労働を助けてくれる素晴らしい道具を手にし、いろいろなことをやることに夢中になっていたのである。
				ファイル、ウィンドウ、アイコン、メニュー、ツールバー、カーソルなど、コンピュータ特有の抽象的な概念とそれを実装するコードが、どんどん積み上げられていった。
				そこに、ヒトが持つ意図とかヒトの身体性とかは、関心外だった。
			</p>
			<p>				
				そして、スマートフォーンが生まれた。
				利用者層は、エリートでなく、人である。
				用途は、情報の流通である。
				もはや、エリートの知能拡張という発想のインターフェイスでは、限界があるのは明らかである。
			</p>
		</details>

		<details open><summary><strong>コンピュータの操作概念は抽象的である</strong></summary>
			<p>
				認知心理学によると（<a href="#rCP">[rCP]</a>）、ピアノ、蛇、時計などの具体的なものの単語は、
				正義、能力、自我などの抽象的な単語よりも、記憶として保持しやすいという。
				具体的なものは、実世界の中で、目で見、耳で聞くものである。
				抽象的なものは、ヒトの頭の中でのみ存在するものである。
				具体的なものは、日常の活動で触れる。
				視覚、聴覚、触覚、体性感覚、脳・神経系という身体の各部位で、やり取りをする対象である。
				一方、抽象的なものは、ある程度の訓練ののちに、理解できるものである。
				それを扱うのは大脳皮質。
			</p>
			<p>
				コンピュータという道具を使って行うことは、ハサミのような具体性がない。
				いくつかの抽象的なステップをたどる。
				行う操作は、ハサミのように、結果がすぐに出るのではない。
				さらに、結果は、電子的な変化に過ぎず、具体的に見えない場合もある。
			</p>
			<p>
				ドナルド・ノーマンは、行為は七段階の手順を踏むと分析した（<a href="#rDN">[rDN]</a>）。
				「(1) ゴールがまずある。(2) 実行しようという意図が起きる。(3) 行為系列へ展開する。(4)行為系列を実行する。」 => 外界を操作し、結果が返る
				=>「(5) 外界の状態を知覚する。(6) 知覚したものを解釈する。 (7) 解釈を評価する。」 => (1)へ戻る。
			</p>
			<p>
				<div align="center">
					<img width=25% src="seven_steps_of_action.png"> 
					<p><a href="#rDN">[rDN]</a>より</p>
				</div>
			</p>
			<p>
				コンピュータを操作する場合に当てはめてみる。
				行為系列へ展開する(3)がコンピュータ寄りで、ヒトの日常の概念より抽象的である。
				また、結果も抽象的なので、(6)の結果の解釈の段階も、抽象的である。
			</p>
			<p>
				例えば、スマホの通知が鳴ったり鳴らなかったり、変えようとするところ、(1)、(2)は明確である。
				鳴ったか鳴らないか、(7)も明確である。
				一方、どうすれば鳴ったり鳴らなかったりを変更できるのかの(3)は、コンピュータの概念を理解していないと難しい。
				また、どういう設定になっていればどうなるかを確認する(6)も、コンピュータの概念を理解していないと難しい。
				例えば、高齢者は、面倒がって、スマホの画面をロックしないでおくことが多い。
				ロックするには、設定アプリを起動して、画面や安全関係のメニューを開いては閉じることを繰り返すことで探し、
				指紋とか顔認証とかパスコードとかパターンとかいう表現の意味を理解し、
				しかるべき設定変更をしなければならまい。
				やりたいのは、ほかの誰も開けないようにすることである。
				そのために理解し、やらなければいけない、中間的なことが多い。
				また、例えば、高齢者は、設定の類はうまくできない。
				それでもいじっているうちに、お休みモードやマナーモードになってしまうことがある。
				すると、戻せなくなって、LINEの音が出なくなった、とかいうトラブルにはまる。
				設定を戻すには、設定アイコンをたたいてと、概念構造に潜る、いくつかの中間ステップが必要となる。
				また、例えば、頻繁に見たいWEBページへのショートカットをスマホの画面に置きたい。
				それをやるには、いくつかのステップを経て、やっとできる。
			</p>			
			<p>
				こういう抽象的な過程は、どんなインターフェイスをかぶせても、ヒトには記憶しにくい。
				高齢者には、発見し、習熟するのも難しい。
				認知負荷が高い。
			</p>
		</details>

		<details open><summary><strong>理想的な道具とは誰でもすぐに目的達成できること</strong></summary>
			<p>
				現在のコンピュータは、理想的な道具なのであろうか？
			</p>
			<p>
				ドナルド・ノーマンは、スニーカーのマジックテープを技術の進歩のお手本とした（<a href="#rDN">[rDN]</a>）。
				ヒトは、靴の紐を結ぶという習慣に慣れていた。
				マジックテープがでてから、それは一瞬の簡単なことに変わった。
				障がい者、子供も、マジックテープの恩恵を受けた。
				また、自動運転は、別のお手本である。
				車の運転は、手段である。
				目的は、ある場所へ移動することである。
				自動運転は、苦痛なく、目的達成を果たしてくれる。
			</p>
			<p>
				目的とは何か。
				ドリルを買う人は、ドリルが欲しいのではなく、穴が欲しい。
				PC が欲しいのは、PC自体が欲しいのではなく、きれいな文章作成できるから。
				またスプレッドシートでグラフをつくりたいから。
				スマホが欲しいのは、いつでもどこでも情報を検索できるから。
				スケジュールを、いつでも参照できるから。
			</p>
			<p>
				道具は、抵抗なく受け入れられ、かつ、中間でない最終目標を達成できることが、理想である。
				コンピュータの操作は、目的達成のための中間の手順をユーザに強いる。
				中間的な手順は、コンピュータ特有の抽象的概念である。
				コンピュータは、道具として操作するのに認知負荷があり、理想ではない。
			</p>
		</details>
	</div>

	<div><h3>4.2 操作インターフェイス</h3>

		<details open><summary><strong>目と手指での操作</strong></summary>
			<p>
				ヒトは、環境から情報を獲得するときは、中でも、特に、視覚を使っている（<a href="#visual_anamal">「人は視覚的動物」</a>）。
				ヒトがコンピュータを操作するときも、情報の獲得はもっぱらモニターを目で見ることで行う。
				一方、コンピュータを相手に、情報を生産し反応するとき、ほぼ手指で行っている（<a href="#hand_dexterity">「手指は器用」</a>）。
				テキストをディジタル的に生産するときも、コンピュータに指示（コマンド）するときも、手指を使う。
				位置指定・対象選択も、手指で行う。
				手は器用である。目と親和性が高い。
				手で操作し、目でモニターする、ということに慣れてきた。
				そのため人がコンピュータを操作するときも、その延長上にいる。
			<p>
				ヒトにとって、目と手指が格別に優秀な器官だったことは、進化上、説明できる。
				しかし、ヒトは、自然な状態では、そもそも、いろんな器官を総合して環境とやり取りをしている（<a href="#association_area">「人は総合する動物」</a>）。
				目だけで環境を知覚し、手指だけで反応するというのは、むしろ特殊な状況ともいえる。
				その特殊な状況で、抽象的な概念体系に向かっている。
				コンピュータは、エリートが知能を拡張するための道具として開発されてきた。
				エリートは、目と手指だけのインターフェイスで、抽象的な概念体系を操作しても、苦痛ではなかったのだろう。
				しかし、これから、現在のインターフェイスの問題をさらに見ていく。
			</p>
			<p>
				石井裕は、マウスとモニターだけで、人がコンピューター・ネットワーク空間に触れないのは、情けないと言った(<a href="#rIY">[rIY]</a>)。
				しかし、それは道具が未熟ということに過ぎない。
				そういった道具を使うヒトの能力の面から見ると、目と手指だけでコンピューター・ネットワーク空間という概念体系とやり取りすることが、情けないのである。
			</p>
		</details>

		<details open><summary><strong>人の能力を生かしていない</strong></summary>
			<p>
				グラフィカル・インターフェイス（GUI）は、ヒトの身体能力を十分に生かしていない。
				<ul>
					<li>
						ヒトの大脳皮質の2/3は、諸情報を総合する連合野である（<a href="#association_area">[人は総合する動物]</a>）。
						ヒトは感覚や運動を特定の部分に頼って生活してはおらず、総合して環境に対処している。
						目と手指だけのインターフェイスは、不自然に限定されたものである。
					</li>
					<li>
						手指は、ヒトの体によって器用さを得て（<a href=#body_supports_hand>[身体と視覚が手指の器用さを支えている]</a>）、
						多様な器用さを発揮できる（<a href="#hand_dexterity">[手指は器用]</a>）。
						しかし、手の一部の機能しか利用していない。
						コンピュータを操作するとき、箸を操り、鋏を操る手の多彩な能力は、どこへ消えた？
					</li>
					<li>
						手は、機械動作するため、遅い（<a href="#fitts_law">[距離の制約]</a>）。
						言語生産（<a href="#text_productivity">「発声のテキスト生産速度は指の5倍」</a>）とか、
						注目を示したり（<a href="#fast_eye_sight">「視線は速い」</a>）とかの動作に比べて、遅い。
						目と手指以外の生体機能をもっと生かせないだろうか？
					</li>
					<li>
						ヒトは、視聴覚を融合して言語を用いて考える（<a href="#言語の発生">「言語の発生」</a>）。
						しかし、コンピュータを操作しているとき、音響器官は遊んでいてもったいない。
					</li>
					<li>
						ヒトは三次元の住民である（<a href="#three_d_space">「二次元と三次元」</a>）。
						しかし、二次元の光る画面だけで、コンピュータを操作する。
						二次元を経由するのは効率的ではある。
						が、三次元の豊かさを見失いやすい。
					</li>
					<li>
						ヒトの小脳（<a href="#小脳は所作の匠">[小脳は所作の匠]</a>）とはかかわらない。
						例えば、歩いているときに、地図をみたくなった。
						歩きながら、手でスマホを持ち、眼はスマホにくぎ付けになる。
						歩いたり転ばないようにバランスをとる知能と、コンピュータを操作する知性は、
						何の関わりもなく独立に動いている。
					</li>
				</ul>
				コンピュータのグラフィック・インターフェイス（GUI）は、目と手指での特定の機能に頼りすぎ、いびつである。
			</p>
		</details>

		<details open><summary><strong>制約が少ないと設計は難しい</strong></summary>
			<p>
				コンピュータのインターフェイスをデザインするのは、水道の蛇口やドアのノブのデザインとは、かなり異なる。
				コンピュータは、計算という目に見えない抽象的なレベルで動く。
				抽象的な操作によって、抽象的な結果を返す。
				どういう操作オプションを提供し、どういう結果提示をすればよいか、抽象的な枠しかない。
				それらをデザインするのは、科学が入りにくい、職人技である。
			</p>
			<p>
				ドナルド・ノーマンによると、日常生活の道具は、物理的な制約、論理的な制約、社会・文化的な制約、意味的な制約などによって、
				操作できることが絞り込まれている（<a href="#rDN">[rDN]</a>）。
			</p>
			<p>
				<ul>
					<li>
						物理的な制約：
						<br>
						例えば、あるプラグはその形状にあったコンセントにしか差し込めない。
						大きな突起は小さな穴に差し込めない。鍵は鍵穴に上下逆さに入れると回らない、などである。
					</li>
					<li>
						社会的文化的な制約：
						<br>
						ネジは時計回りに回すと締り、逆回しにすると緩む。
						車の右側のサイドランプを点灯すると右へ曲がると言う印になる。
						時計の1時と2時の間の時間は、2時と3時の間の時間と同じ長さである、などである。 
					</li>
					<li>
						論理的な制約：
						<br>
						棒を右に倒せば対面した相手からは左に倒すことになる。
						電灯は点灯しているか消灯しているかのいずれかである。
						飛行機の到着時間は、出発時間のあとである、などである。
					</li>
					<li>
						意味的な制約
						<br>
						りんごは歩かない。 オートバイに乗る時に前方は決まっている、などである。
					</li>
				</ul>
			</p>
			<p>
				コンピュータのインターフェイスは、これらのう意味的・論理的な制約くらいしか、利用できない。
				抽象的な世界（コンピューターを作っている人の頭の世界、コマンド用語）の中で、
				必ずしもコンピュータを理解していないヒト向けにデザインしなければならない。
				そういう課題状況で、利用できる制約が少ない。
				非常に難しい領域である。
			</p>
		</details>
	</div>
	
	<div><h3>4.3 操作インターフェイスの問題</h3>

		<details open><summary><strong>アイコンは非力だ</strong></summary>
			<p>
				現在、グラフィカル・インターフェイス（GUI）が主流である。
				グラフィカル・インターフェイスの、物理的に見える操作の側面は、わかりやすい。
				操作は、マウスやタッチで行う。
				マウスは、動かすとカーソルが動き、操作と結果の対応が明確である。
				マウスカーソルを対象の上でクリックすると、対象が選択され、それも操作と結果が明確である。
				タッチは、手が届く範囲内ならば、カーソルを動かすという手間がなく、対象選択が素早くできる。
			</p>
			<p>
				一方、グラフィカル・インターフェイスの表示は、メニューとともにアイコンを多用する。
				アイコンは、視覚的な象徴である。
				<a href="visual_symbol">「象徴を認識できる」</a>でみたように、視覚的な象徴は間接的である。
				水道の蛇口やドアのノブにはある、何ができるかを示す支持機能が、弱い。
				グラフィカル・インターフェイスでは、そういうアイコンを経由して操作する。
			</p>
			<p>
				当初は、デスクトップ・メタファーと言って、デスクにあるペン、文書、ファイルキャビネットなどを暗に図示するという発想があった。
				しかし、機能が、豊かになり、拡大するにつれ、物理的なものから離れた抽象を表現しなければならなくなった。
			</p>
			<p>
				インスタグラムを意味する絵って何だろう？
				ダウンロードや、送信を意味する絵ってなんだろう？
				そういう象徴を設計するが、設計してもバラバラである。
				具体的な対象を示す非言語イメージは、記憶に残りやすいという（<a href="#rCP">[rCP]</a>）。
				しかし、具体を離れたイメージは、曖昧で記憶しにくい。
				象徴は使うのも難しい。
				例えば、高齢者がスマホを使うとき、高齢者は「？」からHELPを連想できない。
				三点リーダーをタップすると画面に掲載できなかった「コマンド」が出てくるとは想像できない。
				プラスマークや三角マークをタップすると、何やら展開して新しい詳しい情報が登場してくるとは、想像できない。
			</p>
			<p>
				現在のスマホのホーム画面のアイコンのラベルが、もしも、なかったとしましょう。
				使えるだろうか？ アプリのアイコンは、ラベルで説明を追加していなければ、利用するのは無理である。
				アイコンという象徴は、何ができるかを示す表現手段としては非力である。
				言葉という手段を併記して初めて意味が通じる象徴とは、何だろう？
				設計する側、利用する側、双方に無駄が多い。
				グラフィカル・インターフェイスは、そこに基本的な問題を持つ。
			</p>
		</details>

		<details open><summary><strong>内部の抽象を露出させている</strong></summary>
			<p>
				うまくアイコン経由でアプリを起動できたとする。
				内部の機能を見てみよう。
				電話アプリ、スプレッドシート、地図アプリ、ワードプロセッサ、電卓アプリなどは、
				対象が明確な意味を備えている。
				そういうアプリは、できることもわかりやすい。
				やったことが目に見えて返ってくる。
				ショートメッセージ、ニュースアプリなど、利用者の意図が明確で単純なアプリも、わかりやすい。
				例えば、高齢者が初めてスマホを使い始めた時、他に何もできなくても、電話アプリは使える。
				昔の黒電話、ガラケーの電話アプリ、で、概念モデルがしっかりとあるからである。
				数字パッドを打つと、ツーツーと音が鳴り、相手が出る、ないし「つながりませんでした」とくる。
				操作も知っているし、結果もすぐに出て、明確である。
			</p>
			<p>
				しかし、そういう利点を持たないアプリ、提供機能自体が抽象的なものも多い。
				それらは、コンピュータ特有の抽象的な用語でしか説明できない。
			</p>
			<p>
				例えば、スプレッドシートで、利用者がやりたいのは表計算である。
				そのファイルを上書き保存や、名前を付けて保存というのは、なにか？
				表は、後で取り出せさえすればよい。
				利用者にとっては、保存方法などの詳細は、どうでもいいことである。
				そこでは、ファイル・フォルダー体系というコンピュータの概念を、ヒトに押し付けている。
				目的以外の抽象概念を、そのままヒトに見せている。
				操作が抽象的な場合、その結果も抽象的である。
				そして、抽象的な操作と抽象的な結果の対応関係は、なおさら抽象的である。
			</p>
			<p>
				例えば、高齢者がスマホを使うとき、「アップデート」の意味はわからない。
				通知が来ても、わけがわからないので、無視する。
				そのうちに、Versionが古くなって、ほかのソフトと整合しなくなり、動作不良を起こす。
				高齢者にとっては、それならもうスマホの買い替え時かとなってしまう。
				アップデートとはコンピュータ特有の抽象概念にすぎないのに、ヒトがそれを理解することを当然視している。
			</p>
			<p>
				また、コンピュータは、ヒトが、IDとパスワードを記憶していることを当然視している。
				生身のヒト世界では、記憶は薄れ、控えは紛失するものである。
				そういう生身のヒトの特質を無視し、コンピュータの都合を押し付けている。
			</p>
			<p>
				石井裕は、「いまのパソコンのユーザー・インターフェイスは抽象化しすぎである」と言った(<a href="#rIY">[rIY]</a>)。
				彼は、そこから、情報という見えない存在を、環境の具体的なものを通して表現する、というタンジブルビッツという考えに至った。
				しかし、そういった抽象的な存在は、ヒトに表現されてほしいものだろうか？　
				ヒトがドリルを買うとき、欲しいのは、ドリルではなく、穴なのである。
				抽象的な概念は、隠して見えないようにするに越したことはない。
			</p>
		</details>

		<details open><summary><strong>メニューは掃きだめだ</strong></summary>
			<p>
				<a href="#magical_number">「短期記憶は4個まで」</a>でみたように、項目が3個か4個までであれば、
				それを記憶したり選択するのは負荷が小さい。
				ところが、今のグラフィカル・インターフェイスのメニューは、機能を増やして詰め込む、いい掃きだめになっている。
				ある一つのメニューの項目が、PCの一画面の底まで並んでいることも、珍しくない。
				しかも、メニューの用語は、多くの場合、コンピュータ・エンジニアの用語である。
				コンピュータ内部の抽象的な用語である。
				利用者が、日常で使わない言葉である。
				メニュー、つまりあるアプリでできることに関して、デザイナーは利用者に必要なことだけに絞り込むことに、多くの場合、失敗している。
			</p>
			<p>
				さらに、利用者がやりたいこと以上に、見つけてほしい広告的なメニュー項目だったりする。
				アプリのインターフェイスが、利用者を操作しようとさえしている。
			</p>
			<p>
				メニューは、簡単に項目数が増える。
				メニューは、別メニューを項目として持つこともある。
				こうして、ヒトがやりたいことを実行するのに、広くて深い探索空間を相手にすることになる。
				理想は、狭くて浅い選択肢のみであってほしいのに。
				使いやすさの逆効果である。
			</p>
			<p>
				例えば、高齢者向けのスマホは、購入時すでにたくさんのアイコンやタイルが掲載されている。
				これらはメニューではないが、選択肢を提供する役割を同じくする。
				電話と連絡先は使える。カメラと写真も使える。
				それ以外のアプリは、何なんでしょう？　
				高齢者にとって、にぎやかな選択肢は、必要な機能を探すときに、便利にするどころか、逆に障害になっている。
				情報処理機器でいろんな便宜を享受できるべきところ、過剰サービス（？）が、かえって逆に、
				シニアのスマホ活用やインターネット活用の阻害要因になっている。
				ヒトのための道具なのに、本末転倒である。
			</p>
		</details>

		<details open><summary><strong>複雑怪奇なインターフェイス</strong></summary>
			<p>
				グラフィカル・インターフェイス（GUI）は、ヒトの概念的な構築物である。
				時間を経て発展するうちに、有機物と同様に、どんどん複雑になる。
			</p>
			<p>
				スマホが登場したとき、当然、グラフィカル・インターフェイスが応用された。
				広い画面のインターフェイスが、小さい画面のものへと変化し、マウスはタッチという直接操作へ変わった。
				しかし、グラフィカル・インターフェイスをもとに、その進化を重ねたため、複雑さはそのまま、ないし、より増した。
				対応関係が複雑怪奇となった。
			</p>
			<p>
				タップは、ボタンを押す日常的な動作と近いので、選択の意味と対応づく。
				ページや地図のスクロールやスワイプは、直感的にわかる。
				地図を大きくしたり小さくしたりするするピンチも、直感的に使える。
				が、そういう直感的な対応ができることばかりではない。
				意味と対応づかないジェスチャーも多い。
				例えば、電話が来た時に、受けるのを右スワイプでやらせるのがある。
				スワイプに一体どういう直感的な意味があるのか？　
				長押しで、アプリのメニューが出てくる。
				長押しにどういう意味があるのか？
				左右スライドで設定のON/OFFを切り替える。
				スライドにどういう意味があるのか？
			</p>
			<p>
				しかも、ジェスチャーと意味対応が複雑に絡み合う。
				一つの操作で、複数の意味がある。
				例えば、タップは、アプリの選択＋起動であり、項目のタップは選択＋メニューの起動でもあり、
				入力フィールドの選択でもあり、などなど。
				一つの意味なのに、その操作は複数ある。
				例えば、スマホでアプリを削除するのはアプリアイコンを長押ししてから行う。
				しかし、写真ギャラリーで写真を削除するには、タップして行う。
				ドナルド・ノーマンの言う、機能の数がスイッチの数を超えている状態である。
				慣れない人が慣れるのは至難である。
			</p>
		</details>

		<details open><summary><strong>操作される危険もある</strong></summary>
			<p>
				ヒトが道具を操作するのは一方向の関係である。
				そこで、こちらの操作から、向こうからの反応を見るまでは、ブラックボックスである。
				そこに、逆の関係が入り込む。	
				利用者が道具に操作されていると解釈もできる現象が、すでに現実に、どこにでも、起きてしまっている。
			</p>
			<p>
				以下のような経験がないだろうか。
				スマホを操作している。
				何気ない操作がトリガーになって、画面いっぱいに広告らしき枠が出てくる。
				了解ボタンがあるので身で、消せない。
				この場合、ボタンを押すしかなくなる。
				スマホのそのアプリないしページに、それを使っているヒトが操作されたのである。
			</p>
			<p>
				スマホをONにする。
				ホーム画面に並んでいるアイコンのそばに、赤い小さな丸と数字が出てくる。
				何かの通知なので、そのアプリを叩いて開く。
				しかし、赤い数字は消えない。
				気になってイライラする。
				スマホのそのアプリが、それを使っているヒトの心を邪魔をしただけでなく、
				何か理解できないことを指示し要求したのである。
			</p>

		</details>

	</div>

</div>

<div class="pagebreak"><hr><h2>5. 操作から会話へ</h2>

	<div><h3>5.1 会話モデル</h3>

		<details open><summary><strong>操作から会話へ</strong></summary>
			<p>
				コンピュータは、道具の一つである。 
				ヒトは、道具を操作する。
				対象を目で把握し、道具を手で操作し、結果を目でモニターする。
				しかし、コンピュータは、操作される道具として、あまりにも具体性がなく、抽象的である。
				そのため、人側に、認知負荷がかかる。
				コンピュータが、エリートの知能拡張の道具という役割ならば、それでもかまわない。
				エリートは、すぐに使いこなせる。
				しかし、コンピュータは、万人が使うものになった。
				そこでは、コンピュータは、ヒトとの関係で、モノ道具を操作するという関係性ではなく、
				会話の相棒という関係性を持たせたほうがよい。
			</p>
			<p>
				ヒトが道具を操作するという一方向の関係であると、
				道具とヒトが調和していなくても看過される。
				例えば、スマホに通知が無秩序に飛び込んでくる。
				高齢者であると、通知のマークを消すやり方がわからず、イライラする。
				そもそも、道具のほうが勝手にヒトに何かをプッシュするというのは、ヒトの意図に反することである。
				ヒトがコンピュータを操作するインターフェイスは、そういう意図を無視した情報の流れを増長している。
				広告ビジネスモデルなどは、機械がヒトを操作することを許容している。
				高齢者であると、広告が画面を占有すると、先に進めなくなって、アプリの利用をやめてしまう。
				操作モデルは、逆説的に、機械がヒトを操作することを増長し、意図の遂行を邪魔する。
			</p><div class="printfriendlyblock">
				<p>
				<table border="2" cellpadding="5">
				<caption align="bottom">ヒトとコンピュータの関係性</caption>
				<tr>
					<th>関係性</th>
					<th>操作</th>
					<th>会話</th>
				</tr>
				<tr>
					<th>特徴</th>
					<td>
							一方向
							<br>&nbsp;&nbsp;ヒトから機械へ
							<br>&nbsp;&nbsp;機械からヒトへもあり		
					</td>
					<td>双方向</td>
				</tr>
				<tr>
					<th>表現水準</th>
					<td>コンピュータ内部の抽象概念</td>
					<td>ヒトの意図と反応</td>
				</tr>
				<tr>
					<th>活躍するヒト生体機能</th>
					<td>目、手指</td>
					<td>身体</td>
				</tr>
				<tr>
					<th>主な利用者</th>
					<td>エリート</td>
					<td>万人</td>
				</tr>
				<tr>
					<th>主な用途</th>
					<td>知能の拡張、情報処理</td>
					<td>情報の流通、ライフライン</td>
				</tr>
				</table>
				</p>
			</div>
			</div>
			<p>
				操作ではなく、ヒト相手の会話をモデルとしてみた場合、コンピュータの道具としての要件が変わってくる。
			</p>
			<p>
				ヒトは、身体のしぐさの一つとして、音声で言語表現する。
				それは、日常的な用語で、意図を表現する。
				ヒトと会話するとき、意図レベルでやり取りをする。
				ヒトの脳だけで構築された抽象的なコンピュータ特有の中間概念は、そこに介在しない。
				ヒトが、電気ドリルを買うとき、欲しいのは穴であり、電気ドリルではない。
				コンピュータの内部の抽象的な概念を隠して、ヒトの意図した目的の水準でやり取りする。
			</p>
			<p>
				人が意図をコンピュータとやり取りするとき、その関係は、ヒトが手と目だけで操作する一方的な関係でない。
				ヒトは連合野で考え、行動する。
				環境に対し、五感を総合して、反応する。
				ヒトは、環境を認知し反応するために、精緻な全身能力を進化させてきた。
				機械・道具のほうも、ヒトの生得的な身体能力を尊重すべきである。
				ヒトの能力をあるがままに引き出し、ヒトの発する身体からの言語を受け止め、反応する。
				ヒトの身体に反応するとは、機械側がヒトの口振り（発話）、目振り（注目）、身振り・手振り（ジェスチャー）、
				表情を、受け止め、やり取りすることである。
				コンピュータ側が、ヒトの身体性を含めてやりとりするものへと設計されるべきである。
				これは、ヒトの生得的な身体能力を、あるがまま、インターフェイスにすることである。
			</p>
			<p>
				それでは、コンピュータは、もっとハイテクでないといけないか？ 
				そうではない、すでに技術はある。
				音声インターフェイスの普及が鍵である。
				口振りに反応するとは、機械側がヒトの音声を認識することである。
				また、ヒトの音声言語は、日常的なレベルで意図を表現する。
				意図表現は、一見、曖昧に見える。
				音声認識で、アプリが組めるのか?
				組み方を変えればいいのである。
				すでにある技術を、ちょっと視点を変えて、デザインをすればよい。
				さらに、身振り、目ぶり、手振りに反応するとは？
				ヒトの身振り、目ぶり、手振りは、日常的な意図を表現し、コンピュータに構築された抽象的な概念階層とは無縁である。　
				コンピュータに、目と耳をつければよい。
				また、スマホはカメラとマイクを持つ。
				モニター付きスマートスピーカーもある。
				目と耳を持つ、サービスロボットあるいは会話ロボットもある。
				すでに、ハードのパーツはある。
				掃除ロボットに目をつけて、ヒトがあっちを掃除してと指示することは、今でも実現できる。
			</p>

		</details>
	</div>

	<div><h3>5.2 音声による意図表現</h3>

		<details open><summary><strong>意図レベルのやりとり</strong></summary>
			<p>
				ヒトがコンピュータを手と目で操作するという関係でなく、会話するという関係では、
				音声が重要な役割を果たす。
			</p>
			<p>
				音声言語は日常生活空間で使われるため、意図を自然に表現できる。
				話し言葉で、一見、曖昧と見える部分は、話す相手や周囲の状況などのコンテキストで、実は明確である。
				ヒトの意図は、周りのコンテキストがあれば、言語音声に自然に表現されている。
				コンピュータという道具は、それを利用し聞くべきだ。
				グラフィカル・インターフェイス（GUI）で、やりたいことを満たすアプリを選ぶのは、視覚的象徴経由である。
				間接的で、意図を伝達するにしては、上等ではない。
				絵を見て、何を意味するのか想起して、アプリの機能を想起する、不自然な認知過程が必要となる。
				一方、音声だと、意図を、日常語彙で、直接、表現できる。
				音声で意図を伝え、それに加えて質問などによるやり取りを利用して、意図解釈をする。
			</p>
			<p>
				音声ならば、意図がすでに包含されているので、道具のほうで何ができるかを示すための設計というのが不要である。
				音声認識で意図を受け取れば、ドナルド・ノーマンの行為7段階説で行為系列への展開という部分がなくせる。
				また、結果も意図のレベルに合わせれば、結果の解釈という中間段階も消せる。
				ヒトは意図を表現し、結果を評価する。
				そうあるべき。
			</p>
		</details>

		<details open><summary><strong>視聴覚を融合すべき</strong></summary>
			<p>
				コンピュータが、ヒトの身体の動作である口振りに反応しようというのが、音声認識である。
			</p>
			<p>
				スマート・スピーカーが出て、音声だけでやり取りするアプリが試されている。
				スマホの音声アシスタントもある。
				いずれも、当初、音声のみのインターフェイスを目指していた。
				ところがどっこい、うまくいかなかった。
				無理に音声のみのやり取りでアプリを組もうとしても、以下の欠点が出る。
			</p>
			<p>
				<ul>
					<li>モードの制御がやりにくい。
						<br>音声のみだと、音声をコンピュータに聞かせている状態なのかそうでないのかの区別ができない。
							また、コマンドなのかテキストなのかの区別ができない。
							音声のみでは、これらの制御がやりにくい。
							「OK Google」とか、「Hey Siri」とか言って、音声をコンピュータに聞き取らせるのを始めるが、不格好である。
							聞き取ったことに対し、音声で返すしか手段がないとすると、アプリも作りにくい。
							音声をコンピュータに聞き取らせるのは、マイクの絵をたたいて始めるほうが自然である。
							またコンピュータが聞き取ったよというフィードバックも、視覚的な反応もあったほうが、アプリは組みやすい。
					</li>
					<li>音声のみでは、そもそも空間内の位置指定ができない。
						<br>
							ヒトは、指差しや、マウスやで、空間の位置指定を容易に行うことができる。
							しかし、それを音声でやるとなると、あいまいな指示か、相当冗長な指示しか表現できない。
					</li>
					<li>ボリューム調整など、アナログ量の制御が苦手である。
						<br>
							アナログ量は、空間的な概念である。
							指つまみで簡単に指定できるボリューム量などは、音声言語で制御するのは難しい。
					</li>
					<li>構造的な情報を扱うのが困難。
						<br>
						視覚は複数個の並列処理ができ、構造を記憶できる（<a href="#space_memory">「空間的に記憶できる」</a>、
						<a href="#structure">「構造を把握できる」</a>）。
						一方、聴覚は逐次情報を対象とし、
						一度にたくさんことを相手にすることは苦手である（<a href="#anti_structure">[聴覚認知は構造保持が苦手]</a>）。
						構造は複数の要素と関係性からなる。
						構造的な情報を扱うのは音声＋聴覚は苦手である。
						例えば、フライトを予約したい。
						日時を指定する、人数を指定する。出発場所と到着場所を指定する。
						その上で、値段込みの選択オプションを検索したい。
						それら関連した情報をコンピュータに指定する過程で、ある時点までに何の指定を済ませたか、何がまだなのか、
						を意識していないと、情報の支持がやりにくいであろう。
						すでに指定したものは、画面上で、目に見えておいてほしい。
					</li>
					<li>多数からの選択が困難。
						<br>
							上記と同じ理由で、2、3個以上の選択肢がある場合、視覚的な補助なしに選択を行うことは困難である。
					</li>
					<li>同音語で困難。
						<br>
							例えば、音声による言語表現では、コンピュータのほうに、キーボードのような選択・修正UIが完備されていない。
							そのため、人名入力や地名入力など同音語が多く、表記を選ぶ必要があるケースでは、コンピュータが音声だけで同音語を識別するのは難しい。
					</li>
				</ul>
			</p>
			<p>			
				こうして、音声のみでコンピュータを操作しようとしても、無理である。
				そもそも、人は<a href="#association_area">「連合野」</a>で行動する。
				ヒトが環境に対するとき、複数の感覚と効果器を総合して、機能する。
				とりわけ、人の言語は<a href="#言語の発生">「言語の発生」</a>にみたように、発生からして視聴覚が融合している。
				音声のみのインターフェイスは、初めから不自然であった。
				音声は意図を表現する。
				その水準でアプリを組もうとしたら、実は、指によるコントロール・空間指示と、視覚による構造把握と、併用せざるを得ない。
				視聴覚融合で、一歩、自然なインターフェイスに近づく。
			</p>
		</details>

		<details open><summary><strong>音声アプリも支持と制約を利用すればよい</strong></summary>
			<p>
				音声でのやり取りは、意図レベルでのやり取りである。
				それは、ヒトの身体レベルのやり取りである。
				ヒト同士の会話に近い。
				すると、高度なテクノロジーを使わないとできないのでは？
				そうではない。
			</p>
			<p>
				ドナルド・ノーマンによれば（<a href="#rDN">[rDN]</a>）、
				日常的な道具は、アフォーダンス（支持、何ができるかをヒトに示す）、制約、概念モデルを利用しているという。
				例えば、はさみの穴は指を入れる場所を示し、指を入れることを支持（アフォード)している。
				穴の大きさは、1本指を入れるという制約を課している。
				ハサミは、二つの刃が交わって紙などを切るという仕組みが目に見えているため、
				何をする道具なのかというハサミの概念モデルは、ヒトにとっては容易に理解できる。
				アフォーダンスは何ができるかを示し、制約は選択肢を制限する。
				アフォーダンスと制約があり、概念モデルが明確であるために、
				生活の中に無数に道具があって使い方を丁寧に教わらなくても、ヒトは使えるのだと。
			<p>
				コンピュータのグラフィカル・インターフェイス（GUI）では、見えているものが支持であり制約となる。
				キーを下に押し下げること自体は、何のための動作か曖昧である。
				しかし、Aと刻印されたキーを押し下げることは、Aの言語表現として、曖昧性がない。
				そして、ヒトは、キーボードを見るとキーAがどこにあるかをすぐに思い出すので、素早くキーボードを操作できる。
				また、グラフィッカル・インターフェイスでは、メニューや画面遷移というコンテキストがある。
				そのために、ある対象をクリックして選択することの意味には、曖昧性がない。
				そのように、視覚情報やコンテキストによって制約されて、手指動作の曖昧性がなくなる。
				そのような曖昧でない動作を前提に、アプリが編み上げられる。
				アプリが組みやすいのである。
			</p>
			<p>
				音声による言語表現は、グラフィカル・インターフェイスと同様に、
				メニューや画面遷移や問い返しという、視覚やコンテキストによる支持と制約を利用すればよい。
				同様な制約を利用すれば、曖昧性をなくすことができ、アプリの構成要素にできる。
				視覚やコンテキストによる支持と制約を利用するというデザインをとれば、音声による意図レベルの表現に曖昧性はない。
				視覚が得意なことは視覚に任せる。
				手指が得意なことは、手指に任せる。
				音声が得意なことは音声に任せる。
				特定の単純なタスクでは、音声会話システムが実用されている。
				もっと複雑なアプリも、このようなデザインの考え方で、実現できる。
			</p>
			<div class="printfriendlyblock">
				<p>
				<table border="2" cellpadding="5">
				<caption align="bottom">支持、制約、概念モデル</caption>
				<tr>
					<th>インターフェイス</th>
					<th>ハサミ</th>
					<th>グラフィカル</th>
					<th>意図</th>
				</tr>
				<tr>
					<th>支持（何ができるかを示唆する）</th>
					<td>指を入れる穴</td>
					<td>キーボードの刻印<br>アプリ・アイコン<br>メニュー<br>ツールバー</td>
					<td>マイクアイコン</td>
				</tr>
				<tr>
					<th>制約（できることを制限する）</th>
					<td>穴の大きさ</td>
					<td>メニュー<br>ツールバー<br>ダイアローグ<br>画面遷移</td>
					<td>プロンプト<br>画面遷移</td>
				</tr>
				<tr>
					<th>概念モデル（何であるか、どうすれば何が起きるかを、説明する）</th>
					<td>2つの刃が交わる物理的形態</td>
					<td>コンピュータ内部の抽象</td>
					<td>ヒトとの会話</td>
				</tr>
				</table>
			</p>
			<p>
				音声アプリは、現在、特定のタスクに特化した専門的なアプリと、汎用でおしゃべりに相手的なものと、二分されることがある。
				特定のタスクに特化したものは実用的で、汎用のものはハイテク、という印象で語られる。
				しかし、支持と制約を利用するという考えは、特定のタスクに特化した機能で、アプリをくみ上げるということである。
				グラフィカル・インターフェイスがアプリの基礎になったのと同じことを、こちらの攻め方でできる。
				汎用のおしゃべりは、生身のヒトか、未来のドラえもんに、任せればばよい。
			</p>
			<p>
				今のコンピュータがさらしている、抽象的なコンピュータ内部概念群は、意図レベルでやりとりすることで、
				隠すことができる。
				無駄に複雑な概念のモデルとは離れられる。
				意図レベルの概念は、わかりやすい。
			</p>
		</details>

		<details open><summary><strong>音声ワープロは可能だ</strong></summary>
			<p>
				ヒトは、考えながらしゃべるのは難しい。
				そのため、音声認識は使えないという考えがある。
				しかし、一瞬考えて、それをしゃべる、それを繰り返す、という時分割なら、うまくできる（<a href="#cognitive_interference">「人は時分割でマルチタスクできる」</a>）。
				そもそも、ヒトにとって言語は、視覚的でありかつ音響的である。
			</p>
			<p>
				また、<a href="#text_productivity">「発声のテキスト生産速度は指の5倍」</a>なため、
				音声を使うと、文書を作成する時間が短くできるという誤解がある。
				しかし、文章を考えるのには、熟考が必要なので、実は、音声表現は、文書作成の時間を短くしない。
			</p>
			<p>
				しかし、音声表現は、アイデアを素早く完全な形で記録することができる。
				たとえ誤認識でノイズが多くても、文であるため、後で編集するときに、想起しやすい。
				その結果、音声でスケッチした後の文書作成はやりやすい。
				音声言語表現は、頭の中にある整理されていないモヤモヤした概念を、忘れないうちに、目に見える形で取り出して、
				編集可能な形にし、考えを発展させられるようにするための手段である<a href="#rNY">[rNY]</a>。
				音声で素早く、視覚的なテキストとして保存してしまえば、それは外部の記憶となる。
				外部の記憶として何回でも再認できるので、じっくり校正する素材となる。
				そうやって、文章を練ることができる。
			</p>
			<p>
				このように、ヒトは、考えてしゃべるという時分割処理を上手にでき、
				素早く音声表現を記録して、後で校正するというやり方で、効率的に文書生産ができる。
			</p>
		</details>

	</div>

	<div><h3>5.3 ジェスチャーによる意図表現</h3>

		<details open><summary><strong>注目は重要な制約</strong></summary>
			<p>
				音声以外に、ヒトが意図を表現するときに重要な役割を果たすものがある。
				視線と身振り、手振り、表情である。
			</p>
			<p>
				ヒトは、視線で注目する。
				ヒトは、指さしで、注目対象を示したりする。
				このような注目行動は、会話の中で、意図を解釈するときの重要な制約となる。
				コンピュータが、ヒトと会話するとき、この制約を利用しない手はない。
			</p>
		</details>

		<details open><summary><strong>視線は操作手段ではない</strong></summary>
			<p>
				ヒトの視線は速い（<a href="#fast_eye_sight">[視線は速い]</a>）。
				このために、視線をパソコンの位置指定手段として使おうという発想があった。
				これなら、手指が動かせないハンディを持っていても、目を動かせられるヒトはいる。
				そういうヒトのための表現手段となる。
				また、健常者でも、指とマウスは指の機械的速度と距離に制約されるが、視線は素早いので距離に束縛されない。
				そこで、画面の位置選択などに、利用されたことがある。
				また、まばたきを検出して、ゲームのミサイル発射の指示として利用されたこともあった。
			</p>
			<p>
				視線の移動は、手指が距離の制約を受ける（<a href="#fitts_law">「手指は距離に束縛される」</a>）のと異なり、距離の束縛を受けない。
				移動は一瞬である。
				また、音声動作は、マイクが音を拾う限り、
				スマホでもデスクトップでも同じスピードでテキスト指定動作ができる。
				音の聞こえる範囲ならば、距離の束縛を受けない。
				現在のコンピュータのインターフェイスは、ヒトが手指で道具を操作するという伝統にあるので、
				こういったヒトの体の特性が無視されている。
			</p>
			<p>
				しかし、ヒトの目は受容器官として進化したものである。
				それを無理に操作手段として使うと、以下の不具合が出る。
				<ul>
					<li>
						受容器官としての動き以外の動きを期待するのは、ヒトに不自然な動作を強いることになる。
						不自然な動作は、疲労に導くし、慣れるまで学習を要する。
					</li>
					<li>
						また目だけを操作手段とすると、位置指定という視線を使った操作なのか、単に探索しているための目の移動なのか、
						状態の区別を機械に伝えるのが難しい。
						メドーサは、見たものを石に変えてしまう。
						注目して確認したくても、どれも石である。
						視線を操作の手段として使うと、そういう不都合が生じる。
					</li>	
				</ul>
			</p>	
		</details>
		
		<details open><summary><strong>視線は注目を示す</strong></summary>
			<p>
				目に、効果器としての役割を求めるのでなく、進化してきた受容器のまま生かすべきである。
				視線は、空間的な注目箇所を示す。
				また、視線は、会話において「まなざし」として、情緒的な情報を伝える重要要素である。
				動物でさえ、まなざしで意思を伝える。
				機械から見れば、ヒトの意図を受け取る際に重要な情報となる。
				知能を持つ機械にとっての、制約をヒトが与えるようにすればよい。
				それだけで、ヒトの意図への反応が自然になる。
				例えば、以下のようなユース・ケースがありうる。
			</p>
			<p>
				<ul>
					<li>
						スマホでニュース記事を読んでいる。
						画面上部のテキストを読みつつ、下にスクロールするため画面下部の画面に触れる。
						たまたま指が触れたところに、広告枠が表示されていた。
						画面が切り替わり広告が表示されて、びっくりする。おいおい。
						この時は、意図はスクロールである。
						もしも、広告を見ながら、そこに触れたなら、広告に興味を持ったのである。		
						そうでなければ、意図はスクロールである。
					</li>
					<li>
						画面に連絡先の人物の写真がいくつか表示されている。
						視線を追跡することで、きょろきょろしているかどうかで、ヒトが現在探索中かどうかが推定できる。
						探索中ならば、探索を助けるように反応する。
						視線がある連絡先に注目し、停留したならば、この連絡先を選択するかと返す。
						それだけで、意図に反応しているかどうかの印象が変わる。
					</li>
					<li>
						部屋にいくつか電化製品がある。
						あるいはPCがあり、いずれかが目と耳を持っているとする。
						その時、エアコンを見ながら、「つけて」といったら、エアコンをつけるという意図である。
						部屋の明かりをつけるという意図ではない。
					</li>
				</ul>
			</p>
			<p>
				このように、視線、まなざしは、ヒトの意図を示す重要な手がかりである。
			</p>
		</details>

		<details open><summary><strong>ジェスチャーは意図を示す</strong></summary>
				<p>
					表情、身振り、手振りは、重要な会話手段である。
					ヒト画像から、それらを読み取るのは、すでに実現できている。
					音声による言語表現と総合して、意図を読み取るのは、技術的にもう可能である。
				</p>
				<p>
					掃除ロボットに目をつけて、ヒトがあっちを掃除してと、指差しで、指示する。
					実三次元世界という物理的な存在があると、意図の解釈を助ける制約が増える。
					例えば、ヒトが台所にいて、居間を指して「あっち」と指さしたら、居間エリアである。
					コンピュータが、このように、ヒトのジェスチャーを見るように変われば、ヒトはコンピュータと、実三次元世界で豊かな会話ができる。
					コンピュータは、二次元のモニターでなく、三次元の実世界の中で、ヒトの身体まるごととやり取りをできるようになる。
				</p>
				<p>
					コンピュータが、目と耳を持ち、ヒトと視聴覚でやり取りする。
					さらに、ヒトの身体を包む三次元で、やり取りをする。
					コンピュータにとって、ヒトは大脳皮質の抽象物の中で概念をこねくり回す主では、なくなる。
					小脳、脊髄を含む脳神経系と身体を持った主となる。
					それを想像すると、コンピュータがどう反応すべきかが、全く変わってくる。
					技術の進歩の方向が変わる。
				</p>
			</details>

</div>

<div class="pagebreak"><hr><h2>6. 終わりに</h2>

		<details open><summary><strong>コンピュータ・アプリのデザインを変える</strong<strong></strong></summary>
			<p>
				当初、コンピュータはエリートの知能の拡張のための道具だった。
				ヒトは、ほかの道具と同様に、コンピュータを操作してきた。
				ところが、逆に、道具が人を操作するとでもいうべきことも、起きている。
				そのようなヒトとコンピュータの間のインターフェイスは、あまり問題視されてこなかった。
				しかし、万人がコンピュータを使う時代になった今、インターフェイスが変わるべき時期である。
			</p>
			<p>
				ヒトがヒトと会話するかのように、コンピュータとやり取りしたい。
				意図のレベルでやり取りする。
				コンピュータがヒトの発話や身体動作を意識する。
				そうすれば、ヒト側の認知負荷とストレスはなくせる。
			</p>
			<p>
				しかし、それを実現するのに、新しい技術開発やハイテクが必要かというとそうではない。
				すでにある技術で十分に実現できる。
				問題は、システムやアプリをデザインするときの考え方である。
				目と手での道具インターフェイスという過去からの延長で考えるのでは、課題が先鋭化するだけである。
				そうではなく、音声と視覚を融合したデザインを最初から目指すべきである。
				音声での意図解釈も、グラフィカル・インターフェイズと同様にコンテキストや制約を利用すれば、
				アプリを組むに足る、曖昧性のないパーツになる。
			</p>
			<p>
				また、スマホ、モニター付きスマートスピーカー、サービスロボットあるいは会話ロボットなどは、目と耳を持つ。
				それらは、ヒトのしゃべりとジェスチャーを認識する能力がある。
				すでに、ハードのパーツはそろっている。
				コンピュータは、それらを利用して、ヒトの目振り、身振り、手振りなども感知し、
				実三次元の制約も利用して、ヒトの意図を三次元の身体込みで解釈するべきである。
			</p>
		</details>

		<details open><summary><strong>メタバースとドラえもん</strong></summary>
			<p>
				頭部にディスプレィを装着し、現実にコンピューター・ネットワーク世界を重ね合わせたり、
				仮想的な世界にいるかのような臨場感を持たせる技術がある。
				コンピューター・ネットワーク空間で得られるリッチな世界を、自分の身体と一体化したかのように感じさせる。
				ヒトの身体を、仮想的な世界に取り込む、ようなことである。
				人の脳は可塑性があるので、そういう技術にヒトが慣れたら、どんなことが起きるか、想像できない。
			</p>
			<p>
				一方、現実の世界と、その中の生身のヒト身体は、またリッチな世界である。
				ヒトの身体と脳神経は、生物の進化の結果である。
				そこを生かすことは、十分開拓されていない。
				それを生かし、意図レベルで、生世界で、ヒトと会話をし、生活する存在は、ドラえもんである。
				ドラえもんが誰にとってもパートナーになったら、どんなことが起きるかわからない。
				人は社会的動物で、生まれて以降の社会的教育で、脳内のいろんな神経をつなぐ。
				実世界のパートナーであるので、大脳皮質だけのパートナーではなく、脊髄や小脳レベルでも付き合うことになる。
				そこのパートナーがドラえもんだ。
				ワクワクしないだろうか？
			</p>
		</details>

		<details open><summary><strong>人体を拡張する</strong></summary>
			<p>
				ヒトの脳・神経は、環境とのやり取りで、生物として、受容・効果器官との協同作業のために進化した。
				そのしくみは、未知なことが多い。
				一方、脳に直接機械・道具を接続することで、道具を自在に操ることが、昔から夢見られてきた。
				脳に直接機械・道具を接続したり、脳の信号を直接解読しようとする技術を、BMI、ブレーン・マシン・インターフェイスという。
			</p>
			<p>
				一方、大脳には可塑性（<a href="#brain_plasticity">［脳の可塑性］</a>）という特質がある。
				つまり、生得の知能よりは、生まれた後で学習して得る知能のほうが、圧倒的に大きい。
				手に第6の指というのを装着し、腕の筋肉の動きでコントロールするように、少し訓練すると、
				自分の体の一部であるかのような感覚で、動かせるという。
				大脳の可塑性のおかげで、人の生体機能さえも機械を使って拡張(進化)できる。
				脳に直接接続するのが近道になるかわからない。
				脳は身体とともに進化してきた。身体から切り離して機械とつなげるよりは、
				むしろ、ヒトの身体と協調するという方向が現実的かもしれない。
			</p>
		</details>

</div>

</div> <!--End of TOC areas-->

<div class="pagebreak"></div><hr>
<h2>脚注</h2>
<details open><summary>注</summary>
<ul>
	<li id=n01>
		[n01] 別のソースでは、視覚は10の8乗ビット/秒、触覚は10の6乗ビット/秒、聴覚と嗅覚はそれぞれ10の5乗ビット/秒、味覚は10の3乗ビット/秒、とも言われる。
	</li>
	<li id=n02>
		[n02] これらに、厳密な根拠はないとも言われる<a href="#rKH">[rKH]。</a>
	</li>
	<li id=n03>
		[n03] gaを発音する唇を見せて、同時時にbaと言う音声を聞かせると、被験者は中間の調音位置をもった「da」を知覚する。
		これをMcGurk効果という。
		<a href="#rSO">[rSO]</a>
		聴覚情報は、知覚レベルになると視覚情報と融合、ないし引っ張られて、知覚される。
	</li>
	<li id=n04>
		[n04] 養老先生による<a href="#rYT">[rYT]</a>と、脳が大きくなったのには、手との相互作用ではなく、内在的な理由があるという。
				知覚系の末端となる脊髄神経細胞は、発生の段階では成人になったときに残る細胞数の数倍、余剰な細胞があるという。
				ヒト以外の動物では、支配している末梢領域を持たない神経は、間引かれるという。
				ヒトではそうでない。
				ヒトの脳が大きくなったのは、知覚に対応しない神経が、互いに接続して支配領域を自前で持ち、
				かつ、互いに、知覚以外から、入力しあう関係を持ったからだという。
				それが意識だと。
	</li>
	<li id=n05>
		[n05] コンピュータと人の情報のやり取りの観点では、
		コンピュータからの出力が知覚したり認知する対象となる。
		また、人からコンピュータへ情報を入力したりする。
		本文書では、コンピュータの入出力という言い方ではなく、ヒトを中心にした言い方をする。
		コンピュータに対して、ヒトは意図的動作をしたり言語表現をする。
		コンピュータからの情報を、ヒトが知覚し認知する。
		また、ユーザという言葉は、コンピュータを中心とした表現なので、使わない。
		あくまで、ヒトあるいは利用者である。
		また、コンピューター・ユーザ・インターフェイスという用語を使わない。
		ヒトがコンピュータを操作するインターフェイス、という言いかたをする。
	</li>
	<li id=n06>
		[n06] 養老先生は、視覚的言語と聴覚的言語の連合を、ヒトの大脳の接続でも解説する。
		ウェルニッケ中枢という聴覚的言語中枢がある。
		そこから、神経が伸びて、角回という視覚的言語中枢に入る。
		聴覚的言語中枢と、視覚的言語中枢との、双方から、神経が伸びて、ブローカ中枢という運動性言語中枢に入る。
		つまり、聴覚と視覚からの入力が、音響発生神経に接続している、と。
		ただし、最近の研究によれば、特定の言語機能はモジュール化されているのではなく、
		広範囲の神経ネットワークによって処理されているらしい、という（<a href="#rCP">[rCP]</a>）。
	</li>
	<li id=n07>
		[n07] 「心の理論」が生まれると、他個体の注意がどこに向いているかを意識できる。
		すると、たまたまそれが自己の身体に向かっているかも、意識できるようになる。
		他個体の心が注意を向けている「じぶん」という意識が、自己意識の源である。
		なお、チンパンジー、ゴリラなどは、鏡を見て自己認知できるという<a href="#rCP">[rCP]</a>。
	</li>
</ul>
</details>

<div class="pagebreak"></div><hr>
<h2>参考文献</h2>
<details open><summary>出典</summary>
<ul>
<li id=rBD>[rBD] Multimodal Interfaces: A Survey of Principles, Models and Frameworks、Bruno Dumas, Denis Lalanne, Sharon Ovian、2009. </li>
<li id=rBS>[rBS] The Limits of Speech Recognition. s.l. : Communication of ACM、Shneiderman, Ben、2000.</li>
<li id=rCL>[rCL] 「137億年の物語」（What On Earth Happendd?)、クリストファー・ロイド（Christopher Lloyd)</li>
<li id=rCP>[rCP] 「認知心理学」、箱田、都築、川端、萩原、有斐閣、ISBN978-4-641-05374-8</li>
<li id=rCW>[rCW] Multiple resources and performance prediction、Wickens, Cristopher D、 2002</li>
<li id=rDE>[rDE] 1968 “Mother of All Demos” by SRI’s Doug Engelbart and Team, https://www.youtube.com/watch?v=B6rKUf9DWRI </li>
<li id=rDN>[rDN] 「誰のためのデザイン？」（The Psychlogy of Everyday Things）、D.A.ノーマン、新曜社認知科学選書,1988</li>
<li id=rFJ>[rFJ] http://fujiidental.jp/clinic/archives/18 </li>
<li id=rFT>[rFT] 「生体情報システム論」、福田忠彦、1995、産業図書、ISBN978-4-7828-5303-0C335</li>
<li id=rHK>[rHK] 「視覚と聴覚はどうちがうか」、樋渡涓二（ひわたり　けんじ）、1997、NHK 第31巻第11号</li>
<li id=rHN>[rHN] 廣野守俊 永雄総一 小脳 脳科学辞典　https://bsd.neuroinf.jp/wiki/%E5%B0%8F%E8%84%B3 (2021)</li>
<li id=rIN>[rIN] 「図解・感覚器の進化 原始動物からヒトへ水中から陸上へ」、岩堀修明（いわほり・のぶはる）著、2011、講談社、ISBN978-4-06-257712-0</li>
<li id=rIY>[rIY] Tangible Bits: コンピューター・ネットワークスペースと人間との物理的な接点、石井裕インタビュー記事　http://www.rm2c.ise.ritsumei.ac.jp/tamura/maindoc/ishii.html</li>
<li id=rKH>[rKH] 『「視覚は人間の情報入力の８０％」説の来し方と行方』、加藤宏、筑波技術大学テクノレポート Vol.25 (1) Dec. 2017</li>
<li id=rKH2>[rKH2] 「霊長類の音声の運動基盤及び多様性とその進化的な背景」、香田啓貴（こだ　ひろき）、日本音響学会誌71巻7号、2015
<li id=KH3> [rKH3] サルの発声から見るヒトの言語の起源、香田啓貴 https://www.brh.co.jp/publication/journal/102/rp/research01/ </li>	
<li id=rKK>[rKK] Eye Tracking in Human-Computer Interaction and Usability Research; Ready to Deliver the Promises、Karn, J. K. Jacob and Keith S.、2003</li>
<li id=rKW>[rKW] 蔵田潔、渡辺雅彦 大脳皮質 脳科学辞典 https://bsd.neuroinf.jp/wiki/%E5%A4%A7%E8%84%B3%E7%9A%AE%E8%B3%AA (2021)</li>
<li id=rMA>[rMA] 「脳の神経細胞の数」、三上章允（みかみ あきちか）、http://web2.chubu-gu.ac.jp/web_labo/mikami/brain/10/index-10.html</li>
<li id=rMC>[rMC] https://www.ted.com/talks/michael_corballis_evolution_s_great_mystery_language/transcript?language=ja</li>
<li id=rMK>[rMK] 『ヒトとは視覚を発達させ、嗅覚を退化させた「か弱きサル」である』、三谷宏治（みたに こうじ）、https://www.careerinq.com/blog/mitani/2015/05/post-6.shtml</li>
<li id=rMS>[rMS] 「皮膚感覚の情報処理」、下条誠、計測と制御、第４１巻、第１０号、２００２年１０月。
<li id=rNT>[rNT] 「霊長類の音声器官の比較発達ー言葉の系統発生」、西村剛(（にしむら　たけし）, The Japanese Journal of Animal Psychology, 60, 1 49-58 (2010)</li>
<li id=rNY>[rNY] 野口悠紀雄『「超」書く技術』(プレジデント社)</li>
<li id=rPN>[rPN] 「生命とは何か？」、ポール・ナース、ダイヤモンド社、ISBN978-4-478-11107-9 C0045</li> 
<li id=rRT>[rRT] 高齢者はICTの何に困っているの？https://www.soumu.go.jp/main_content/000115449.pdf </li>
<li id=rSO>[rSO] 「生体情報処理」、杉江昇、大西昇、昭晃堂、2001年、ISBN4-7856-9060-7</li>
<li id=rST>[rST] 橘木 修志 視細胞 脳科学辞典　https://bsd.neuroinf.jp/wiki/%E8%A6%96%E7%B4%B0%E8%83%9E (2019)</li>
<li id=rSW>[rSW] 「インターフェイスデザインの心理学」(100 Things Every Designer Needs to KNow ABout People)、スーザン・ワインチェンク(Suzan Weinschenk)、O’REILLY、ISBN978-4-87311-557-3</li>
<li id=rTK>[rTK] 「嗅覚の匂い受容メカニズム」、東原和成（とうはら かずしげ）、2015</li>
<li id=rYH>[rYH] 「サピエンス全史」、ユヴァル・ノア・ハラリ、2016</li>
<li id=rYH2>[rYH2] 「指の機能」、米満弘之、1973、精密機械、40巻1号、https://www.jstage.jst.go.jp/article/jjspe1933/40/468/40_468_18/_pdf</li>
<li id=rYT>[rYT]「唯脳論」養老孟司、1998、ISBN4-480-08439-8</li>
<li id=rUT>[rUT] 上村朋子 空間記憶 脳科学辞典 https://bsd.neuroinf.jp/wiki/%E7%A9%BA%E9%96%93%E8%A8%98%E6%86%B6 (2018)</li>
<li id=rVB>[rVB] 「ヒトの脳 : 解剖学的構造と機能」、https://www.visiblebody.com/ja/learn/nervous/brain</li>
<li id=rVB2>[rVB2] 「考えてみるに As We May Think」、Vannevar Bush、山形浩生訳</li>

</li>
</ul>
</details>

<!--https://shu-sait.com/mokuji-jidou-seisei/-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
<script src="toc.js"></script>
<script>
    $(function () {
        $(".toc-contents").toc({
			startLevel: 'h2',
		});
    });
</script>

</body>
</html>